[
  {
    "objectID": "Vision.html",
    "href": "Vision.html",
    "title": "Machine Vision",
    "section": "",
    "text": "Fire/Smoke Detection Algorithm using Deep Learning and Machine Vision\n\n\n\nVision\n\n\n\nFocus on the research and output of the analysis made over the comparison of 2 models implemented with different parameters, as well as different practical implementation ofâ€¦\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "Tools/git/git.html",
    "href": "Tools/git/git.html",
    "title": "Git",
    "section": "",
    "text": "Git is a version control system created by Linus Torvalds, who is also the creator of Linux. Since its first release, Git has largely replaced other version control systems and is used by 93% of software developers worldwide, according to survey results published by StackOverflow.\nThanks to Git, I can sleepğŸ’¤soundly at night, reassured by the knowledge that all of my progress on all of my ongoing project is recorded. My progress is not only stored on my local computer, but also on Git repository hosting services such as GitHub and GitLab.\nA Git repository (repo) is a directory that tracks the changes made to its contents. For more information on GitHub, the largest Git repo host, take a look at the â€œGitHub for supporting, reusing, contributing, and failing safelyâ€ post by Allison Horst and Julie Lowndes on the Openscapes blog.\nGitHub and GitLab do not only host repos, but also websites via GitHub Pages and GitLab Pages. My personal site is hosted for free on both GitHub Pages and GitLab Pages.\nBefore you can benefit from everything GitHub and/or GitLab have to offer, you will need to set up your computer so that you can work locally and/or configure a service like GitHub Codespaces or GitLab Web IDE so you can work remotely in your web browser. In this post, I discuss local development environment setup with the Homebrew package manager and repo setup with the GitHub and GitLab CLI.\nIf you follow along to end of this post, you will have a repo called dotfiles that can be used for Codespaces setup."
  },
  {
    "objectID": "Tools/git/git.html#sec-setup",
    "href": "Tools/git/git.html#sec-setup",
    "title": "Git",
    "section": "Setup",
    "text": "Setup\n\nHomebrew\nLinux, macOS, or Windows Subsystem for Linux (WSL) users can use Homebrew to install everything needed to work through all of the examples in this blog post. First, install Homebrew itself with the latest .pkg installer for macOS or by running the ExampleÂ 1 Unix shell code in your terminal.\n\nExample 1 Â \n/bin/bash -c \"$(curl -fsSL https://\\\nraw.githubusercontent.com/Homebrew/\\\ninstall/HEAD/install.sh)\"\n\nIf you are not completely satisfied with the integrated terminal built into your preferred source-code editor or the standalone terminal that comes with your operating system (OS), you can use Homebrew to install a new one. The standalone terminal I use most often is iTerm2, which is only for macOS, but I also have the following multi-OS terminals: Alacritty, Hyper, Kitty, and Tabby.\nAfter installing Homebrew, you can run brew doctor in your terminal to confirm that everything is set up correctly. If the brew command is not available, you need to follow the instructions provided after installation to add brew to your PATH variable.\nOnce Homebrew is ready, you can run the shell code in ExampleÂ 2 to create a file called Brewfile with the echo shell command and install everything listed in this newly created Brewfile with the brew bundle shell command.\n\nExample 2 Â \necho 'brew \"gh\"\\nbrew \"git\"\nbrew \"glab\"\\ncask \"github\"' &gt; Brewfile\nbrew bundle\n\nThe Brewfile created by the shell code in ExampleÂ 2 installs:\n\nGit,\nthe command line interfaces (CLIs) for\n\nGitHub and\nGitLab, and\n\nthe GitHub Desktop Git Graphical User Interface (GUI).\n\nIf you are curious about how I set up my computer, you can take a look at my Brewfile and other configuration files in my setup repo on GitHub and GitLab. I will highlight a few configuration files in SectionÂ 2.\n\n\nRepository\nBefore we get started, you will need a GitHub and/or GitLab account and a way to authenticate into your account(s).\nOf the many authentication methods, passkeys stand out because they can function as both a password and two-factor authentication%20is%20an%20electronic%20authentication%20method%20in%20which%20a%20user%20is%20granted%20access%20to%20a%20website%20or%20application%20only%20after%20successfully%20presenting%20two%20or%20more%20pieces%20of%20evidence%20(or%20factors)%20to%20an%20authentication%20mechanism.) (2FA), thus combining the two steps in the 2FA sign-in process into one. Passkeys will certaily GitHub recently announced its plan to make 2FA mandatory for code contributors, which will make passkeys . SSH key.\nsatisfy both requirements, including SSH keys are still the easiest way to authenticate to GitHub and GitLab in my honest opinion.\nYou can create a repo using the web interface of https://github.com or https://gitlab.com in your browser, but the best way to start a new project is using the CLI for GitHub or GitLab in your terminal. First, run gh auth login or glab auth login and follow the prompts to authenticate via your web browser or with an authentication token.\nThe GitHub CLI allows you to add an SSH key to your account during or after authentication. The GitLab CLI does not handle SSH keys during authentication, but has a similar command for adding an SSH key to your GitLab account.\nAfter authentication and SSH key setup, you can run the code in either of the code chunks in ExampleÂ 3 to set up your local and remote repos and create a Quarto website project in the local repo. You can create shell alias that combine all of the repo creation steps like I did in my .zshrc.\n\nExample 3 Â \n\nGitHubGitLab\n\n\ncd # start in home directory\nmkdir USERNAME\ncd USERNAME\ngh repo create REPONAME --add-readme --clone --public\ncd REPONAME\n\n\ncd # start in home directory\nmkdir USERNAME\ncd USERNAME\nglab repo create REPONAME --readme --defaultBranch main --public\ncd REPONAME\ngit pull origin main\ngit branch --set-upstream-to=origin/main main\n\n\n\n\nTo make it easier to backup my repos on both GitHub and GitLab, I set up each local repo to have two origin remote URLs using the code as shown in ExampleÂ 4. With this setting, running git push in my local repo updates my remote repos on both GitHub and GitLab.\n\nExample 4 Â \ngit remote add lab git@gitlab.com:maptv/maptv.gitlab.io\ngit remote add hub git@github.com:maptv/maptv.github.io\ngit remote set-url --add origin $(git remote get-url lab)"
  },
  {
    "objectID": "Tools/git/git.html#sec-workflow",
    "href": "Tools/git/git.html#sec-workflow",
    "title": "Git",
    "section": "Git workflow",
    "text": "Git workflow\nWhen I want to add or update the content on my site, I go through the steps in the standard Git workflow shown in FigureÂ 1. Every time I â€œpushâ€ a collection of changes called a commit to my GitHub repo on GitHub, a continuous integration (CI) system called GitHub Actions automatically completes the steps required to build and publish my website.\n\n\n\n\n\nflowchart LR\n   A[working&lt;br/&gt;directory]-.git&lt;br/&gt;add.-&gt;B{{staging&lt;br/&gt;area}}-.git&lt;br/&gt;commit.-&gt;C([local&lt;br/&gt;repo])-.git&lt;br/&gt;push.-&gt;D(remote&lt;br/&gt;repo)\n\n\nFigureÂ 1: Git workflow\n\n\n\n\n\nShell aliases\nTo make it easier to make incremental changes to my website and frequently release new content, I combined all of the git shell commands in FigureÂ 1 into shell aliases. You can add shell aliases to a shell configuration file like .bashrc or .zshrc on your computer to shorten one or more commands and any associated command arguments.\nThe git commit aliases in the .zshrc file in my setup repo on GitHub and GitLab target different groups of files for inclusion in the next commit. For example, cmp targets staged files, camp targets tracked files, a.cmp targets files in the current directory, and aacmp targets files in the repo.\nExampleÂ 5 shows the aacmp alias as an example of the shell alias syntax. The mnemonic device for this alias is add all, commit with a message, and push.\n\nExample 5 Â \n\n\n.zshrc\n\nalias aacmp=\"func() { git add --all && git commit --message \\\n    \\\"$(echo '${*:-$(echo $(git diff --name-status --cached \\\n    | tr \"[:space:]\" \" \"))}')\\\" && git push; }; func\"\n\n\nAliases like aacmp allow me to enter free-form commit messages directly on the command line without quotes, e.g.Â aacmp edit first post. If you decide to try one of these aliases, please exercise extreme caution as any un-escaped or un-quoted metacharacters may yield surprising effects instead of being included verbatim in the commit message, e.g.Â * is replaced by all of the file and directory names in the current directory!\nIf no commit message is provided after the aliases, a generic commit message is created that includes the change type and name of each changed file. In SectionÂ 2.2, I describe how I used this generic commit message approach to further simplify the Git workflow.\n\n\nKeybindings\nAn alternative to a shell alias that combines git commands is to use a keyboard shortcut in a Git Graphical User Interface (GUI) such as GitHub Desktop or the Git interface in a code editor like VSCode, VSCodium, or RStudio. I use keyboard shortcuts in VSCode and VSCodium to send shell commands to the integrated terminal without moving my focus away from the files I am editing.\nI created different shortcuts to control which files are included in each commit: âŒ¥â‡§F for the current file only, âŒ¥â‡§S for already staged files, âŒ¥â‡§T for all tracked files, and âŒ¥â‡§U for all files including untracked files. I also have keyboard shortcuts that affect a specific directory (and all of its subdirectories): âŒ¥â‡§D for the current fileâ€™s directory, âŒ¥â‡§. for shellâ€™s current directory, âŒ¥â‡§C for the current working directory according to VSCode/VSCodium, âŒ¥â‡§W for the Workspace directory.\nExampleÂ 6 displays the âŒ¥â‡§F shortcut as an example of the VSCode/VSCodium shortcut syntax. This shortcut uses the escape code for the return key (\\u000D) to run several git commands and predefined variables to insert the absolute (${file}) and relative (${relativeFile}) path to the currently open file.\n\nExample 6 Â \n\n\nkeybindings.json\n\n{\n  \"key\": \"shift+alt+f\",\n  \"command\": \"workbench.action.terminal.sendSequence\", \"args\": { \"text\":\n    \"git add ${file} && git commit -m \\\"M ${relativeFile}\\\" && git push\\u000D\" },\n  \"when\": \"terminalIsOpen\"\n}\n\n\nIf you want to set up similar shortcuts for yourself, take a look at my keybindings.json in my setup repo on GitHub and GitLab. As you create keyboard shortcuts, please be mindful of keybinding conflicts that may arise.\nTo set up a keyboard shortcut that runs a series of steps rather than a single line of shell code, I suggest you use the VSCode/VSCodium Tasks mechanism, a system designed to automate software build tasks. The default keyboard shortcut to run all tasks in a local or global task.json file is âŒƒâ‡§B on Linux/Windows or âŒ˜â‡§B on Mac (mnemonic: B is for Build), but you can bind other shortcuts to specific tasks.\nIf you use a text editor like Vim or Emacs, you can create keybindings for Vim plugins like fugitive or Emacs packages like magit that run through the entire Git workflow. ExampleÂ 7 shows the Vim+fugitive equivalent of my âŒ¥â‡§F VSCode/VSCodium keybinding.\n\nExample 7 Â \n\n\n.vimrc\n\nnnoremap &lt;A-S-f&gt; :Gw&lt;bar&gt;G! commit -m \"M \"%&lt;bar&gt;G! push&lt;CR&gt;\n\n\nThe drawback of my keyboard shortcut approach for the Git workflow is that it produces generic commit messages that are no very informative. Anyone reading the messages will not be able to tell what changes were made and more importantly why the changes were made.\nTo automatically generate commit messages based on the currently staged changes, we can use a Large Language Model (LLM). Generative artificial intelligence models like LLMs tend to be large in size and have atypical computational requirements.\nI really enjoy using Git, especially with shell aliases in my terminal and keyboard shortcuts in my favorite text editors. If we ever collaborate on a project together, you can be sure that I will insist on using Git!"
  },
  {
    "objectID": "robotics.html",
    "href": "robotics.html",
    "title": "Robotics",
    "section": "",
    "text": "Robot Control\n\n\n\nRobotics\n\n\n\nRobot Control for stability involves algorithms ensuring precise and stable robot motion, for both compliant and rigid body motions, crucial for safe and reliable operationâ€¦\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobot Dynamics\n\n\n\nRobotics\n\n\n\nRobot Dynamics encompasses the analysis of forces like Coriolis effects, mass distribution, and gravity on a robot, crucial for designing control strategies. Thisâ€¦\n\n\n\n\n\n\n\n\n\n\n\n\n\nInverse Kinematics Simulation\n\n\n\nRobotics\n\n\n\nIKO in robotics is the computational process of determining optimal joint configurations to achieve a desired end-effector pose. It plays a vital role in tasks like motionâ€¦\n\n\n\n\n\n\n\n\n\n\n\n\n\nMotion Planning & Trajectory Optimization\n\n\n\nRobotics\n\n\n\nMotion Planning in robotics focuses on devising efficient paths for a robot from its current state to a goal, while Trajectory Optimization refines these paths for smoothâ€¦\n\n\n\n\n\n\n\n\n\n\n\n\n\nRRT Algorithm Simulation\n\n\n\nRobotics\n\n\n\nThe Rapidly Exploring Random Trees (RRT) algorithm is a computational method used in motion planning for robots, rapidly exploring the configuration space to generateâ€¦\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "Robotics/Potential_trajectory.html",
    "href": "Robotics/Potential_trajectory.html",
    "title": "Motion Planning & Trajectory Optimization",
    "section": "",
    "text": "Here I will use potential fields to get a motion plan for the 2-DoF environment shown below. Here the droneâ€™s position is \\(\\normalsize \\theta = [ğ‘¥, ğ‘¦]^ğ‘‡\\).\nImplementing the potential fields approach:\nâ€¢ Setting \\(\\normalsize \\theta_{ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡} = [0, 0]^ğ‘‡ \\; and \\; \\theta_{ğ‘”ğ‘œğ‘l} = [1, 1]^ğ‘‡\\)\nâ€¢ The first obstacle has \\(\\normalsize center \\; ğ‘_1 = [0.3, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.125\\).\nâ€¢ The second obstacle has center \\(\\normalsize center \\; ğ‘_2 = [0.7, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.225\\).\nNOTE: Always start with a low learning rate \\(\\normalsize \\alpha\\) in your g\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Parameters\ntheta_start = np.array([0.0, 0.0])\ntheta_goal = np.array([1.0, 1.0])\ncenters = np.array([[0.3, 0.5], [0.7, 0.5]])\nradii = np.array([0.125, 0.225])\n\n# Function to compute potential field\ndef field(theta, theta_goal, centers, radii):\n    U = 0.5 * np.linalg.norm(theta_goal - theta)**2\n    for idx in range(len(radii)):\n        center = centers[idx]\n        radius = radii[idx]\n        dist = np.linalg.norm(center - theta)\n        if dist &lt; radius:\n            U += 0.5 * (1/dist - 1/radius)**2\n    return U\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\n\n# Drawing circles\nfor idx in range(len(radii)):\n    circle = plt.Circle(centers[idx], radii[idx], color=[0.5, 0.5, 0.5], fill=False)\n    ax.add_patch(circle)\n\n# Start and goal points\nax.plot(theta_start[0], theta_start[1], 'ko', markerfacecolor='k')\nax.plot(theta_goal[0], theta_goal[1], 'ko', markerfacecolor='k')\nax.axis('equal')\n\n# Parameters for gradient descent\ntheta = theta_start\ndelta = 0.01  # Adjusted delta value\nlearning_rate = 0.01  # Adjusted learning rate\npath = [theta_start]  # To store the path\n\n# Animation update function\ndef update(frame):\n    global theta\n    if np.linalg.norm(theta - theta_goal) &lt; 0.1:\n        return\n\n    U = field(theta, theta_goal, centers, radii)\n    U1 = field(theta + np.array([delta, 0.0]), theta_goal, centers, radii)\n    U2 = field(theta + np.array([0.0, delta]), theta_goal, centers, radii)\n    Ugrad = np.array([U1 - U, U2 - U]) / delta\n    theta -= learning_rate * Ugrad\n    path.append(theta.copy())\n    ax.plot([p[0] for p in path], [p[1] for p in path], 'o', color=[1, 0.5, 0], markerfacecolor=[1, 0.5, 0])\n\n# Create animation\nani = FuncAnimation(fig, update, frames=np.arange(300), repeat=False)\n\n# Save to GIF using PillowWriter\ngif_path = 'Potential_Field.gif'\nani.save(gif_path, writer=PillowWriter(fps=30))\n\n\nMATLAB\n\n\nclear\nclose all\n% Parameters\ntheta_start = [0; 0];\ntheta_goal = [1; 1];\ncenters = [.3, .7; .5, .5];\nradii = [.125, .225];\n% Create figure\nfigure\ngrid on\nhold on\nfor idx = 1:length(radii)\n viscircles(centers(:,idx)', radii(idx), 'Color', [0.5, 0.5, 0.5]);\nend\nplot(theta_start(1), theta_start(2), 'ko', 'MarkerFaceColor', 'k')\nplot(theta_goal(1), theta_goal(2), 'ko', 'MarkerFaceColor', 'k')\naxis equal\n% Gradient descent down potential field\ntheta = theta_start;\ndelta = 0.01;\nlearning_rate = 0.01;\nfor idx = 1:1000\n if norm(theta - theta_goal) &lt; 0.1\n break\n end\n U = field(theta, theta_goal, centers, radii);\n U1 = field(theta + [delta; 0], theta_goal, centers, radii);\n U2 = field(theta + [0; delta], theta_goal, centers, radii);\n Ugrad = [U1 - U; U2 - U] / delta;\n theta = theta - learning_rate * Ugrad;\n plot(theta(1), theta(2), 'o', 'color', [1, 0.5, 0], ...\n 'MarkerFaceColor', [1, 0.5, 0])\nend\n% Find potential field at position theta\nfunction U = field(theta, theta_goal, centers, radii)\n U = 0.5 * norm(theta_goal - theta)^2;\n for idx = 1:length(radii)\n center = centers(:, idx);\n radius = radii(idx);\n dist = norm(center - theta);\n if dist &lt; radius\n U = U + 0.5 * (1/dist - 1/radius)^2;\n end\n end\nend\n\n\n\n\n\n\nModifying the position of the obstacles so that a valid plan from ğœƒğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ to ğœƒğ‘”ğ‘œğ‘ğ‘™ exists but the potential fields planner fails (i.e., gets stuck). Below is a simulation that shows the obstacles and the failed motion plan.\n\nThere are many possible solutions. For instance, I am setting \\(\\normalsize ğ‘_1 = [0.4, 0.5]^ğ‘‡\\).\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Parameters\ntheta_start = np.array([0.0, 0.0])\ntheta_goal = np.array([1.0, 1.0])\ncenters = np.array([[0.4, 0.5], [0.5, 0.5]])\nradii = np.array([0.125, 0.225])\n\n# Function to compute potential field\ndef field(theta, theta_goal, centers, radii):\n    U = 0.5 * np.linalg.norm(theta_goal - theta)**2\n    for idx in range(len(radii)):\n        center = centers[idx]\n        radius = radii[idx]\n        dist = np.linalg.norm(center - theta)\n        if dist &lt; radius:\n            U += 0.5 * (1/dist - 1/radius)**2\n    return U\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\n\n# Drawing circles\nfor idx in range(len(radii)):\n    circle = plt.Circle(centers[idx], radii[idx], color=[0.5, 0.5, 0.5], fill=False)\n    ax.add_patch(circle)\n\n# Start and goal points\nax.plot(theta_start[0], theta_start[1], 'ko', markerfacecolor='k')\nax.plot(theta_goal[0], theta_goal[1], 'ko', markerfacecolor='k')\nax.axis('equal')\n\n# Parameters for gradient descent\ntheta = theta_start\ndelta = 0.01  # Adjusted delta value\nlearning_rate = 0.01  # Adjusted learning rate\npath = [theta_start]  # To store the path\n\n# Animation update function\ndef update(frame):\n    global theta\n    if np.linalg.norm(theta - theta_goal) &lt; 0.1:\n        return\n\n    U = field(theta, theta_goal, centers, radii)\n    U1 = field(theta + np.array([delta, 0.0]), theta_goal, centers, radii)\n    U2 = field(theta + np.array([0.0, delta]), theta_goal, centers, radii)\n    Ugrad = np.array([U1 - U, U2 - U]) / delta\n    theta -= learning_rate * Ugrad\n    path.append(theta.copy())\n    ax.plot([p[0] for p in path], [p[1] for p in path], 'o', color=[1, 0.5, 0], markerfacecolor=[1, 0.5, 0])\n\n# Create animation\nani = FuncAnimation(fig, update, frames=np.arange(300), repeat=False)\n\n# Save to GIF using PillowWriter\ngif_path = 'Potential_Field_fail.gif'\nani.save(gif_path, writer=PillowWriter(fps=30))\n\n\nMATLAB\n\nclear\nclose all\n\n% Start and goal environments\ntheta_start= [0; 0];\ntheta_goal = [1; 1];\n\n% Modified obstacle positions and sizes\nobs_c21 = [0.3; 0.5];  % Shifted first obstacle closer to the second\nobs_r21 = 0.15;        % Slightly increased radius\nobs_c22 = [0.8; 0.5];  % Shifted second obstacle closer to the first\nobs_r22 = 0.25;        % Slightly increased radius\n\n% Visualize the environment\nfigure\ngrid on\nhold on\naxis([0, 1, 0, 1])\naxis equal\nviscircles(obs_c21', obs_r21, 'Color', [0.5, 0.5, 0.5]);\nviscircles(obs_c22', obs_r22, 'Color', [0.5, 0.5, 0.5]);\nplot(0, 0, 'ko', 'MarkerFaceColor', 'k');\nplot(1, 1, 'ko', 'MarkerFaceColor', 'k');\n\n% Setting the variables\nalpha= 0.01;\nepsilon = 0.1;\ndelta= 0.01;\n\n% Initial trajectory\ntheta(:,1) = theta_start;\nt=1;\ndel_Unet=1;\n\nwhile norm(del_Unet)&gt; epsilon\n    del_Ux= U_theta(theta(:,t) + [delta;0]);\n    del_Uy= U_theta(theta(:,t)+ [0;delta]);\n    del_U= U_theta(theta(:,t));\n\n    del_Unet=[del_Ux-del_U; del_Uy-del_U]/delta;\n\n    theta(:,t+1)= theta(:,t)- alpha*del_Unet;\n    t=t+1;\nend\n\n\ngrid on\nhold on\naxis equal\nplot(theta(1,:), theta(2,:), 'o-',...\n    'Color', [1, 0.5, 0], 'LineWidth', 2);\n    \n\nfunction U = U_theta(theta) \n    beta=2;\n    gamma=1;\n    theta_goal = [1; 1];\n    % Modified obstacle positions and sizes\n    obs_c21 = [0.3; 0.5];  % Update obstacle parameters\n    obs_r21 = 0.15;\n    obs_c22 = [0.6; 0.5];\n    obs_r22 = 0.25;\n\n    Urep1=0;\n    Urep2=0;\n\n    Uatt=0.5*beta*norm(theta_goal-theta)^2;    \n\n    if norm(obs_c21-theta)&lt;= obs_r21\n        Urep1=0.5* gamma*((1/norm(obs_c21-theta))- (1/obs_r21))^2;\n    end\n    if norm(obs_c22-theta)&lt;= obs_r22\n        Urep2= 0.5*gamma*((1/norm(obs_c22-theta))- (1/obs_r22))^2;\n    end\n\n    Urep= Urep1+ Urep2;\n    disp(Urep);\n    U= Uatt + Urep;\nend\n\n\n\n\n\nThis failure occurs because the robot uses gradient descent to move towards decreasing potential energy, and the robot gets trapped in a local minimum.\nIn this modification, the obstacles are placed closer to each other, potentially creating a narrow corridor that the drone might not be able to navigate due to the combined repulsive forces. This setup can result in the planner getting stuck in a local minimum, where the drone is unable to progress towards the goal. Thus, we place a local minimim in between the start and goal. Gradient descent can only climb â€œdownâ€ so if itâ€™s surrounded by high gradients, it canâ€™t escape.\n\n\n\n\n\n(MATLAB Implementations provided below)\nHere we are using trajectory optimization to perform motion planning in 2-DoF environments. As before, the mobile robotâ€™s position is \\(\\normalsize \\theta = [ğ‘¥, ğ‘¦]^ğ‘‡\\).\nImplementing the trajectory optimization algorithm Below. My code should be able to work with an arbitrary number of waypoints and circular obstacles.\nThus setting the initial trajectory \\(\\normalsize \\xi^0\\) as:\n\\[\\normalsize \\xi^0 = \\left(\\begin{array}{cc}\nlinspace(\\theta_{start}(1), \\; \\theta_{goal}(1), \\; k)\\\\\nlinspace(\\theta_{start}(1), \\; \\theta_{goal}(1), \\; k)\n\\end{array}\\right)\n\\]\n\nEnvironment 1: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.2, 0.35]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.2\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\). A third obstacle with \\(\\normalsize center \\; ğ‘_3 = [0.7, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_3 = 0.2\\). Setting \\(\\normalsize ğ‘˜ = 20\\).\n\n\n\nMATLAB Code\n\nclear\nclose all\n\n% start and goal\ntheta_start = [0;0];\ntheta_goal = [1;1];\ncenters = [0.2 0.5, 0.7; 0.35 0.3, 0.5];\nradii = [0.2, 0.2, 0.2];\n\n% initial trajectory\nn = 2;\nk = 20;\nxi_0 = [linspace(theta_start(1), theta_goal(1), k);...\n linspace(theta_start(2), theta_goal(2), k)];\nxi_0_vec = reshape(xi_0, [], 1);\n\n% start and goal equality constraints\nA = [eye(n) zeros(n, n*(k-1));...\n zeros(n, n*(k-1)), eye(n)];\nB = [theta_start; theta_goal];\n\n% nonlinear optimization\noptions = optimoptions('fmincon','Display','final',...\n 'Algorithm','sqp','MaxFunctionEvaluations',1e5);\nxi_star_vec = fmincon(@(xi) cost(xi, centers, radii), xi_0_vec, ...\n [], [], A, B, [], [], [], options);\nxi_star = reshape(xi_star_vec, 2, []);\n\n% plot result\nfigure\ngrid on\nhold on\naxis equal\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\nplot(xi_0(1,:), xi_0(2,:), 'o-', 'Color', [0.3, 0.3, ...\n 0.3], 'LineWidth', 3);\nplot(xi_star(1,:), xi_star(2,:), 'o-', 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n% cost function to minimize\n\nfunction C = cost(xi, centers, radii)\n xi = reshape(xi, 2, []);\n C = 0;\n for idx = 2:length(xi)\n theta_curr = xi(:, idx);\n theta_prev = xi(:, idx - 1);\n C = C + norm(theta_curr - theta_prev)^2;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_curr - center) &lt; radius\n C = C + 0.5*(1/norm(theta_curr - center) - 1/radius)^2;\n end\n end\n end\nend\n\n\n\n\n\n\n\n\n\nEnvironment 2: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.7]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\). Setting \\(\\normalsize ğ‘˜ = 15\\) waypoints.\n\n\n\nMATLAB Code\n\nclear\nclose all\n\n% start and goal\ntheta_start = [0;0];\ntheta_goal = [1;1];\ncenters = [0.5 0.5; 0.3 0.7];\nradii = [0.3, 0.2];\n\n% initial trajectory\nn = 2;\nk = 15;\nxi_0 = [linspace(theta_start(1), theta_goal(1), k);...\n linspace(theta_start(2), theta_goal(2), k)];\nxi_0_vec = reshape(xi_0, [], 1);\n\n% start and goal equality constraints\nA = [eye(n) zeros(n, n*(k-1));...\n zeros(n, n*(k-1)), eye(n)];\nB = [theta_start; theta_goal];\n\n% nonlinear optimization\noptions = optimoptions('fmincon','Display','final',...\n 'Algorithm','sqp','MaxFunctionEvaluations',1e5);\nxi_star_vec = fmincon(@(xi) cost(xi, centers, radii), xi_0_vec, ...\n [], [], A, B, [], [], [], options);\nxi_star = reshape(xi_star_vec, 2, []);\n\n% plot result\nfigure\ngrid on\nhold on\naxis equal\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\nplot(xi_0(1,:), xi_0(2,:), 'o-', 'Color', [0.3, 0.3, ...\n 0.3], 'LineWidth', 3);\nplot(xi_star(1,:), xi_star(2,:), 'o-', 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n\n% cost function to minimize\nfunction C = cost(xi, centers, radii)\n xi = reshape(xi, 2, []);\n C = 0;\n for idx = 2:length(xi)\n theta_curr = xi(:, idx);\n theta_prev = xi(:, idx - 1);\n C = C + norm(theta_curr - theta_prev)^2;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_curr - center) &lt; radius\n C = C + 0.5*(1/norm(theta_curr - center) - 1/radius)^2;\n end\n end\n end\nend\n\n\n\n\n\n\n\n\n\n\nEnvironment 3: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.55, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\). Trajectory should have \\(\\normalsize ğ‘˜ = 10\\) waypoints.\n\n\n\nMATLAB Code\n\nclear\nclose all\n\n% start and goal\ntheta_start = [0;0];\ntheta_goal = [1;1];\ncenters = [0.55; 0.5];\nradii = 0.3;\n\n% initial trajectory\nn = 2;\nk = 10;\nxi_0 = [linspace(theta_start(1), theta_goal(1), k);...\n linspace(theta_start(2), theta_goal(2), k)];\nxi_0_vec = reshape(xi_0, [], 1);\n\n% start and goal equality constraints\nA = [eye(n) zeros(n, n*(k-1));...\n zeros(n, n*(k-1)), eye(n)];\nB = [theta_start; theta_goal];\n\n% nonlinear optimization\noptions = optimoptions('fmincon','Display','final',...\n 'Algorithm','sqp','MaxFunctionEvaluations',1e5);\nxi_star_vec = fmincon(@(xi) cost(xi, centers, radii), xi_0_vec, ...\n [], [], A, B, [], [], [], options);\nxi_star = reshape(xi_star_vec, 2, []);\n\n% plot result\nfigure\ngrid on\nhold on\naxis equal\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\nplot(xi_0(1,:), xi_0(2,:), 'o-', 'Color', [0.3, 0.3, ...\n 0.3], 'LineWidth', 3);\nplot(xi_star(1,:), xi_star(2,:), 'o-', 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n\n% cost function to minimize\nfunction C = cost(xi, centers, radii)\n xi = reshape(xi, 2, []);\n C = 0;\n for idx = 2:length(xi)\n theta_curr = xi(:, idx);\n theta_prev = xi(:, idx - 1);\n C = C + norm(theta_curr - theta_prev)^2;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_curr - center) &lt; radius\n C = C + 0.5*(1/norm(theta_curr - center) - 1/radius)^2;\n end\n end\n end\nend\n\n\n\n\n\n\n\n\n\nLegend: Here, in all the three environments the gray line is the initial trajectory \\(\\normalsize \\xi^0\\), while the orange line is the optimal trajectory \\(\\normalsize \\xi\\)."
  },
  {
    "objectID": "Robotics/Potential_trajectory.html#potential-fields",
    "href": "Robotics/Potential_trajectory.html#potential-fields",
    "title": "Motion Planning & Trajectory Optimization",
    "section": "",
    "text": "Here I will use potential fields to get a motion plan for the 2-DoF environment shown below. Here the droneâ€™s position is \\(\\normalsize \\theta = [ğ‘¥, ğ‘¦]^ğ‘‡\\).\nImplementing the potential fields approach:\nâ€¢ Setting \\(\\normalsize \\theta_{ğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡} = [0, 0]^ğ‘‡ \\; and \\; \\theta_{ğ‘”ğ‘œğ‘l} = [1, 1]^ğ‘‡\\)\nâ€¢ The first obstacle has \\(\\normalsize center \\; ğ‘_1 = [0.3, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.125\\).\nâ€¢ The second obstacle has center \\(\\normalsize center \\; ğ‘_2 = [0.7, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.225\\).\nNOTE: Always start with a low learning rate \\(\\normalsize \\alpha\\) in your g\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Parameters\ntheta_start = np.array([0.0, 0.0])\ntheta_goal = np.array([1.0, 1.0])\ncenters = np.array([[0.3, 0.5], [0.7, 0.5]])\nradii = np.array([0.125, 0.225])\n\n# Function to compute potential field\ndef field(theta, theta_goal, centers, radii):\n    U = 0.5 * np.linalg.norm(theta_goal - theta)**2\n    for idx in range(len(radii)):\n        center = centers[idx]\n        radius = radii[idx]\n        dist = np.linalg.norm(center - theta)\n        if dist &lt; radius:\n            U += 0.5 * (1/dist - 1/radius)**2\n    return U\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\n\n# Drawing circles\nfor idx in range(len(radii)):\n    circle = plt.Circle(centers[idx], radii[idx], color=[0.5, 0.5, 0.5], fill=False)\n    ax.add_patch(circle)\n\n# Start and goal points\nax.plot(theta_start[0], theta_start[1], 'ko', markerfacecolor='k')\nax.plot(theta_goal[0], theta_goal[1], 'ko', markerfacecolor='k')\nax.axis('equal')\n\n# Parameters for gradient descent\ntheta = theta_start\ndelta = 0.01  # Adjusted delta value\nlearning_rate = 0.01  # Adjusted learning rate\npath = [theta_start]  # To store the path\n\n# Animation update function\ndef update(frame):\n    global theta\n    if np.linalg.norm(theta - theta_goal) &lt; 0.1:\n        return\n\n    U = field(theta, theta_goal, centers, radii)\n    U1 = field(theta + np.array([delta, 0.0]), theta_goal, centers, radii)\n    U2 = field(theta + np.array([0.0, delta]), theta_goal, centers, radii)\n    Ugrad = np.array([U1 - U, U2 - U]) / delta\n    theta -= learning_rate * Ugrad\n    path.append(theta.copy())\n    ax.plot([p[0] for p in path], [p[1] for p in path], 'o', color=[1, 0.5, 0], markerfacecolor=[1, 0.5, 0])\n\n# Create animation\nani = FuncAnimation(fig, update, frames=np.arange(300), repeat=False)\n\n# Save to GIF using PillowWriter\ngif_path = 'Potential_Field.gif'\nani.save(gif_path, writer=PillowWriter(fps=30))\n\n\nMATLAB\n\n\nclear\nclose all\n% Parameters\ntheta_start = [0; 0];\ntheta_goal = [1; 1];\ncenters = [.3, .7; .5, .5];\nradii = [.125, .225];\n% Create figure\nfigure\ngrid on\nhold on\nfor idx = 1:length(radii)\n viscircles(centers(:,idx)', radii(idx), 'Color', [0.5, 0.5, 0.5]);\nend\nplot(theta_start(1), theta_start(2), 'ko', 'MarkerFaceColor', 'k')\nplot(theta_goal(1), theta_goal(2), 'ko', 'MarkerFaceColor', 'k')\naxis equal\n% Gradient descent down potential field\ntheta = theta_start;\ndelta = 0.01;\nlearning_rate = 0.01;\nfor idx = 1:1000\n if norm(theta - theta_goal) &lt; 0.1\n break\n end\n U = field(theta, theta_goal, centers, radii);\n U1 = field(theta + [delta; 0], theta_goal, centers, radii);\n U2 = field(theta + [0; delta], theta_goal, centers, radii);\n Ugrad = [U1 - U; U2 - U] / delta;\n theta = theta - learning_rate * Ugrad;\n plot(theta(1), theta(2), 'o', 'color', [1, 0.5, 0], ...\n 'MarkerFaceColor', [1, 0.5, 0])\nend\n% Find potential field at position theta\nfunction U = field(theta, theta_goal, centers, radii)\n U = 0.5 * norm(theta_goal - theta)^2;\n for idx = 1:length(radii)\n center = centers(:, idx);\n radius = radii(idx);\n dist = norm(center - theta);\n if dist &lt; radius\n U = U + 0.5 * (1/dist - 1/radius)^2;\n end\n end\nend\n\n\n\n\n\n\nModifying the position of the obstacles so that a valid plan from ğœƒğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ to ğœƒğ‘”ğ‘œğ‘ğ‘™ exists but the potential fields planner fails (i.e., gets stuck). Below is a simulation that shows the obstacles and the failed motion plan.\n\nThere are many possible solutions. For instance, I am setting \\(\\normalsize ğ‘_1 = [0.4, 0.5]^ğ‘‡\\)."
  },
  {
    "objectID": "Robotics/Potential_trajectory.html#motion-plan-failure",
    "href": "Robotics/Potential_trajectory.html#motion-plan-failure",
    "title": "Motion Planning & Trajectory Optimization",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.animation import FuncAnimation, PillowWriter\n\n# Parameters\ntheta_start = np.array([0.0, 0.0])\ntheta_goal = np.array([1.0, 1.0])\ncenters = np.array([[0.4, 0.5], [0.5, 0.5]])\nradii = np.array([0.125, 0.225])\n\n# Function to compute potential field\ndef field(theta, theta_goal, centers, radii):\n    U = 0.5 * np.linalg.norm(theta_goal - theta)**2\n    for idx in range(len(radii)):\n        center = centers[idx]\n        radius = radii[idx]\n        dist = np.linalg.norm(center - theta)\n        if dist &lt; radius:\n            U += 0.5 * (1/dist - 1/radius)**2\n    return U\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\n\n# Drawing circles\nfor idx in range(len(radii)):\n    circle = plt.Circle(centers[idx], radii[idx], color=[0.5, 0.5, 0.5], fill=False)\n    ax.add_patch(circle)\n\n# Start and goal points\nax.plot(theta_start[0], theta_start[1], 'ko', markerfacecolor='k')\nax.plot(theta_goal[0], theta_goal[1], 'ko', markerfacecolor='k')\nax.axis('equal')\n\n# Parameters for gradient descent\ntheta = theta_start\ndelta = 0.01  # Adjusted delta value\nlearning_rate = 0.01  # Adjusted learning rate\npath = [theta_start]  # To store the path\n\n# Animation update function\ndef update(frame):\n    global theta\n    if np.linalg.norm(theta - theta_goal) &lt; 0.1:\n        return\n\n    U = field(theta, theta_goal, centers, radii)\n    U1 = field(theta + np.array([delta, 0.0]), theta_goal, centers, radii)\n    U2 = field(theta + np.array([0.0, delta]), theta_goal, centers, radii)\n    Ugrad = np.array([U1 - U, U2 - U]) / delta\n    theta -= learning_rate * Ugrad\n    path.append(theta.copy())\n    ax.plot([p[0] for p in path], [p[1] for p in path], 'o', color=[1, 0.5, 0], markerfacecolor=[1, 0.5, 0])\n\n# Create animation\nani = FuncAnimation(fig, update, frames=np.arange(300), repeat=False)\n\n# Save to GIF using PillowWriter\ngif_path = 'Potential_Field_fail.gif'\nani.save(gif_path, writer=PillowWriter(fps=30))\n\n\nMATLAB\n\nclear\nclose all\n\n% Start and goal environments\ntheta_start= [0; 0];\ntheta_goal = [1; 1];\n\n% Modified obstacle positions and sizes\nobs_c21 = [0.3; 0.5];  % Shifted first obstacle closer to the second\nobs_r21 = 0.15;        % Slightly increased radius\nobs_c22 = [0.8; 0.5];  % Shifted second obstacle closer to the first\nobs_r22 = 0.25;        % Slightly increased radius\n\n% Visualize the environment\nfigure\ngrid on\nhold on\naxis([0, 1, 0, 1])\naxis equal\nviscircles(obs_c21', obs_r21, 'Color', [0.5, 0.5, 0.5]);\nviscircles(obs_c22', obs_r22, 'Color', [0.5, 0.5, 0.5]);\nplot(0, 0, 'ko', 'MarkerFaceColor', 'k');\nplot(1, 1, 'ko', 'MarkerFaceColor', 'k');\n\n% Setting the variables\nalpha= 0.01;\nepsilon = 0.1;\ndelta= 0.01;\n\n% Initial trajectory\ntheta(:,1) = theta_start;\nt=1;\ndel_Unet=1;\n\nwhile norm(del_Unet)&gt; epsilon\n    del_Ux= U_theta(theta(:,t) + [delta;0]);\n    del_Uy= U_theta(theta(:,t)+ [0;delta]);\n    del_U= U_theta(theta(:,t));\n\n    del_Unet=[del_Ux-del_U; del_Uy-del_U]/delta;\n\n    theta(:,t+1)= theta(:,t)- alpha*del_Unet;\n    t=t+1;\nend\n\n\ngrid on\nhold on\naxis equal\nplot(theta(1,:), theta(2,:), 'o-',...\n    'Color', [1, 0.5, 0], 'LineWidth', 2);\n    \n\nfunction U = U_theta(theta) \n    beta=2;\n    gamma=1;\n    theta_goal = [1; 1];\n    % Modified obstacle positions and sizes\n    obs_c21 = [0.3; 0.5];  % Update obstacle parameters\n    obs_r21 = 0.15;\n    obs_c22 = [0.6; 0.5];\n    obs_r22 = 0.25;\n\n    Urep1=0;\n    Urep2=0;\n\n    Uatt=0.5*beta*norm(theta_goal-theta)^2;    \n\n    if norm(obs_c21-theta)&lt;= obs_r21\n        Urep1=0.5* gamma*((1/norm(obs_c21-theta))- (1/obs_r21))^2;\n    end\n    if norm(obs_c22-theta)&lt;= obs_r22\n        Urep2= 0.5*gamma*((1/norm(obs_c22-theta))- (1/obs_r22))^2;\n    end\n\n    Urep= Urep1+ Urep2;\n    disp(Urep);\n    U= Uatt + Urep;\nend\n\n\n\n\n\nThis failure occurs because the robot uses gradient descent to move towards decreasing potential energy, and the robot gets trapped in a local minimum.\nIn this modification, the obstacles are placed closer to each other, potentially creating a narrow corridor that the drone might not be able to navigate due to the combined repulsive forces. This setup can result in the planner getting stuck in a local minimum, where the drone is unable to progress towards the goal. Thus, we place a local minimim in between the start and goal. Gradient descent can only climb â€œdownâ€ so if itâ€™s surrounded by high gradients, it canâ€™t escape."
  },
  {
    "objectID": "Robotics/Potential_trajectory.html#trajectory-optimization",
    "href": "Robotics/Potential_trajectory.html#trajectory-optimization",
    "title": "Motion Planning & Trajectory Optimization",
    "section": "",
    "text": "(MATLAB Implementations provided below)\nHere we are using trajectory optimization to perform motion planning in 2-DoF environments. As before, the mobile robotâ€™s position is \\(\\normalsize \\theta = [ğ‘¥, ğ‘¦]^ğ‘‡\\).\nImplementing the trajectory optimization algorithm Below. My code should be able to work with an arbitrary number of waypoints and circular obstacles.\nThus setting the initial trajectory \\(\\normalsize \\xi^0\\) as:\n\\[\\normalsize \\xi^0 = \\left(\\begin{array}{cc}\nlinspace(\\theta_{start}(1), \\; \\theta_{goal}(1), \\; k)\\\\\nlinspace(\\theta_{start}(1), \\; \\theta_{goal}(1), \\; k)\n\\end{array}\\right)\n\\]\n\nEnvironment 1: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.2, 0.35]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.2\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\). A third obstacle with \\(\\normalsize center \\; ğ‘_3 = [0.7, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_3 = 0.2\\). Setting \\(\\normalsize ğ‘˜ = 20\\).\n\n\n\nMATLAB Code\n\nclear\nclose all\n\n% start and goal\ntheta_start = [0;0];\ntheta_goal = [1;1];\ncenters = [0.2 0.5, 0.7; 0.35 0.3, 0.5];\nradii = [0.2, 0.2, 0.2];\n\n% initial trajectory\nn = 2;\nk = 20;\nxi_0 = [linspace(theta_start(1), theta_goal(1), k);...\n linspace(theta_start(2), theta_goal(2), k)];\nxi_0_vec = reshape(xi_0, [], 1);\n\n% start and goal equality constraints\nA = [eye(n) zeros(n, n*(k-1));...\n zeros(n, n*(k-1)), eye(n)];\nB = [theta_start; theta_goal];\n\n% nonlinear optimization\noptions = optimoptions('fmincon','Display','final',...\n 'Algorithm','sqp','MaxFunctionEvaluations',1e5);\nxi_star_vec = fmincon(@(xi) cost(xi, centers, radii), xi_0_vec, ...\n [], [], A, B, [], [], [], options);\nxi_star = reshape(xi_star_vec, 2, []);\n\n% plot result\nfigure\ngrid on\nhold on\naxis equal\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\nplot(xi_0(1,:), xi_0(2,:), 'o-', 'Color', [0.3, 0.3, ...\n 0.3], 'LineWidth', 3);\nplot(xi_star(1,:), xi_star(2,:), 'o-', 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n% cost function to minimize\n\nfunction C = cost(xi, centers, radii)\n xi = reshape(xi, 2, []);\n C = 0;\n for idx = 2:length(xi)\n theta_curr = xi(:, idx);\n theta_prev = xi(:, idx - 1);\n C = C + norm(theta_curr - theta_prev)^2;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_curr - center) &lt; radius\n C = C + 0.5*(1/norm(theta_curr - center) - 1/radius)^2;\n end\n end\n end\nend\n\n\n\n\n\n\n\n\n\nEnvironment 2: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.7]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\). Setting \\(\\normalsize ğ‘˜ = 15\\) waypoints.\n\n\n\nMATLAB Code\n\nclear\nclose all\n\n% start and goal\ntheta_start = [0;0];\ntheta_goal = [1;1];\ncenters = [0.5 0.5; 0.3 0.7];\nradii = [0.3, 0.2];\n\n% initial trajectory\nn = 2;\nk = 15;\nxi_0 = [linspace(theta_start(1), theta_goal(1), k);...\n linspace(theta_start(2), theta_goal(2), k)];\nxi_0_vec = reshape(xi_0, [], 1);\n\n% start and goal equality constraints\nA = [eye(n) zeros(n, n*(k-1));...\n zeros(n, n*(k-1)), eye(n)];\nB = [theta_start; theta_goal];\n\n% nonlinear optimization\noptions = optimoptions('fmincon','Display','final',...\n 'Algorithm','sqp','MaxFunctionEvaluations',1e5);\nxi_star_vec = fmincon(@(xi) cost(xi, centers, radii), xi_0_vec, ...\n [], [], A, B, [], [], [], options);\nxi_star = reshape(xi_star_vec, 2, []);\n\n% plot result\nfigure\ngrid on\nhold on\naxis equal\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\nplot(xi_0(1,:), xi_0(2,:), 'o-', 'Color', [0.3, 0.3, ...\n 0.3], 'LineWidth', 3);\nplot(xi_star(1,:), xi_star(2,:), 'o-', 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n\n% cost function to minimize\nfunction C = cost(xi, centers, radii)\n xi = reshape(xi, 2, []);\n C = 0;\n for idx = 2:length(xi)\n theta_curr = xi(:, idx);\n theta_prev = xi(:, idx - 1);\n C = C + norm(theta_curr - theta_prev)^2;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_curr - center) &lt; radius\n C = C + 0.5*(1/norm(theta_curr - center) - 1/radius)^2;\n end\n end\n end\nend\n\n\n\n\n\n\n\n\n\n\nEnvironment 3: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.55, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\). Trajectory should have \\(\\normalsize ğ‘˜ = 10\\) waypoints.\n\n\n\nMATLAB Code\n\nclear\nclose all\n\n% start and goal\ntheta_start = [0;0];\ntheta_goal = [1;1];\ncenters = [0.55; 0.5];\nradii = 0.3;\n\n% initial trajectory\nn = 2;\nk = 10;\nxi_0 = [linspace(theta_start(1), theta_goal(1), k);...\n linspace(theta_start(2), theta_goal(2), k)];\nxi_0_vec = reshape(xi_0, [], 1);\n\n% start and goal equality constraints\nA = [eye(n) zeros(n, n*(k-1));...\n zeros(n, n*(k-1)), eye(n)];\nB = [theta_start; theta_goal];\n\n% nonlinear optimization\noptions = optimoptions('fmincon','Display','final',...\n 'Algorithm','sqp','MaxFunctionEvaluations',1e5);\nxi_star_vec = fmincon(@(xi) cost(xi, centers, radii), xi_0_vec, ...\n [], [], A, B, [], [], [], options);\nxi_star = reshape(xi_star_vec, 2, []);\n\n% plot result\nfigure\ngrid on\nhold on\naxis equal\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\nplot(xi_0(1,:), xi_0(2,:), 'o-', 'Color', [0.3, 0.3, ...\n 0.3], 'LineWidth', 3);\nplot(xi_star(1,:), xi_star(2,:), 'o-', 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n\n% cost function to minimize\nfunction C = cost(xi, centers, radii)\n xi = reshape(xi, 2, []);\n C = 0;\n for idx = 2:length(xi)\n theta_curr = xi(:, idx);\n theta_prev = xi(:, idx - 1);\n C = C + norm(theta_curr - theta_prev)^2;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_curr - center) &lt; radius\n C = C + 0.5*(1/norm(theta_curr - center) - 1/radius)^2;\n end\n end\n end\nend\n\n\n\n\n\n\n\n\n\nLegend: Here, in all the three environments the gray line is the initial trajectory \\(\\normalsize \\xi^0\\), while the orange line is the optimal trajectory \\(\\normalsize \\xi\\)."
  },
  {
    "objectID": "Robotics/Inverse_kinematics.html",
    "href": "Robotics/Inverse_kinematics.html",
    "title": "Inverse Kinematics Simulation",
    "section": "",
    "text": "We are building a snake robot. This snake robot moves in a plane and has 5 joints, making it a redundant robot. We are using this redundancy to mimic the motion of real snakes.\n\n\n\n\n\nLeaving \\(\\normalsize ğ‘ = 0\\) within the Jacobian pseudoinverse. Implementing the numerical inverse kinematics algorithm to find the inverse kinematics solutions when:\n\n\n\nPlot of the snake robot in its initial position ğœƒ = [ğœ‹/8, ğœ‹/8, ğœ‹/8, ğœ‹/8, ğœ‹/8]â€™\n\n\n\nCase 1: ğ¿ = 1 and the desired end-effector pose is:\n\n\\(\\normalsize T_{sb} = [rotz(\\pi/4), [3; 2; 0]; 0 \\, 0 \\, 0 \\, 1]\\)\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        theta = np.arccos(0.5 * (np.trace(R) - 1))\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = adjointM(T) @ Si\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(np.pi/4), np.array([[3, 2, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V)\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_1.gif\", writer=writer)\n\n\n[0.1993027  0.46598915 0.5549693  0.45091411 0.16786868]\n[0.06113478 0.51001426 0.66899306 0.48188162 0.00600057]\n[-0.04783409  0.54015932  0.75714363  0.5007544  -0.12217812]\n[-0.13708095  0.56186278  0.82784958  0.51267239 -0.22751968]\n[-0.21165874  0.57790022  0.88568751  0.52017065 -0.31583542]\n[-0.27477388  0.5899323   0.93358891  0.52472423 -0.39080532]\n[-0.32867117  0.59904312  0.97361294  0.5272736  -0.45499474]\n[-0.37501922  0.60597918  1.00728854  0.52845196 -0.51030329]\n[-0.41510644  0.61127299  1.03579107  0.52869999 -0.55819525]\n[-0.4499529   0.61531389  1.06004448  0.52833026 -0.59983118]\n[-0.48038014  0.61839181  1.08078598  0.52756653 -0.6361495 ]\n[-0.50705787  0.62072567  1.09860977  0.52656924 -0.66792036]\n[-0.53053703  0.62248263  1.11399827  0.52545283 -0.69578303]\n[-0.55127405  0.62379138  1.12734533  0.52429798 -0.7202729 ]\n[-0.5696494   0.62475176  1.13897385  0.52316041 -0.74184164]\n[-0.5859818   0.62544159  1.14914958  0.52207742 -0.76087249]\n[-0.60053959  0.62592179  1.15809195  0.52107267 -0.77769233]\n[-0.61354965  0.62624023  1.16598274  0.52015989 -0.79258114]\n[-0.62520472  0.62643455  1.17297302  0.51934559 -0.80577965]\n[-0.63566914  0.62653443  1.17918872  0.51863112 -0.81749553]\n[-0.64508373  0.62656321  1.18473524  0.51801426 -0.82790846]\n[-0.65356956  0.62653929  1.18970101  0.51749037 -0.83717436]\n[-0.66123122  0.62647712  1.19416061  0.5170533  -0.84542885]\n[-0.66815939  0.62638801  1.19817709  0.51669598 -0.85279014]\n[-0.674433    0.62628077  1.20180402  0.51641096 -0.85936152]\n[-0.68012103  0.62616224  1.2050871   0.5161907  -0.86523342]\n[-0.68528399  0.62603768  1.20806547  0.51602782 -0.87048512]\n[-0.68997511  0.62591108  1.21077282  0.51591529 -0.87518629]\n[-0.69424143  0.62578545  1.21323825  0.51584648 -0.87939821]\n[-0.6981246   0.625663    1.21548703  0.51581529 -0.88317488]\n[-0.70166164  0.62554531  1.21754119  0.51581611 -0.88656397]\n[-0.70488554  0.62543349  1.21942004  0.51584388 -0.88960757]\n[-0.70782576  0.62532823  1.22114053  0.51589405 -0.89234295]\n\n\nAt each iteration we first plot the robot and save a video frame. Then we calculate the Jacobian and perform numerical inverse kinematics. The loop terminates when the actual pose is close to the desired pose.\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_1.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(pi/4), [3;2;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[0;0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nCase 2: ğ¿ = 1 and the desired end-effector pose is:\n\n\\(\\normalsize T_{sb} = [rotz(\\pi/2), [-2; 4; 0]; 0 \\, 0 \\, 0 \\, 1]\\)\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        theta = np.arccos(0.5 * (np.trace(R) - 1))\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = np.dot(adjointM(T), Si)\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(np.pi/2), np.array([[-2, 4, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V)\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_2.gif\", writer=writer)\n\n\n[0.27558403 0.52539357 0.58147356 0.43337151 0.10176109]\n[ 0.23291267  0.62281115  0.69826779  0.42871219 -0.10691892]\n[ 0.22580829  0.70313343  0.77671288  0.40497409 -0.27382697]\n[ 0.24119701  0.77132002  0.82821562  0.37105773 -0.41150535]\n[ 0.2718153   0.8295479   0.85923151  0.33181449 -0.52627559]\n[ 0.31274707  0.87904332  0.8743584   0.2903601  -0.62219375]\n[ 0.36037301  0.92067736  0.8772505   0.24879702 -0.70229657]\n[ 0.41195291  0.95520389  0.8709295   0.2085294  -0.76907122]\n[ 0.46541083  0.98335279  0.85790084  0.17044848 -0.82464119]\n[ 0.51919361  1.00585499  0.84021288  0.13506709 -0.870841  ]\n[ 0.5721616   1.02343655  0.81950593  0.10262535 -0.90925052]\n[ 0.62349983  1.03680166  0.79706383  0.07317388 -0.94121782]\n[ 0.67264561  1.04661494  0.77386781  0.04663753 -0.96788097]\n[ 0.71923024  1.05348758  0.75064905  0.02286273 -0.99019128]\n[ 0.7630329   1.05796872  0.72793692  0.00165141 -1.00893724]\n[ 0.8039445   1.06054162  0.70610121 -0.01721531 -1.02476784]\n[ 0.84193941  1.06162373  0.68538768 -0.03396117 -1.03821437]\n[ 0.87705364  1.0615696   0.66594718 -0.04880443 -1.04970985]\n[ 0.90936772  1.06067554  0.64785912 -0.06195155 -1.05960604]\n[ 0.93899362  1.05918532  0.63114996 -0.07359367 -1.06818794]\n[ 0.96606452  1.05729631  0.61580777 -0.08390494 -1.07568598]\n[ 0.9907269   1.05516563  0.60179336 -0.09304224 -1.08228624]\n[ 1.01313455  1.05291598  0.58904898 -0.10114556 -1.08813882]\n[ 1.0334439   1.05064118  0.57750482 -0.10833905 -1.09336475]\n[ 1.05181053  1.04841102  0.567084   -0.11473216 -1.09806168]\n[ 1.06838657  1.04627563  0.55770621 -0.12042109 -1.10230846]\n[ 1.08331882  1.04426933  0.5492905  -0.1254901  -1.10616887]\n[ 1.09674737  1.04241382  0.54175722 -0.13001288 -1.10969466]\n[ 1.10880476  1.04072101  0.53502938 -0.13405379 -1.11292799]\n[ 1.1196154   1.03919526  0.52903362 -0.13766903 -1.11590337]\n[ 1.12929533  1.03783535  0.52370084 -0.14090766 -1.11864926]\n[ 1.13795221  1.03663596  0.51896646 -0.14381257 -1.12118929]\n[ 1.14568536  1.03558894  0.51477065 -0.14642125 -1.12354325]\n[ 1.15258609  1.0346843   0.51105826 -0.14876653 -1.12572791]\n[ 1.15873796  1.03391096  0.50777879 -0.15087723 -1.12775764]\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_2.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(pi/4), [-2;4;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[0;0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nCase 3: ğ¿ = 1 and the desired end-effector pose is:\n\n\\(\\normalsize T_{sb} = [rotz(0), [3; -1; 0]; 0 \\, 0 \\, 0 \\, 1]\\)\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        trace = np.clip((np.trace(R) - 1) / 2, -1, 1)  # Clip trace to the valid range\n        if trace &lt; -1:\n            trace = -1\n        elif trace &gt; 1:\n            trace = 1\n        theta = np.arccos(0.5*trace)\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = np.dot(adjointM(T), Si)\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(0), np.array([[3, -1, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V)\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_3.gif\", writer=writer)\n\n\n[-0.05314555  0.56290261  0.78072182  0.56314343 -0.06077186]\n[-0.33026032  0.63092663  1.01551888  0.63990985 -0.33531103]\n[-0.55313051  0.66623181  1.19775149  0.69005746 -0.55022883]\n[-0.74036733  0.68131901  1.34397836  0.72586206 -0.72416418]\n[-0.899747    0.68214433  1.46120003  0.75317194 -0.86472893]\n[-1.03582782  0.67258771  1.55378335  0.77545496 -0.97669791]\n[-1.1519017   0.65561888  1.62525653  0.79481972 -1.06406388]\n[-1.25062936  0.63367531  1.6788238   0.81244052 -1.13056494]\n[-1.33428373  0.60877274  1.71746794  0.82885285 -1.1797263 ]\n[-1.40486005  0.58256283  1.74395497  0.84415241 -1.21486659]\n[-1.46406795  0.55687648  1.76118173  0.85743896 -1.24027725]\n[-1.51353785  0.53217219  1.77100399  0.86915723 -1.25745585]\n[-1.55474869  0.50873305  1.77503385  0.87962813 -1.26781729]\n[-1.58900369  0.4867206   1.77463151  0.8890719  -1.27265526]\n[-1.61742724  0.46621157  1.77091752  0.8976355  -1.27311668]\n[-1.6409756   0.44722316  1.76479726  0.90541746 -1.2701912 ]\n[-1.66045459  0.42973107  1.75699074  0.91248714 -1.26471358]\n[-1.67653945  0.4136828   1.74806283  0.9188978  -1.25737455]\n[-1.68979423  0.39900764  1.73845126  0.92469464 -1.24873636]\n[-1.7006894   0.38562402  1.72849118  0.9299192  -1.23925005]\n[-1.70961713  0.37344497  1.71843577  0.93461151 -1.22927246]\n[-1.71690439  0.36238196  1.70847343  0.93881083 -1.21908203]\n[-1.72282384  0.35234764  1.69874154  0.9425557  -1.20889267]\n[-1.72760309  0.34325763  1.68933768  0.94588371 -1.19886582]\n[-1.73143222  0.3350316   1.68032849  0.94883113 -1.18912055]\n[-1.73447014  0.327594    1.67175674  0.95143261 -1.17974204]\n[-1.73684978  0.32087437  1.6636469   0.95372086 -1.17078857]\n[-1.73868244  0.31480739  1.65600958  0.95572658 -1.16229719]\n[-1.74006138  0.30933284  1.64884495  0.95747827 -1.15428843]\n[-1.74106475  0.3043954   1.64214551  0.95900226 -1.14677005]\n[-1.74175811  0.29994445  1.6358982   0.96032272 -1.13974006]\n[-1.74219649  0.29593375  1.63008604  0.96146173 -1.13318921]\n[-1.74242602  0.29232115  1.62468945  0.9624394  -1.12710291]\n[-1.74248544  0.28906833  1.61968724  0.96327395 -1.12146281]\n[-1.74240725  0.28614046  1.61505735  0.96398183 -1.11624799]\n[-1.74221868  0.28350593  1.61077745  0.96457789 -1.11143595]\n[-1.74194258  0.2811361   1.60682536  0.96507547 -1.10700338]\n[-1.74159806  0.279005    1.60317935  0.96548656 -1.10292668]\n[-1.74120108  0.27708916  1.59981843  0.96582187 -1.09918245]\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_3.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(0), [3;-1;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[0;0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all"
  },
  {
    "objectID": "Robotics/Inverse_kinematics.html#inverse-kinematics-optimazation",
    "href": "Robotics/Inverse_kinematics.html#inverse-kinematics-optimazation",
    "title": "Inverse Kinematics Simulation",
    "section": "",
    "text": "We are building a snake robot. This snake robot moves in a plane and has 5 joints, making it a redundant robot. We are using this redundancy to mimic the motion of real snakes.\n\n\n\n\n\nLeaving \\(\\normalsize ğ‘ = 0\\) within the Jacobian pseudoinverse. Implementing the numerical inverse kinematics algorithm to find the inverse kinematics solutions when:\n\n\n\nPlot of the snake robot in its initial position ğœƒ = [ğœ‹/8, ğœ‹/8, ğœ‹/8, ğœ‹/8, ğœ‹/8]â€™\n\n\n\nCase 1: ğ¿ = 1 and the desired end-effector pose is:\n\n\\(\\normalsize T_{sb} = [rotz(\\pi/4), [3; 2; 0]; 0 \\, 0 \\, 0 \\, 1]\\)\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        theta = np.arccos(0.5 * (np.trace(R) - 1))\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = adjointM(T) @ Si\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(np.pi/4), np.array([[3, 2, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V)\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_1.gif\", writer=writer)\n\n\n[0.1993027  0.46598915 0.5549693  0.45091411 0.16786868]\n[0.06113478 0.51001426 0.66899306 0.48188162 0.00600057]\n[-0.04783409  0.54015932  0.75714363  0.5007544  -0.12217812]\n[-0.13708095  0.56186278  0.82784958  0.51267239 -0.22751968]\n[-0.21165874  0.57790022  0.88568751  0.52017065 -0.31583542]\n[-0.27477388  0.5899323   0.93358891  0.52472423 -0.39080532]\n[-0.32867117  0.59904312  0.97361294  0.5272736  -0.45499474]\n[-0.37501922  0.60597918  1.00728854  0.52845196 -0.51030329]\n[-0.41510644  0.61127299  1.03579107  0.52869999 -0.55819525]\n[-0.4499529   0.61531389  1.06004448  0.52833026 -0.59983118]\n[-0.48038014  0.61839181  1.08078598  0.52756653 -0.6361495 ]\n[-0.50705787  0.62072567  1.09860977  0.52656924 -0.66792036]\n[-0.53053703  0.62248263  1.11399827  0.52545283 -0.69578303]\n[-0.55127405  0.62379138  1.12734533  0.52429798 -0.7202729 ]\n[-0.5696494   0.62475176  1.13897385  0.52316041 -0.74184164]\n[-0.5859818   0.62544159  1.14914958  0.52207742 -0.76087249]\n[-0.60053959  0.62592179  1.15809195  0.52107267 -0.77769233]\n[-0.61354965  0.62624023  1.16598274  0.52015989 -0.79258114]\n[-0.62520472  0.62643455  1.17297302  0.51934559 -0.80577965]\n[-0.63566914  0.62653443  1.17918872  0.51863112 -0.81749553]\n[-0.64508373  0.62656321  1.18473524  0.51801426 -0.82790846]\n[-0.65356956  0.62653929  1.18970101  0.51749037 -0.83717436]\n[-0.66123122  0.62647712  1.19416061  0.5170533  -0.84542885]\n[-0.66815939  0.62638801  1.19817709  0.51669598 -0.85279014]\n[-0.674433    0.62628077  1.20180402  0.51641096 -0.85936152]\n[-0.68012103  0.62616224  1.2050871   0.5161907  -0.86523342]\n[-0.68528399  0.62603768  1.20806547  0.51602782 -0.87048512]\n[-0.68997511  0.62591108  1.21077282  0.51591529 -0.87518629]\n[-0.69424143  0.62578545  1.21323825  0.51584648 -0.87939821]\n[-0.6981246   0.625663    1.21548703  0.51581529 -0.88317488]\n[-0.70166164  0.62554531  1.21754119  0.51581611 -0.88656397]\n[-0.70488554  0.62543349  1.21942004  0.51584388 -0.88960757]\n[-0.70782576  0.62532823  1.22114053  0.51589405 -0.89234295]\n\n\nAt each iteration we first plot the robot and save a video frame. Then we calculate the Jacobian and perform numerical inverse kinematics. The loop terminates when the actual pose is close to the desired pose.\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_1.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(pi/4), [3;2;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[0;0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nCase 2: ğ¿ = 1 and the desired end-effector pose is:\n\n\\(\\normalsize T_{sb} = [rotz(\\pi/2), [-2; 4; 0]; 0 \\, 0 \\, 0 \\, 1]\\)\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        theta = np.arccos(0.5 * (np.trace(R) - 1))\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = np.dot(adjointM(T), Si)\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(np.pi/2), np.array([[-2, 4, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V)\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_2.gif\", writer=writer)\n\n\n[0.27558403 0.52539357 0.58147356 0.43337151 0.10176109]\n[ 0.23291267  0.62281115  0.69826779  0.42871219 -0.10691892]\n[ 0.22580829  0.70313343  0.77671288  0.40497409 -0.27382697]\n[ 0.24119701  0.77132002  0.82821562  0.37105773 -0.41150535]\n[ 0.2718153   0.8295479   0.85923151  0.33181449 -0.52627559]\n[ 0.31274707  0.87904332  0.8743584   0.2903601  -0.62219375]\n[ 0.36037301  0.92067736  0.8772505   0.24879702 -0.70229657]\n[ 0.41195291  0.95520389  0.8709295   0.2085294  -0.76907122]\n[ 0.46541083  0.98335279  0.85790084  0.17044848 -0.82464119]\n[ 0.51919361  1.00585499  0.84021288  0.13506709 -0.870841  ]\n[ 0.5721616   1.02343655  0.81950593  0.10262535 -0.90925052]\n[ 0.62349983  1.03680166  0.79706383  0.07317388 -0.94121782]\n[ 0.67264561  1.04661494  0.77386781  0.04663753 -0.96788097]\n[ 0.71923024  1.05348758  0.75064905  0.02286273 -0.99019128]\n[ 0.7630329   1.05796872  0.72793692  0.00165141 -1.00893724]\n[ 0.8039445   1.06054162  0.70610121 -0.01721531 -1.02476784]\n[ 0.84193941  1.06162373  0.68538768 -0.03396117 -1.03821437]\n[ 0.87705364  1.0615696   0.66594718 -0.04880443 -1.04970985]\n[ 0.90936772  1.06067554  0.64785912 -0.06195155 -1.05960604]\n[ 0.93899362  1.05918532  0.63114996 -0.07359367 -1.06818794]\n[ 0.96606452  1.05729631  0.61580777 -0.08390494 -1.07568598]\n[ 0.9907269   1.05516563  0.60179336 -0.09304224 -1.08228624]\n[ 1.01313455  1.05291598  0.58904898 -0.10114556 -1.08813882]\n[ 1.0334439   1.05064118  0.57750482 -0.10833905 -1.09336475]\n[ 1.05181053  1.04841102  0.567084   -0.11473216 -1.09806168]\n[ 1.06838657  1.04627563  0.55770621 -0.12042109 -1.10230846]\n[ 1.08331882  1.04426933  0.5492905  -0.1254901  -1.10616887]\n[ 1.09674737  1.04241382  0.54175722 -0.13001288 -1.10969466]\n[ 1.10880476  1.04072101  0.53502938 -0.13405379 -1.11292799]\n[ 1.1196154   1.03919526  0.52903362 -0.13766903 -1.11590337]\n[ 1.12929533  1.03783535  0.52370084 -0.14090766 -1.11864926]\n[ 1.13795221  1.03663596  0.51896646 -0.14381257 -1.12118929]\n[ 1.14568536  1.03558894  0.51477065 -0.14642125 -1.12354325]\n[ 1.15258609  1.0346843   0.51105826 -0.14876653 -1.12572791]\n[ 1.15873796  1.03391096  0.50777879 -0.15087723 -1.12775764]\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_2.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(pi/4), [-2;4;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[0;0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nCase 3: ğ¿ = 1 and the desired end-effector pose is:\n\n\\(\\normalsize T_{sb} = [rotz(0), [3; -1; 0]; 0 \\, 0 \\, 0 \\, 1]\\)\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        trace = np.clip((np.trace(R) - 1) / 2, -1, 1)  # Clip trace to the valid range\n        if trace &lt; -1:\n            trace = -1\n        elif trace &gt; 1:\n            trace = 1\n        theta = np.arccos(0.5*trace)\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = np.dot(adjointM(T), Si)\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(0), np.array([[3, -1, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V)\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_3.gif\", writer=writer)\n\n\n[-0.05314555  0.56290261  0.78072182  0.56314343 -0.06077186]\n[-0.33026032  0.63092663  1.01551888  0.63990985 -0.33531103]\n[-0.55313051  0.66623181  1.19775149  0.69005746 -0.55022883]\n[-0.74036733  0.68131901  1.34397836  0.72586206 -0.72416418]\n[-0.899747    0.68214433  1.46120003  0.75317194 -0.86472893]\n[-1.03582782  0.67258771  1.55378335  0.77545496 -0.97669791]\n[-1.1519017   0.65561888  1.62525653  0.79481972 -1.06406388]\n[-1.25062936  0.63367531  1.6788238   0.81244052 -1.13056494]\n[-1.33428373  0.60877274  1.71746794  0.82885285 -1.1797263 ]\n[-1.40486005  0.58256283  1.74395497  0.84415241 -1.21486659]\n[-1.46406795  0.55687648  1.76118173  0.85743896 -1.24027725]\n[-1.51353785  0.53217219  1.77100399  0.86915723 -1.25745585]\n[-1.55474869  0.50873305  1.77503385  0.87962813 -1.26781729]\n[-1.58900369  0.4867206   1.77463151  0.8890719  -1.27265526]\n[-1.61742724  0.46621157  1.77091752  0.8976355  -1.27311668]\n[-1.6409756   0.44722316  1.76479726  0.90541746 -1.2701912 ]\n[-1.66045459  0.42973107  1.75699074  0.91248714 -1.26471358]\n[-1.67653945  0.4136828   1.74806283  0.9188978  -1.25737455]\n[-1.68979423  0.39900764  1.73845126  0.92469464 -1.24873636]\n[-1.7006894   0.38562402  1.72849118  0.9299192  -1.23925005]\n[-1.70961713  0.37344497  1.71843577  0.93461151 -1.22927246]\n[-1.71690439  0.36238196  1.70847343  0.93881083 -1.21908203]\n[-1.72282384  0.35234764  1.69874154  0.9425557  -1.20889267]\n[-1.72760309  0.34325763  1.68933768  0.94588371 -1.19886582]\n[-1.73143222  0.3350316   1.68032849  0.94883113 -1.18912055]\n[-1.73447014  0.327594    1.67175674  0.95143261 -1.17974204]\n[-1.73684978  0.32087437  1.6636469   0.95372086 -1.17078857]\n[-1.73868244  0.31480739  1.65600958  0.95572658 -1.16229719]\n[-1.74006138  0.30933284  1.64884495  0.95747827 -1.15428843]\n[-1.74106475  0.3043954   1.64214551  0.95900226 -1.14677005]\n[-1.74175811  0.29994445  1.6358982   0.96032272 -1.13974006]\n[-1.74219649  0.29593375  1.63008604  0.96146173 -1.13318921]\n[-1.74242602  0.29232115  1.62468945  0.9624394  -1.12710291]\n[-1.74248544  0.28906833  1.61968724  0.96327395 -1.12146281]\n[-1.74240725  0.28614046  1.61505735  0.96398183 -1.11624799]\n[-1.74221868  0.28350593  1.61077745  0.96457789 -1.11143595]\n[-1.74194258  0.2811361   1.60682536  0.96507547 -1.10700338]\n[-1.74159806  0.279005    1.60317935  0.96548656 -1.10292668]\n[-1.74120108  0.27708916  1.59981843  0.96582187 -1.09918245]\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_3.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(0), [3;-1;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[0;0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all"
  },
  {
    "objectID": "Robotics/Inverse_kinematics.html#jacobian-pseudoinverse-and-redundancy",
    "href": "Robotics/Inverse_kinematics.html#jacobian-pseudoinverse-and-redundancy",
    "title": "Inverse Kinematics Simulation",
    "section": "Jacobian Pseudoinverse and Redundancy",
    "text": "Jacobian Pseudoinverse and Redundancy\nThis problem continues exploring the redundant snake robot used simulated above. So far we have left \\(\\normalsize ğ‘ = 0\\) in our Jacobian pseudoinverse. More generally, choosing \\(\\normalsize ğ‘\\) allows us to set a secondary objective for the inverse kinematics of redundant robots.\nHere we establish that numerical inverse kinematics finds a solution for \\(\\normalsize \\theta\\) such that \\(\\normalsize T_{sb}(\\theta)\\) equals the desired end-effector pose. But when working with redundant robots, multiple solutions are often possible. Choosing \\(\\normalsize ğ‘\\) affects which of these solutions the algorithm selects.\nNow I set \\(\\normalsize ğ‘\\) as the following vector (and update \\(\\normalsize ğ‘\\) as \\(\\normalsize \\theta_1\\) changes):\n\\[\\normalsize b = \\left(\\begin{array}{cc}\n-\\theta_1(0)\\\\\n0\\\\\n0\\\\\n0\\\\\n0\\\\\n\\end{array}\\right)\n\\]\nWhich is a \\(\\normalsize (6 * 1) \\, vector\\)\n\nEquation: \\(\\normalsize \\delta\\theta = pinv(J)*V + (I - pinv(J)*J)*b\\)\n\nHere we change the delta_theta by manipulating the pseudoinverse and introducing the null-space\nNote: \\(\\normalsize ğ‘\\) was a zero vector till now for all the three cases, but now we will notice the change for Case 3:\nCASE 3: Modified delta_theta for Redundancy\n\nImplementations\n\nPython\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\n\n# Define the necessary functions\ndef bracket3(S):\n    return np.array([[0, -S[2], S[1]], [S[2], 0, -S[0]], [-S[1], S[0], 0]])\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef adjointM(T):\n    R = T[0:3, 0:3]\n    p = T[0:3, 3]\n    return np.block([[R, np.zeros((3, 3))], [bracket3(p) @ R, R]])\n\ndef r2axisangle(R):\n    if np.linalg.norm(R - np.eye(3)) &lt; 1e-3:\n        return np.array([0, 0, 0])\n    else:\n        trace = np.clip((np.trace(R) - 1) / 2, -1, 1)  # Clip trace to the valid range\n        if trace &lt; -1:\n            trace = -1\n        elif trace &gt; 1:\n            trace = 1\n        theta = np.arccos(0.5*trace)\n        omega_hat = 1 / (2 * np.sin(theta)) * (R - R.T)\n        omega = np.array([omega_hat[2, 1], omega_hat[0, 2], omega_hat[1, 0]])\n        return omega * theta\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    theta = np.atleast_1d(theta)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\ndef JacS(S, theta):\n    T = np.eye(4)\n    Js = np.zeros((6, len(theta)))\n    for i in range(len(theta)):\n        Si = S[:, i]\n        Js[:, i] = np.dot(adjointM(T), Si)\n        T = np.dot(T, expm(bracket_s(Si) * theta[i]))\n    return Js\n\ndef rotz(angle):\n    c, s = np.cos(angle), np.sin(angle)\n    return np.array([[c, -s, 0], [s, c, 0], [0, 0, 1]])\n\n# Initialize plot\nfig, ax = plt.subplots()\nax.set_xlim(-6, 6)\nax.set_ylim(-6, 6)\nax.grid(True)\nline, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\nplt.ion()\n\n# Initialize parameters\nL = 1\ntheta = np.array([np.pi/8] * 5)\n\nS1 = np.array([0, 0, 1, 0, 0, 0])\nS2 = np.array([0, 0, 1, 0, -1 * L, 0])\nS3 = np.array([0, 0, 1, 0, -2 * L, 0])\nS4 = np.array([0, 0, 1, 0, -3 * L, 0])\nS5 = np.array([0, 0, 1, 0, -4 * L, 0])\n\n# Create S_eq by concatenating the individual S vectors horizontally\nS = np.column_stack((S1, S2, S3, S4, S5))\n\nM = np.vstack([np.hstack([np.eye(3), np.array([[5*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM1 = np.vstack([np.hstack([np.eye(3), np.array([[1*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM2 = np.vstack([np.hstack([np.eye(3), np.array([[2*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM3 = np.vstack([np.hstack([np.eye(3), np.array([[3*L, 0, 0]]).T]), [0, 0, 0, 1]])\nM4 = np.vstack([np.hstack([np.eye(3), np.array([[4*L, 0, 0]]).T]), [0, 0, 0, 1]])\n\n# Desired transformation\nT_d = np.vstack([np.hstack([rotz(0), np.array([[3, -1, 0]]).T]), [0, 0, 0, 1]])\nXd = np.concatenate([r2axisangle(T_d[0:3, 0:3]), T_d[0:3, 3]])\n\n# Initial forward kinematics\nT = fk(M, S, theta)\nX = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n# Animation function\ndef update(frame):\n    global theta, T, X\n    if np.linalg.norm(Xd - X) &gt; 1e-1:\n        # Calculate joint transformations\n        T1 = fk(M1, S[:, 0:1], theta[0])\n        T2 = fk(M2, S[:, 0:2], [theta[0], theta[1]])\n        T3 = fk(M3, S[:, 0:3], [theta[0], theta[1], theta[2]])\n        T4 = fk(M4, S[:, 0:4], [theta[0], theta[1], theta[2], theta[3]])\n        P_v = np.array([[0, 0]] + [T1[0:2, 3]]+ [T2[0:2, 3]]+ [T3[0:2, 3]]+ [T4[0:2, 3]] + [T[0:2, 3]]).T\n        \n        # Draw the robot\n        line.set_data(P_v[0, :], P_v[1, :])\n        \n        # Update Jacobians and compute delta_theta\n        JS = JacS(S, theta)\n        Jb = np.dot(adjointM(np.linalg.inv(T)), JS)\n        J_geometric = np.block([[T[0:3, 0:3], np.zeros((3, 3))], [np.zeros((3, 3)), T[0:3, 0:3]]]) @ Jb\n        V = Xd - X\n\n        # [del_theta = pinv(J)*V + (I - pinv(J)*J)*b] &lt;- Updated delta_theta to include the null-space with the specified b vector\n        delta_theta = np.dot(np.linalg.pinv(J_geometric), V) + np.dot((np.eye(5) - np.dot(np.linalg.pinv(J_geometric), J_geometric)), np.array([-theta[0], 0, 0, 0, 0]))\n        \n        # Update theta\n        theta += 0.1 * delta_theta\n        print(theta)\n        T = fk(M, S, theta)\n        X = np.concatenate([r2axisangle(T[0:3, 0:3]), T[0:3, 3]])\n\n    return line,\n\n# Create and save the animation\nani = animation.FuncAnimation(fig, update, frames=np.arange(100), interval=200, blit=False)\n\n# Save the animation\nwriter = PillowWriter(fps=7)  \nani.save(\"Inverse_Kinematics_Null.gif\", writer=writer)\n\n\n[-0.05825544  0.57353743  0.77725608  0.55724953 -0.05693715]\n[-0.33556013  0.64086137  1.01264325  0.63447309 -0.33163482]\n[-0.55265956  0.6656847   1.19866303  0.68949924 -0.55051267]\n[-0.72772164  0.6618604   1.35115636  0.73355974 -0.73223516]\n[-0.86867406  0.63683335  1.47639744  0.77174108 -0.88425018]\n[-0.98076153  0.59594891  1.57808655  0.80704634 -1.01096916]\n[-1.06833973  0.54347282  1.65917606  0.8414017  -1.11585497]\n[-1.1353063   0.48290628  1.72238131  0.87600273 -1.20200833]\n[-1.18514846  0.41710933  1.77026723  0.91149258 -1.27228264]\n[-1.2209048   0.34840453  1.80524701  0.9480471  -1.32935905]\n[-1.24502034  0.27912295  1.8300012   0.98468336 -1.37700902]\n[-1.25970342  0.21019292  1.84615904  1.02171894 -1.41627532]\n[-1.26680285  0.14229845  1.85516799  1.05926318 -1.44822922]\n[-1.26782221  0.07593138  1.85828783  1.09726722 -1.47392191]\n[-1.2639633   0.01142763  1.85659734  1.13558306 -1.49433971]\n[-1.25618019 -0.05100306  1.85100817  1.17401545 -1.510373  ]\n[-1.24523121 -0.11125002  1.84228215  1.21235971 -1.52280077]\n[-1.23172299 -0.16927717  1.83104994  1.25042495 -1.53228789]\n[-1.2161452  -0.22510185  1.81782933  1.2880459  -1.53939092]\n[-1.19889701 -0.27877757  1.80304255  1.32508688 -1.54456879]\n[-1.18030676 -0.3303806   1.7870319   1.36144127 -1.54819533]\n[-1.16064685 -0.38000008  1.77007342  1.39702856 -1.55057206]\n[-1.14014478 -0.42773089  1.75238878  1.4317905  -1.55193998]\n[-1.1189917  -0.47366884  1.73415522  1.46568709 -1.55249019]\n[-1.09734903 -0.51790738  1.71551396  1.49869293 -1.55237297]\n[-1.07535365 -0.56053566  1.69657713  1.530794   -1.55170554]\n[-1.0531221  -0.60163728  1.6774335   1.56198494 -1.55057842]\n[-1.0307539  -0.64128977  1.65815321  1.59226689 -1.54906086]\n[-1.00833432 -0.67956441  1.63879161  1.62164574 -1.54720524]\n[-0.98593665 -0.71652631  1.61939242  1.65013075 -1.54505065]\n[-0.96362401 -0.75223473  1.59999035  1.67773349 -1.54262592]\n[-0.9414509  -0.78674343  1.58061313  1.7044671  -1.53995205]\n[-0.91946448 -0.8201012   1.56128328  1.73034568 -1.53704415]\n[-0.89770558 -0.8523523   1.54201945  1.75538388 -1.5339131 ]\n[-0.87620958 -0.88353702  1.52283751  1.7795967  -1.53056685]\n[-0.8550071  -0.91369218  1.50375142  1.80299926 -1.52701146]\n[-0.83412458 -0.94285159  1.48477389  1.82560676 -1.52325189]\n[-0.81358477 -0.97104661  1.46591687  1.84743444 -1.5192927 ]\n[-0.79340707 -0.9983065   1.44719191  1.8684976  -1.51513849]\n[-0.77360792 -1.02465885  1.42861042  1.88881166 -1.51079429]\n[-0.75420097 -1.05012997  1.41018377  1.90839219 -1.50626577]\n[-0.73519738 -1.07474516  1.39192337  1.927255   -1.50155939]\n\n\n\n\nMATLAB\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-6, 6, -6, 6])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Inverse_Kinematics_3_null_space.mp4', 'MPEG-4');\nv.FrameRate = 25;\nopen(v);\n\n% initial joint values\nL = 1;\ntheta = [pi/8; pi/8; pi/8; pi/8; pi/8];\n\nS1 = [0 0 1 0 0 0]';\nS2 = [0 0 1 0 -1*L 0]';\nS3 = [0 0 1 0 -2*L 0]';\nS4 = [0 0 1 0 -3*L 0]';\nS5 = [0 0 1 0 -4*L 0]';\n\nS_eq = [S1, S2, S3, S4, S5];   \nM = [eye(3), [5*L;0;0]; 0 0 0 1];\nM1 = [eye(3), [1*L;0;0]; 0 0 0 1];\nM2 = [eye(3), [2*L;0;0]; 0 0 0 1];\nM3 = [eye(3), [3*L;0;0]; 0 0 0 1];\nM4 = [eye(3), [4*L;0;0]; 0 0 0 1];\n\n% Given desired Transformation matrices T_d\nT_d = [rotz(0), [3;-1;0]; 0 0 0 1];\nXd = [r2axisangle(T_d(1:3, 1:3)); T_d(1:3,4)];\n\n% T with initial joint positions\nT = fk(M, S_eq, theta);\nX = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\n\nwhile norm(Xd - X) &gt; 1e-2\n\n    p0 = [0; 0]; % plot the robot\n    T1 = fk(M1, S1, theta(1)); % 1. get the position of each link\n    T2 = fk(M2, [S1, S2], [theta(1), theta(2)]);\n    T3 = fk(M3, [S1, S2, S3], [theta(1), theta(2), theta(3)]);\n    T4 = fk(M4, [S1, S2, S3, S4], [theta(1), theta(2), theta(3), theta(4)]);\n    P_v = [p0, T1(1:2, 4), T2(1:2, 4), T3(1:2, 4), T4(1:2, 4), T(1:2, 4)];\n\n    cla; % 2. draw the robot and save the frame\n    plot(P_v(1,:), P_v(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v, frame); % My Implementation for inverse kinematics calculation below\n\n    JS = JacS(S_eq, theta); % Updated Space Jacobian\n    Jb = adjointM(inv(T))*JS; %Updated Body Jacobian\n    J = [T(1:3, 1:3) zeros(3); zeros(3) T(1:3, 1:3)] * Jb; % Updated Geometric Jacobian\n    V = Xd - X;\n\n    delta_theta = pinv(J)*V +(eye(5) - pinv(J)*J)*[-theta(1);0;0;0;0];\n\n    theta = double(theta + 0.1 * delta_theta); % Updating theta until the while loop is satisfied to get the desired joint positions\n    T = fk(M, S_eq, theta);\n    X = [r2axisangle(T(1:3, 1:3)); T(1:3,4)];\nend\n\nclose(v);\nclose all\n\n\n\nResult:\n\n\n\nThe final joint positions (with due approximation):\n\nCase 3 with \\(\\normalsize ğ‘ = 0\\):\n\\[\\normalsize \\theta = \\left(\\begin{array}{cc}\nâˆ’1.51\\\\\n0.29\\\\\n1.36\\\\\n0.78\\\\\nâˆ’0.92\n\\end{array}\\right)\n\\]\nCase 3 with \\(\\normalsize ğ‘ = -\\theta(1)\\):\n\\[\\normalsize \\theta = \\left(\\begin{array}{cc}\nâˆ’0.38\\\\\nâˆ’1.40\\\\\n1.20\\\\\n1.70\\\\\nâˆ’1.12\n\\end{array}\\right)\n\\]\n\nComparing these two results, we have that |\\(\\normalsize \\theta_1\\)| is smaller (and \\(\\normalsize \\theta_1\\) is closer to zero) with the secondary objective:\n\n\\(\\normalsize 0.38 &lt; 1.51\\)\n\nThere are several reasons why we may want to minimize a joint angle:\nâ€¢ The actuator at that joint moves more slowly than the other actuators.\n\nâ€¢ Moving the actuator at that joint consumes more power as compared to the other actuators along the robot arm.\n\nâ€¢ We want to avoid colliding with an obstacle, and we need to keep one or more joints at a specific angle to avoid that obstacle"
  },
  {
    "objectID": "Robotics/Control.html",
    "href": "Robotics/Control.html",
    "title": "Robot Control",
    "section": "",
    "text": "Now we are simulating the rigid system and compliant system in Simulink (Matlab). For both systems set \\(\\normalsize \\theta_ğ‘‘\\) as a Pulse Generator with amplitude 1 m, period 5 s, and pulse width 50%. The model stop time should be 50 s. Here we are simulating two separate plots.\nOne plot should show \\(\\normalsize \\theta_ğ‘‘, \\, \\theta_2\\) (for the rigid system) and ğœƒ2 (for the compliant system) when \\(\\normalsize ğ‘˜_ğ‘ = 10\\). The other plot should show \\(\\normalsize \\theta_ğ‘‘, \\, \\theta_2\\) (for the rigid system) and ğœƒ2 (for the compliant system) when \\(\\normalsize ğ‘˜_ğ‘ = 110\\), including labels and captions.\n\n\n\n\n\n\n\n\n\n\nClosed-loop behavior of both systems when \\(\\normalsize ğ‘˜_ğ‘ = 10\\).\n\n\n\n\n\n\n\nClosed-loop behavior of both systems when \\(\\normalsize ğ‘˜_ğ‘ = 110\\).\n\n\n\n\n\n\nDoes introducing compliance (e.g., the spring between the motor and link) make it easier or harder to control the robot?\nCompliance makes it harder to control the system. Our analysis shows that the added compliance restricts the range of ğ‘˜ğ‘ for which the system is stable. Physically, the added spring causes the second mass to oscillate, and the controllers we design must work to mitigate these oscillations."
  },
  {
    "objectID": "Robotics/Control.html#compliant-joints",
    "href": "Robotics/Control.html#compliant-joints",
    "title": "Robot Control",
    "section": "",
    "text": "Now we are simulating the rigid system and compliant system in Simulink (Matlab). For both systems set \\(\\normalsize \\theta_ğ‘‘\\) as a Pulse Generator with amplitude 1 m, period 5 s, and pulse width 50%. The model stop time should be 50 s. Here we are simulating two separate plots.\nOne plot should show \\(\\normalsize \\theta_ğ‘‘, \\, \\theta_2\\) (for the rigid system) and ğœƒ2 (for the compliant system) when \\(\\normalsize ğ‘˜_ğ‘ = 10\\). The other plot should show \\(\\normalsize \\theta_ğ‘‘, \\, \\theta_2\\) (for the rigid system) and ğœƒ2 (for the compliant system) when \\(\\normalsize ğ‘˜_ğ‘ = 110\\), including labels and captions.\n\n\n\n\n\n\n\n\n\n\nClosed-loop behavior of both systems when \\(\\normalsize ğ‘˜_ğ‘ = 10\\).\n\n\n\n\n\n\n\nClosed-loop behavior of both systems when \\(\\normalsize ğ‘˜_ğ‘ = 110\\).\n\n\n\n\n\n\nDoes introducing compliance (e.g., the spring between the motor and link) make it easier or harder to control the robot?\nCompliance makes it harder to control the system. Our analysis shows that the added compliance restricts the range of ğ‘˜ğ‘ for which the system is stable. Physically, the added spring causes the second mass to oscillate, and the controllers we design must work to mitigate these oscillations."
  },
  {
    "objectID": "Robotics/Control.html#multivariable-control",
    "href": "Robotics/Control.html#multivariable-control",
    "title": "Robot Control",
    "section": "Multivariable Control",
    "text": "Multivariable Control\n\n\n\n\n\nIn this problem, we will simulate control the robot shown above. We have already obtained the dynamics of this robot (which are the mass, coriolis matrices and the gravity vector) which is mentioned in the code below. The robot starts at joint position \\(\\normalsize \\theta = 0\\).\nHere \\(\\normalsize ğ¿ = 1, ğ‘š_1 = ğ‘š_2 = ğ‘š_3 = 1, and \\, ğ¼_3 = 0.1.\\)\nUsing the given simulation parameters and frame rates as required, we are modifing the code as needed so that the robot reaches for \\(\\normalsize \\theta_d\\) using multivariable PD control with gravity compensation:\n\\(\\normalsize \\tau = ğ¾_ğ‘ƒ(\\theta_ğ‘‘ âˆ’ \\theta) âˆ’ ğ¾_ğ· \\dot\\theta + ğ‘”(\\theta)\\)\n\nCASE 1: A simulation where \\(\\normalsize ğ¾_P = ğ¼\\) and \\(\\normalsize ğ¾_ğ· = ğ¼\\). Let the desired position be:\n\n\\[\\normalsize \\theta_d = \\left(\\begin{array}{cc}\n-2\\\\\n2\\\\\n\\pi/4\n\\end{array}\\right)\n\\]\n\nImplementation\n\n\nMATLAB Code\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-4, 4, -4, 4])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Multivariable_1.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nm1 = 1;\nm2 = 1;\nm3 = 1;\nI3 = 0.1;\nL = 1;\ng = 9.81;\ndeltaT = 0.01;\n\n% initial conditions\ntheta = [0; 0; 0];\nthetadot = [0; 0; 0];\nthetadotdot = [0; 0; 0];\ntime = 0;\n\n% forward kinematics to end-effector\nS1 = [0;0;0;1;0;0];\nS2 = [0;0;0;0;1;0];\nS3 = [0;0;1;0;-L;0];\nS = [S1, S2, S3];\nM3 = [eye(3), [2*L;0;0]; 0 0 0 1];\n\n% For CASE 1\nKp = eye(3);\nKd = eye(3);\n\nM0 = [eye(3), [L;0;0]; 0 0 0 1];\nM1 = [eye(3), [L;0;0]; 0 0 0 1];\nM2 = [eye(3), [L;0;0]; 0 0 0 1];\n\nfor idx = 1:1000\n\n    % get desired position\n    theta_d = [-2; 2; pi/4];\n    thetadot_d = [0; 0; 0];\n    T_d = fk(M3, [S1 S2 S3], theta_d);\n    \n    % plot the robot\n    p0 = [0; 0];\n    T1 = fk(M1, S1,theta(1:1,:));\n    p1 = T1(1:2,4);                         % position of end of link 1\n    T2 = fk(M2,[S1 S2],theta(1:2,:));\n    p2 = T2(1:2,4);                         % position of end of link 2\n    T3 = fk(M3,[S1 S2 S3],theta(1:3,:));\n    p3 = T3(1:2,4);                         % position of end of link 3\n    P = [p0, p1, p2, p3];\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n\n    % plot the desired position\n    plot(T_d(1,4), T_d(2,4), 'ok', 'MarkerFaceColor','k')\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n    \n    % Mass matrix\n    M = [m1 + m2 + m3, 0, -L*m3*sin(theta(3));\n            0, m2 + m3, L*m3*cos(theta(3));\n            -L*m3*sin(theta(3)), L*m3*cos(theta(3)), m3*L^2 + I3];\n\n    % Coriolis matrix\n    C = [0, 0, -L*thetadot(3)*m3*cos(theta(3));\n        0, 0, -L*thetadot(3)*m3*sin(theta(3));\n        0, 0,                 0];\n\n    % Gravity vector\n    G = [0; g*m2 + g*m3; L*g*m3*cos(theta(3))];\n    \n    % Choose your controller tau (Just for reference and knowledge)\n    e1 =    theta_d - theta;\n    e1dot = thetadot_d - thetadot;\n\n    % Reference from a journal and a book (Modern Robotics) \n    tau = Kp*(theta_d-theta) + Kd*(thetadot_d - thetadot) + G;\n    \n    % integrate to update velocity and position\n    thetadotdot = M \\ (tau - C*thetadot - G);\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n    time = time + deltaT;\n\nend\n\nclose(v);\nclose all\n\n\nResult:\n\n\nCASE 2: Reaching for the same \\(\\normalsize \\theta_d\\) as in the previous part, but now we are tuning \\(\\normalsize ğ¾_ğ‘ƒ\\) and \\(\\normalsize ğ¾_ğ·\\) to improve the robotâ€™s performance, thus making a simulation with our best performing gains.\n\n\\[\\normalsize \\theta_d = \\left(\\begin{array}{cc}\n-2\\\\\n2\\\\\n\\pi/4\n\\end{array}\\right)\n\\]\nAnd here we choose our gains to be:\n\\(\\normalsize K_p = eye(3)*25\\)\n\\(\\normalsize K_d = eye(3)*25\\)\n\n\n\nImplementation\n\n\nMATLAB Code\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-4, 4, -4, 4])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Multivariable_2.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nm1 = 1;\nm2 = 1;\nm3 = 1;\nI3 = 0.1;\nL = 1;\ng = 9.81;\ndeltaT = 0.01;\n\n% initial conditions\ntheta = [0; 0; 0];\nthetadot = [0; 0; 0];\nthetadotdot = [0; 0; 0];\ntime = 0;\n\n% forward kinematics to end-effector\nS1 = [0;0;0;1;0;0];\nS2 = [0;0;0;0;1;0];\nS3 = [0;0;1;0;-L;0];\nS = [S1, S2, S3];\nM3 = [eye(3), [2*L;0;0]; 0 0 0 1];\n\n% For CASE 2 (Our best gains that yields the best and the most stable result)\nKp = eye(3)*25;\nKd = eye(3)*25;\n\nM0 = [eye(3), [L;0;0]; 0 0 0 1];\nM1 = [eye(3), [L;0;0]; 0 0 0 1];\nM2 = [eye(3), [L;0;0]; 0 0 0 1];\n\nfor idx = 1:1000\n\n    % get desired position\n    theta_d = [-2; 2; pi/4];\n    thetadot_d = [0; 0; 0];\n    T_d = fk(M3, [S1 S2 S3], theta_d);\n    \n    % plot the robot\n    p0 = [0; 0];\n    T1 = fk(M1, S1,theta(1:1,:));\n    p1 = T1(1:2,4);                         % position of end of link 1\n    T2 = fk(M2,[S1 S2],theta(1:2,:));\n    p2 = T2(1:2,4);                         % position of end of link 2\n    T3 = fk(M3,[S1 S2 S3],theta(1:3,:));\n    p3 = T3(1:2,4);                         % position of end of link 3\n    P = [p0, p1, p2, p3];\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    % plot the desired position\n    plot(T_d(1,4), T_d(2,4), 'ok', 'MarkerFaceColor','k')\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n    \n    % mass matrix\n    M = [m1 + m2 + m3, 0, -L*m3*sin(theta(3));\n            0, m2 + m3, L*m3*cos(theta(3));\n            -L*m3*sin(theta(3)), L*m3*cos(theta(3)), m3*L^2 + I3];\n\n    % Coriolis matrix\n    C = [0, 0, -L*thetadot(3)*m3*cos(theta(3));\n        0, 0, -L*thetadot(3)*m3*sin(theta(3));\n        0, 0,                 0];\n\n    % Gravity vector\n    G = [0; g*m2 + g*m3; L*g*m3*cos(theta(3))];\n    \n    % Choose your controller tau (Just for reference and knowledge)\n    e1 =    theta_d - theta;\n    e1dot = thetadot_d - thetadot;\n\n    % Reference from a journal and a book (Modern Robotics) \n    tau = Kp*(theta_d-theta) + Kd*(thetadot_d - thetadot) + G;\n    \n    % integrate to update velocity and position\n    thetadotdot = M \\ (tau - C*thetadot - G);\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n    time = time + deltaT;\n\nend\n\nclose(v);\nclose all\n\n\nResult:\n\n\nCASE 3: Controlling the robot to move in a circle. Let ğ‘¡ be the simulation time (variable time in the code), and define the desired trajectory as:\n\n\\[\\normalsize \\theta_d = \\left(\\begin{array}{cc}\n2\\cos(0.5\\pi t)\\\\\n2\\sin(0.5\\pi t)\\\\\n\\pi/2\n\\end{array}\\right)\n\\]\n\\[\\normalsize \\dot\\theta_d = \\left(\\begin{array}{cc}\n-\\pi\\sin(0.5\\pi t)\\\\\n\\pi\\cos(0.5\\pi t)\\\\\n0\n\\end{array}\\right)\n\\]\nAnd here we still keep our best gains:\n\\(\\normalsize K_p = eye(3)*25\\)\n\\(\\normalsize K_d = eye(3)*25\\)\n\n\n\nImplementation\n\n\nMATLAB Code\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-4, 4, -4, 4])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Multivariable_3.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nm1 = 1;\nm2 = 1;\nm3 = 1;\nI3 = 0.1;\nL = 1;\ng = 9.81;\ndeltaT = 0.01;\n\n% initial conditions\ntheta = [0; 0; 0];\nthetadot = [0; 0; 0];\nthetadotdot = [0; 0; 0];\ntime = 0;\n\n% forward kinematics to end-effector\nS1 = [0;0;0;1;0;0];\nS2 = [0;0;0;0;1;0];\nS3 = [0;0;1;0;-L;0];\nS = [S1, S2, S3];\nM3 = [eye(3), [2*L;0;0]; 0 0 0 1];\n\n% For CASE 3\nKp = eye(3)*25;\nKd = eye(3)*25;\n\nM0 = [eye(3), [L;0;0]; 0 0 0 1];\nM1 = [eye(3), [L;0;0]; 0 0 0 1];\nM2 = [eye(3), [L;0;0]; 0 0 0 1];\n\nfor idx = 1:1000\n\n    % get desired position\n    theta_d = [2*cos(pi*time/2); 2*sin(pi*time/2); pi/2];\n    thetadot_d = [-pi*sin(pi*time/2); pi*cos(pi*time/2); 0];\n    T_d = fk(M3, [S1 S2 S3], theta_d);\n    \n    % plot the robot\n    p0 = [0; 0];\n    T1 = fk(M1, S1,theta(1:1,:));\n    p1 = T1(1:2,4);                         % position of end of link 1\n    T2 = fk(M2,[S1 S2],theta(1:2,:));\n    p2 = T2(1:2,4);                         % position of end of link 2\n    T3 = fk(M3,[S1 S2 S3],theta(1:3,:));\n    p3 = T3(1:2,4);                         % position of end of link 3\n    P = [p0, p1, p2, p3];\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4)\n    % plot the desired position\n    plot(T_d(1,4), T_d(2,4), 'ok', 'MarkerFaceColor','k')\n    drawnow\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n    \n    % mass matrix\n    M = [m1 + m2 + m3, 0, -L*m3*sin(theta(3));\n            0, m2 + m3, L*m3*cos(theta(3));\n            -L*m3*sin(theta(3)), L*m3*cos(theta(3)), m3*L^2 + I3];\n\n    % Coriolis matrix\n    C = [0, 0, -L*thetadot(3)*m3*cos(theta(3));\n        0, 0, -L*thetadot(3)*m3*sin(theta(3));\n        0, 0,                 0];\n\n    % Gravity vector\n    G = [0; g*m2 + g*m3; L*g*m3*cos(theta(3))];\n    \n    % Choose your controller tau (Just for reference and knowledge)\n    e1 =    theta_d - theta;\n    e1dot = thetadot_d - thetadot;\n\n    % Reference from a journal and a book (Modern Robotics) \n    tau = Kp*(theta_d-theta) + Kd*(thetadot_d - thetadot) + G;\n    \n    % integrate to update velocity and position\n    thetadotdot = M \\ (tau - C*thetadot - G);\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n    time = time + deltaT;\n\nend\n\nclose(v);\nclose all\n\n\nResult:"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "In the past, I lost track of reality trying to track a gazillion links covering every data-science-friendly programming language under the sun. **shakes head** Bad idea. Since I program in MATLAB and python daily, I like to keep track of Python and VStudio developments. Iâ€™m mostly going to share python and robotics/control dynamics resources that I find useful for analytics, statistical programming, machine learning, data science workflows, robot modelling and web app development. Iâ€™m enjoying MATLAB a lot more recently so Iâ€™ll slowly build up this resources page with MATLAB sub-topics that I find bookmark worthy.\nIn terms of the best place to start for getting into data analysis, I recommend learning SQL as this is by far the most widely used data querying language across the corporate and academic landscapes and if you master SQL, youâ€™ve mastered most of the transformations that are possible for tabular numeric data sets. Nonetheless, I will not cover SQL resources here as I rarely write raw SQL anymore. Instead, I use R to establish data warehouse connections and I query that raw data using the common tidyverse collection of R packages to execute SQL code in the back-end (via the dbplyr package).\nPython and R are open-source programming languages for statistical computing and graphics. These two languages have friendly online (and in-person) communities devoted to making data science easier to consume, easier to apply, and more effective at solving business problems. One of the things that I like most about both languages is the thousands of packages available making almost everything in R or Python a little easier from ETL, to method chaining, to developing predictive models and interactive web apps. I certainly welcome any suggestions that you might have for the lists below!"
  },
  {
    "objectID": "resources.html#language-agnostic-etl-frameworks",
    "href": "resources.html#language-agnostic-etl-frameworks",
    "title": "Resources",
    "section": "Language Agnostic ETL Frameworks",
    "text": "Language Agnostic ETL Frameworks\n\nArrow: Apache Arrow is a columnar memory format for flat and hierarchical data, organized for efficient analytic operations, supporting zero-copy reads for lightning-fast data access without serialization overhead\nDuckDB: DuckDB is an in-process SQL OLAP database management system (that plays nicely with Arrow) capable of larger than memory processing of tabular data\nPolars: Polars is a lightning fast DataFrame library/in-memory query engine written in Rust and built upon the Arrow specification - Itâ€™s a great tool for efficient data wrangling, data pipelines, snappy APIs and much more\nSpark: Apache Spark is a multi-language engine for executing data engineering, data science, and machine learning on single-node machines or clusters"
  },
  {
    "objectID": "resources.html#python-books",
    "href": "resources.html#python-books",
    "title": "Resources",
    "section": "Python Books",
    "text": "Python Books\n\nThe Quick Python Book (3e): This book by Naomi Ceder is a few years old now (2018) but itâ€™s the best end-to-end intro on Python that Iâ€™ve yet read taking you from basic classes / structures to function writing to working with modules\nPython Data Science Handbook: Introduction to the core libraries essential for working with data in Python\nEffective Pandas: Patterns for Data Manipulation: Easy to follow tutorials, at your own pace, for mastering the popular Pandas library\nTidy Finance with Python: This is one of my favorite newer books covering complex financial modeling, valuation, and pricing and represents â€œan opinionated approach to empirical research in financial economics [with an] open-source code baseâ€"
  },
  {
    "objectID": "resources.html#python-packages",
    "href": "resources.html#python-packages",
    "title": "Resources",
    "section": "Python Packages",
    "text": "Python Packages\n\nNumPy: Brings the computational power of C and Fortran to Python programmers for applying high-level mathematical functions to arrays and more\nPandas: This is the most popular package for data manipulation and analysis with extended operations available for tabular and time series data\nMatplotlib: A comprehensive library for creating static, animated, and interactive visualizations in Python\nscikit-learn: Built on top of NumPy, SciPy, and matplotlib, â€œsklearnâ€ makes the development of predictive analysis workflows a simple and reproducible process\nBeautiful Soup: The beautifulsoup4 library makes web scraping HTML and XML data a breeze\nStreamlit: Using pure Python, this package lets you build interactive web apps in minutes with no UI / front-end experience required\nShiny for Python: The popular Shiny framework for R is finally available for Python - Create highly interactive visualizations, realtime dashboards, data explorers, model demos, sophisticated workflow apps, and anything else you can imagineâ€”all in pure Python, with no web development skills required"
  },
  {
    "objectID": "resources.html#r-books-applied-resources",
    "href": "resources.html#r-books-applied-resources",
    "title": "Resources",
    "section": "R Books: Applied Resources",
    "text": "R Books: Applied Resources\n\nTidy Modeling with R: Over the last few months, Iâ€™ve learned a lot from this A to Z resource on predictive modeling workflows using the tidymodels framework\nDeep Learning with R (2e): In-depth introduction to artificial intelligence and deep learning applications with R using the Keras library\nForecasting Principles and Practice (3e): Said best by the author, â€œThe book is written for three audiences: (1) people finding themselves doing forecasting in business when they may not have had any formal training in the area; (2) undergraduate students studying business; (3) MBA students doing a forecasting electiveâ€\nRegression and Other Studies: Super applied textbook on advanced regression techniques, Bayesian inference, and causal inference\nSupervised Machine Learning for Text Analysis in R: Written by two Posit software engineers, Emil Hvitfeldt and Julia Silge, this book is a masterclass in natural language processing taking you from the basics of NLP to real-life applications including inference and prediction\nTidy Finance with R: This is one of my favorite newer books covering complex financial modeling, valuation, and pricing and represents â€œan opinionated approach to empirical research in financial economics [with an] open-source code base in multiple programming languagesâ€"
  },
  {
    "objectID": "resources.html#r-packages",
    "href": "resources.html#r-packages",
    "title": "Resources",
    "section": "R Packages",
    "text": "R Packages\n\ntidyverse: A collection of packages for data manipulation and functional programming (I use dplyr, stringr, and purrr on a daily basis)\ntidymodels: Hands-down my preferred collection of packages for building reproducible machine learning recipes, workflows, model tuning, model stacking, and cross-validation\ntidyverts: A collection of packages for time series analysis that comes out of Rob Hyndmanâ€™s lab\nDT: This is an R implementation of the popular DataTables JavaScript library that lets you build polished, configurable tables for use in web reports, slides, and Shiny apps\nbs4Dash: This R Shiny framework brings Bootstrap + AdminLTE dependencies to Shiny (including 1:1 support for shinydashboard functions) and itâ€™s my go-to for developing enterprise-grade Shiny apps\nleaflet: R implementation of the popular Leaflet JavaScript library for developing interactive maps\nplotly: An extensive graphic library for creating interactive visualizations and 3D (WebGL) charts\nembed: This package is one of my go-to packages for machine learning and I if Iâ€™m working on a classification problem, you can count on me incorporating some of the extra steps it provides for the recipes package for embedding predictors into one or more numeric columns"
  },
  {
    "objectID": "posts/Random Processes/index.html",
    "href": "posts/Random Processes/index.html",
    "title": "Probability Theory and Random Processes",
    "section": "",
    "text": "In this blog post I will discuss a few examples of probability in machine learning. If you are new to probability, I recommend one of great textbooks that cover the topic and are available for free online, such as Think Bayes by Allen Downey and Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu.\nClassification algorithms algorithms can estimate \\(n \\times k\\) class membership probabilities for each dataset, where n is the number of data points in the dataset and k is the number of classes in the training dataset. Similarly, the Gaussian Mixtures clustering algorithm can generate \\(n \\times k\\) cluster label probabilities.\nBesides a data point and the Gaussian Mixtures models can estimate cluster membership probability. point , especially Logistic Regression and Naive Bayes. Every classification algorithm can estimate probabilities of belonging to each class.\n\\(\\Huge P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#gaining-insights-on-weather-data-with-data-science-and-machine-learning",
    "href": "posts/Random Processes/index.html#gaining-insights-on-weather-data-with-data-science-and-machine-learning",
    "title": "Probability Theory and Random Processes",
    "section": "",
    "text": "In this blog post I will discuss a few examples of probability in machine learning. If you are new to probability, I recommend one of great textbooks that cover the topic and are available for free online, such as Think Bayes by Allen Downey and Bayes Rules! by Alicia A. Johnson, Miles Q. Ott, and Mine Dogucu.\nClassification algorithms algorithms can estimate \\(n \\times k\\) class membership probabilities for each dataset, where n is the number of data points in the dataset and k is the number of classes in the training dataset. Similarly, the Gaussian Mixtures clustering algorithm can generate \\(n \\times k\\) cluster label probabilities.\nBesides a data point and the Gaussian Mixtures models can estimate cluster membership probability. point , especially Logistic Regression and Naive Bayes. Every classification algorithm can estimate probabilities of belonging to each class.\n\\(\\Huge P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#understanding-weather-forecasting-with-the-help-of-probability-theory-and-random-processes",
    "href": "posts/Random Processes/index.html#understanding-weather-forecasting-with-the-help-of-probability-theory-and-random-processes",
    "title": "Probability Theory and Random Processes",
    "section": "Understanding Weather Forecasting with the help of Probability theory and Random Processes",
    "text": "Understanding Weather Forecasting with the help of Probability theory and Random Processes\nIntroduction\nMachine learning plays a pivotal role in understanding and predicting various natural phenomena, and weather forecasting is no exception. To harness the power of machine learning for weather data analysis, it is essential to have a solid foundation in random processes and probability theory. These fundamental concepts are the building blocks that enable us to model the inherent uncertainty and variability present in weather data.\nWeather data, such as temperature, humidity, wind speed, and precipitation, exhibit random behavior due to the complex interplay of atmospheric processes. Random processes are mathematical models used to describe the evolution of these variables over time. These processes capture the idea that weather conditions are not deterministic but rather stochastic, influenced by a multitude of factors, including geographical location, time of year, and local phenomena.\nProbability theory, on the other hand, provides the framework to quantify and reason about uncertainty in weather data. It allows us to assign probabilities to different weather outcomes and make informed predictions based on observed data. For example, we can calculate the probability of rain on a given day or estimate the likelihood of extreme weather events, such as hurricanes or heatwaves, occurring in a specific region.\nMachine learning techniques, such as regression, classification, and time series analysis, heavily rely on probabilistic and random process models to extract meaningful insights from weather data. By incorporating these techniques, we can build predictive models that not only provide accurate weather forecasts but also account for uncertainty, enabling better decision-making in various applications like agriculture, transportation, and disaster management.\nIn this context, the weather dataset you are using serves as a valuable source of information for exploring and applying these concepts. By understanding random processes and probability theory, you can leverage machine learning to unlock the potential hidden within weather data, improving the accuracy and reliability of weather forecasts and facilitating data-driven decision-making in various sectors that rely on weather information.\nData Loading and Basic Visualization\n\n# Importing libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import expon\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Load the dataset\nweather_df = pd.read_csv('weather_data.csv')\nweather_df['Date'] = pd.to_datetime(weather_df['Date'])\n\nExploratory Data Analysis\n\n#Exploratory Data Analysis\n#Histograms and KDE (Kernel Density Estimation) plots for Temperature and Humidity.\nsns.histplot(weather_df['Temperature'], kde=True, color='blue', label='Temperature')\nsns.histplot(weather_df['Humidity'], kde=True, color='green', label='Humidity', alpha=0.5)\nplt.legend()\nplt.show()\n\n\n\n\n\n# Pair Plot to visualize all variables together.\nsns.pairplot(weather_df, hue='Weather_Condition')\nplt.show()\n\n\n\n\nProbability Distributions\n\n#Normal Distribution Fit for Temperature.\nsns.histplot(weather_df['Temperature'], kde=False, color='blue', label='Temperature')\n\n# Fitting a normal distribution and plotting it\nmean, std = norm.fit(weather_df['Temperature'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = norm.pdf(x, mean, std)\nplt.plot(x, p * max(weather_df['Temperature'].value_counts()), 'k', linewidth=2)\n\ntitle = \"Fit results: mean = %.2f,  std = %.2f\" % (mean, std)\nplt.title(title)\n\nplt.show()\n\n\n\n\n\n#Exponential Distribution Fit for Humidity.\nfrom scipy.stats import expon\n\n# Plotting histogram\nsns.histplot(weather_df['Humidity'], kde=False, color='green', label='Humidity')\n\n# Fitting an exponential distribution and plotting it\nparams = expon.fit(weather_df['Humidity'])\nxmin, xmax = plt.xlim()\nx = np.linspace(xmin, xmax, 100)\np = expon.pdf(x, *params)\nplt.plot(x, p * max(weather_df['Humidity'].value_counts()), 'k', linewidth=2)\n\nplt.show()\n\n\n\n\nTime Series Analysis\nTemperature Trend over Time.\n\n#Checking the temperature trned against time\nweather_df.set_index('Date')['Temperature'].plot()\nplt.title(\"Temperature Trend Over Time\")\nplt.ylabel(\"Temperature\")\nplt.show()\n\n\n\n\nProbability Theory in Action\n\n#Conditional Probability: Probability of High Humidity given Rainy Weather.\nhigh_humidity = weather_df['Humidity'] &gt; 80\nrainy_days = weather_df['Weather_Condition'] == 'Rainy'\nprob_high_humidity_given_rain = np.mean(high_humidity[rainy_days])\nprint(f\"Probability of High Humidity given Rainy Weather: {prob_high_humidity_given_rain}\")\n\nProbability of High Humidity given Rainy Weather: 0.27586206896551724\n\n\n\n#Joint Distribution: Temperature and Humidity.\nsns.jointplot(data=weather_df, x='Temperature', y='Humidity', kind='kde', color='red')\nplt.show()\n\n\n\n\nCorrelation Analysis\n\n#Correlation Heatmap\n# Selecting only numerical columns for correlation analysis\nnumerical_weather_df = weather_df.select_dtypes(include=[np.number])\n\n# Plotting the correlation heatmap\nsns.heatmap(numerical_weather_df.corr(), annot=True, cmap='coolwarm')\nplt.title(\"Correlation Heatmap\")\nplt.show()\n\n\n\n\nLinear Regression for Temperature Prediction\n\n#Model Training and Evaluation.\n# Preparing data for linear regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n# Preparing data for linear regression\nX = weather_df[['Humidity']]\ny = weather_df['Temperature']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n\n# Training the model\nmodel = LinearRegression()\nmodel.fit(X_train, y_train)\n\n# Making predictions\ny_pred = model.predict(X_test)\n\n# Plotting actual vs predicted values\nplt.scatter(X_test, y_test, color='blue', label='Actual')\nplt.scatter(X_test, y_pred, color='red', label='Predicted')\nplt.xlabel('Humidity')\nplt.ylabel('Temperature')\nplt.title('Actual vs Predicted Temperature')\nplt.legend()\nplt.show()\n\n# Model evaluation\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Mean Squared Error:\", mse)\n\n\n\n\nMean Squared Error: 24.010797366937965\n\n\nMarkov Chain for Weather Condition Transitions\n\n#Let's simulate a simple Markov chain to model the transitions between different weather conditions.\nimport pandas as pd\n\n# Calculating transition probabilities\nweather_conditions = weather_df['Weather_Condition'].unique()\ntransition_matrix = pd.DataFrame(index=weather_conditions, columns=weather_conditions).fillna(0)\n\nfor (prev, curr) in zip(weather_df['Weather_Condition'], weather_df['Weather_Condition'][1:]):\n    transition_matrix.at[prev, curr] += 1\n\n# Normalizing the rows to sum to 1\ntransition_matrix = transition_matrix.div(transition_matrix.sum(axis=1), axis=0)\n\n# Display the transition matrix\nprint(transition_matrix)\n\n           Windy     Snowy    Cloudy     Foggy     Sunny     Rainy\nWindy   0.155116  0.165017  0.161716  0.194719  0.181518  0.141914\nSnowy   0.196667  0.166667  0.153333  0.166667  0.156667  0.160000\nCloudy  0.144543  0.182891  0.171091  0.165192  0.188791  0.147493\nFoggy   0.199346  0.124183  0.199346  0.147059  0.140523  0.189542\nSunny   0.167247  0.167247  0.222997  0.167247  0.128920  0.146341\nRainy   0.131488  0.179931  0.211073  0.166090  0.141869  0.169550\n\n\nThis code calculates the probabilities of transitioning from one weather condition to another. Itâ€™s a basic form of a Markov chain.\nMonte Carlo Simulation for Temperature Extremes\n\n#Use Monte Carlo simulation to estimate the probability of extreme temperature events.\nnp.random.seed(0)\nnum_simulations = 10000\nextreme_temp_count = 0\nextreme_temp_threshold = 30  # Define what you consider as extreme temperature\n\nfor _ in range(num_simulations):\n    simulated_temp = np.random.choice(weather_df['Temperature'])\n    if simulated_temp &gt; extreme_temp_threshold:\n        extreme_temp_count += 1\n\nprobability_of_extreme_temp = extreme_temp_count / num_simulations\nprint(f\"Probability of Extreme Temperature (&gt; {extreme_temp_threshold}Â°C): {probability_of_extreme_temp}\")\n\nProbability of Extreme Temperature (&gt; 30Â°C): 0.0226\n\n\nThis Monte Carlo simulation randomly samples temperatures from the dataset and calculates the probability of encountering temperatures above a certain threshold."
  },
  {
    "objectID": "posts/Random Processes/index.html#step-1.-write-out-the-linear-regression-equation",
    "href": "posts/Random Processes/index.html#step-1.-write-out-the-linear-regression-equation",
    "title": "Probability Theory and Random Processes",
    "section": "Step 1. Write out the linear regression equation",
    "text": "Step 1. Write out the linear regression equation\n\\(\\Huge y=\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-2.-the-logistic-regression-equation-is-the-same-as-above-except-output-is-log-odds",
    "href": "posts/Random Processes/index.html#step-2.-the-logistic-regression-equation-is-the-same-as-above-except-output-is-log-odds",
    "title": "Probability Theory and Random Processes",
    "section": "Step 2. The logistic regression equation is the same as above except output is log odds",
    "text": "Step 2. The logistic regression equation is the same as above except output is log odds\n\\(\\Huge log(odds)=\\beta_0+\\beta_1 x_1+...+\\beta_n x_n\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-3.-exponentiate-both-sides-of-the-logistic-regression-equation-to-get-odds",
    "href": "posts/Random Processes/index.html#step-3.-exponentiate-both-sides-of-the-logistic-regression-equation-to-get-odds",
    "title": "Probability Theory and Random Processes",
    "section": "Step 3. Exponentiate both sides of the logistic regression equation to get odds",
    "text": "Step 3. Exponentiate both sides of the logistic regression equation to get odds\n\\(\\Huge odds=e^{\\beta_0+\\beta_1 x_1+...+\\beta_n x_n}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-4.-write-out-the-probability-equation",
    "href": "posts/Random Processes/index.html#step-4.-write-out-the-probability-equation",
    "title": "Probability Theory and Random Processes",
    "section": "Step 4. Write out the probability equation",
    "text": "Step 4. Write out the probability equation\n\\(\\Huge p=\\frac{odds}{1+odds}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-5.-plug-odds-from-step-3-into-the-probability-equation",
    "href": "posts/Random Processes/index.html#step-5.-plug-odds-from-step-3-into-the-probability-equation",
    "title": "Probability Theory and Random Processes",
    "section": "Step 5. Plug odds (from step 3) into the probability equation",
    "text": "Step 5. Plug odds (from step 3) into the probability equation\n\\(\\Huge p=\\frac{e^{\\beta_0+\\beta_1 x_1+...+\\beta_n x_n}}{1+e^{\\beta_0+\\beta_1 x_1+...+\\beta_n x_n}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#step-6.-divide-the-numerator-and-denominator-by-the-odds-from-step-3",
    "href": "posts/Random Processes/index.html#step-6.-divide-the-numerator-and-denominator-by-the-odds-from-step-3",
    "title": "Probability Theory and Random Processes",
    "section": "Step 6. Divide the numerator and denominator by the odds (from step 3)",
    "text": "Step 6. Divide the numerator and denominator by the odds (from step 3)\n\\(\\Huge p=\\frac{1}{1+e^{-(\\beta_0+\\beta_1 x_1+...+\\beta_n x_n)}}\\)\n\\(\\Huge P(A\\vert B)={\\frac {P(B\\vert A)P(A)}{P(B)}}\\)"
  },
  {
    "objectID": "posts/Random Processes/index.html#conclusion",
    "href": "posts/Random Processes/index.html#conclusion",
    "title": "Probability Theory and Random Processes",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis shows more sophisticated ways of applying probability theory and random processes to the weather dataset, providing insights into weather patterns and temperature predictions.\nSummarized the findings from the above analysis. Discussed the relevance of these probabilistic models in understanding weather patterns."
  },
  {
    "objectID": "posts/Clustering/index.html",
    "href": "posts/Clustering/index.html",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "",
    "text": "Employing sophisticated clustering techniques with retail customer data and comparing further with prediction models\nClustering is a fundamental technique in machine learning that involves grouping data points so that the objects in the same group (or cluster) are more similar to each other than to those in other groups. Itâ€™s a form of unsupervised learning, as the groups are not predefined but rather determined by the algorithm itself. This approach is particularly useful in understanding the structure within data, identifying patterns, and making strategic decisions.\nIn this blog, we will explore how to apply clustering techniques to a customer dataset. Our dataset contains customer information with attributes like Customer ID, Age, Annual Income, and Spending Score. The goal is to segment customers into distinct groups based on these features, which can help in tailoring marketing strategies, understanding customer behavior, and improving customer service."
  },
  {
    "objectID": "posts/Clustering/index.html#introduction",
    "href": "posts/Clustering/index.html#introduction",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Introduction",
    "text": "Introduction\nData Loading and Basic Visualization\n\n# Importing libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler\n\n# Load the dataset\ncustomer_df = pd.read_csv('clusteringCustomerData.csv')\nprint(customer_df.head())\n\n   CustomerID  Age  Annual_Income  Spending_Score\n0           1   62             75              81\n1           2   65             76              23\n2           3   18             98              80\n3           4   21             48              90\n4           5   21             47               9\n\n\nExploring Data Analysis\n\n#Visualize the distributions and relationships between features.\nsns.pairplot(customer_df.drop('CustomerID', axis=1))\nplt.show()\n\n\n\n\nData Preprocessing\n\n#Encoding categorical data and scaling features.\n#Standardize the data.\n# Selecting features to be scaled\nfeatures = customer_df[['Age', 'Annual_Income', 'Spending_Score']]\n\n# Standardizing the features\nscaler = StandardScaler()\nscaled_features = scaler.fit_transform(features)\n\n# Converting scaled features back to a DataFrame\nscaled_features_df = pd.DataFrame(scaled_features, columns=features.columns)\n\n# Displaying the first few rows of the scaled data\nprint(scaled_features_df.head())\n\n        Age  Annual_Income  Spending_Score\n0  1.237684       0.746278        0.901945\n1  1.428146       0.788416       -1.020966\n2 -1.555756       1.715469        0.868791\n3 -1.365294      -0.391469        1.200327\n4 -1.365294      -0.433608       -1.485117\n\n\nExploratory Data Analysis (EDA)\nBeyond basic visualizations, weâ€™ll use EDA to understand the data distributions and potential relationships.\n\n# Visualizing the distribution of features\nplt.figure(figsize=(12, 4))\nfor i, col in enumerate(['Age', 'Annual_Income', 'Spending_Score']):\n    plt.subplot(1, 3, i+1)\n    sns.histplot(customer_df[col], kde=True)\n    plt.title(f'Distribution of {col}')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#advanced-clustering-with-multiple-techniques",
    "href": "posts/Clustering/index.html#advanced-clustering-with-multiple-techniques",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Advanced Clustering with Multiple Techniques",
    "text": "Advanced Clustering with Multiple Techniques\nWeâ€™ll explore different clustering algorithms beyond K-Means, such as Hierarchical Clustering and DBSCAN, to understand how they segment the data differently.\n1. K-Means Clustering\n\n#Using the Elbow Method to determine the optimal number of clusters.\nwcss = []\nfor i in range(1, 11):\n    kmeans = KMeans(n_clusters=i, n_init=10, random_state=42)\n    kmeans.fit(scaled_features)\n    wcss.append(kmeans.inertia_)\n    customer_df['KMeans_Cluster'] = kmeans.labels_\nplt.plot(range(1, 11), wcss)\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()\n\n\n\n\nApplication of K-Means\n\n#Applying K-Means and visualizing the clusters.\nkmeans = KMeans(n_clusters=5, n_init=10, random_state=42)\ncustomer_df['Cluster'] = kmeans.fit_predict(scaled_features)\nsns.scatterplot(x='Annual_Income', y='Spending_Score', hue='Cluster', data=customer_df, palette='viridis')\nplt.title('Customer Segments')\nplt.show()\n\n\n\n\nK-Means Visualization\n\nsns.pairplot(customer_df, vars=['Age', 'Annual_Income', 'Spending_Score'], hue='KMeans_Cluster', palette='viridis')\nplt.suptitle('K-Means Clustering', y=1.02)\nplt.show()\n\n\n\n\n2. DBSCAN clustering\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN clustering\ndbscan = DBSCAN(eps=0.5, min_samples=5).fit(scaled_features_df)\ncustomer_df['DBSCAN_Cluster'] = dbscan.labels_\n\nDensity-Based Spatial Clustering of Applications with Noise (DBSCAN) Visualization\n\n# Visualizing DBSCAN Clusters\nsns.scatterplot(x='Annual_Income', y='Spending_Score', hue='DBSCAN_Cluster', data=customer_df, palette='viridis')\nplt.title('DBSCAN Clustering')\nplt.show()\n\n\n\n\n3. Hierarchical Cluster Visualization\nVisualize the clusters formed by each algorithm in multiple dimensions to gain deeper insights.\n\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nfrom sklearn.cluster import AgglomerativeClustering  # Import AgglomerativeClustering\n\n# Performing Hierarchical Clustering\nlinked = linkage(scaled_features_df, method='ward')\n\n# Plotting the Dendrogram\nplt.figure(figsize=(10, 7))\ndendrogram(linked, orientation='top', distance_sort='descending', show_leaf_counts=True)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n\n# Perform hierarchical clustering and add the cluster labels to customer_df\nclustering = AgglomerativeClustering(n_clusters=3)  # Specify the number of clusters\ncustomer_df['Hierarchical_Cluster'] = clustering.fit_predict(scaled_features_df)\n\n# Hierarchical Clustering Visualization\nsns.pairplot(customer_df, vars=['Age', 'Annual_Income', 'Spending_Score'], hue='Hierarchical_Cluster', palette='viridis')\nplt.suptitle('Hierarchical Clustering', y=1.02)\nplt.show()\n\n\n\n\n\n\n\nHierarchical Clustering Cut\n\nfrom scipy.cluster.hierarchy import fcluster\n\n# Cutting the Dendrogram to form clusters\ncustomer_df['Hierarchical_Cluster'] = fcluster(linked, t=3, criterion='maxclust')\n\nsns.scatterplot(data=customer_df, x='Annual_Income', y='Spending_Score', hue='Hierarchical_Cluster', palette='viridis')\nplt.title('Hierarchical Clustering')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#silhouette-analysis-for-k-means-and-hierarchical-clustering",
    "href": "posts/Clustering/index.html#silhouette-analysis-for-k-means-and-hierarchical-clustering",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Silhouette Analysis for K-means and Hierarchical Clustering",
    "text": "Silhouette Analysis for K-means and Hierarchical Clustering\n1. K-Means Clustering\n\nfrom sklearn.metrics import silhouette_samples, silhouette_score\nimport numpy as np\n\n# Calculate silhouette scores for different cluster numbers\nsilhouette_scores = []\nfor n_clusters in range(2, 11):\n    kmeans = KMeans(n_clusters=n_clusters, n_init=10, random_state=42)\n    cluster_labels = kmeans.fit_predict(scaled_features)\n    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores.append(silhouette_avg)\n\n# Plot silhouette scores\nplt.plot(range(2, 11), silhouette_scores, marker='o')\nplt.title('Silhouette Analysis for K-Means')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()\n\n\n\n\n2. Hierarchical Clustering\n\nfrom sklearn.cluster import AgglomerativeClustering\n\n# Calculate silhouette scores for different cluster numbers in hierarchical clustering\nsilhouette_scores_hierarchical = []\nfor n_clusters in range(2, 11):\n    hierarchical = AgglomerativeClustering(n_clusters=n_clusters)\n    cluster_labels = hierarchical.fit_predict(scaled_features)\n    silhouette_avg = silhouette_score(scaled_features, cluster_labels)\n    silhouette_scores_hierarchical.append(silhouette_avg)\n\n# Plot silhouette scores for hierarchical clustering\nplt.plot(range(2, 11), silhouette_scores_hierarchical, marker='o')\nplt.title('Silhouette Analysis for Hierarchical Clustering')\nplt.xlabel('Number of clusters')\nplt.ylabel('Silhouette Score')\nplt.show()"
  },
  {
    "objectID": "posts/Clustering/index.html#conclusion",
    "href": "posts/Clustering/index.html#conclusion",
    "title": "Advanced Clustering and Prediction techniques",
    "section": "Conclusion",
    "text": "Conclusion\n#In this in-depth analysis, we have explored different clustering techniques and visualized their results to segment customers in a comprehensive manner. Each method offers unique insights: K-Means provides clear segmentations, Hierarchical Clustering helps us understand the data structure, and DBSCAN identifies core and outlier points effectively.\nBy comparing these methods, we can choose the one that best suits our specific needs for customer segmentation. This advanced clustering analysis can guide strategic decisions, improve customer engagement, and enhance targeting in marketing campaigns."
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html",
    "href": "posts/Anomaly-Outlier Detection/index.html",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "",
    "text": "Detecting anomalies in financial data involves using specialized algorithms to identify irregularities, enhancing risk management and fraud prevention"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#anomalyoutlier-detection-in-a-financial-dataset",
    "href": "posts/Anomaly-Outlier Detection/index.html#anomalyoutlier-detection-in-a-financial-dataset",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "",
    "text": "Detecting anomalies in financial data involves using specialized algorithms to identify irregularities, enhancing risk management and fraud prevention"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#introduction",
    "href": "posts/Anomaly-Outlier Detection/index.html#introduction",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Introduction",
    "text": "Introduction\nIn the modern world of finance, data plays a pivotal role in decision-making processes. Financial datasets often contain critical information about customers, transactions, and more. One crucial aspect of financial data analysis is the detection of anomalies or outliers. Anomalies are data points that significantly deviate from the norm or expected behavior. Detecting these anomalies is essential for fraud detection, risk management, and maintaining data integrity.\nThis report explores the application of anomaly/outlier detection techniques to a financial dataset. The dataset under consideration contains information such as Age, Income, Transaction Amount, Transaction Type, and Location. We aim to identify unusual patterns or data points within this dataset that may require further investigation.\nAdvanced Analysis\nI will now conduct an advanced analysis of this dataset, focusing on the following aspects:\nData Exploration: Understanding the basic statistics of the dataset. Visualization: Plotting the data to observe any unusual patterns. Anomaly Detection Techniques: Implementing various methods to detect outliers, such as statistical methods, clustering-based methods, and machine learning models. For this analysis, I will be using Python libraries like Pandas, NumPy, Matplotlib, Seaborn, and Scikit-Learn. Letâ€™s start with the data exploration and visualization.\nAnalysis Overview\nData Exploration\nThe descriptive statistics of the dataset provide a basic understanding of its features:\nA. Age: Ranges from 18 to 69, with an average of around 43 years. B. Income: The average income is approximately $51,518, with a wide range from around $988 to $181,196, suggesting significant variance. C. Transaction Amount: On average, transactions are around $200, but there are values as high as $1,999, which might indicate potential outliers.\nVisualization Insights\nA. Histograms: These show the distribution of numerical features. While age appears fairly uniformly distributed, income and transaction amounts show right-skewed distributions. B. Boxplots: The boxplots highlight potential outliers, especially in the income and transaction amount data. PCA Scatter Plot: After applying PCA for dimensionality reduction, we get a 2D visualization of the scaled numerical data. This plot can help us identify clusters and potential outliers visually.\nNext Steps for Anomaly Detection\nBased on these visualizations and statistics, the next step is to apply anomaly detection techniques. I will use the Isolation Forest algorithm, a popular method for outlier detection, especially effective with high-dimensional datasets. This method isolates anomalies instead of profiling normal data points. Anomalies are few and different, hence easier to isolate."
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#data-exploration-and-analysis",
    "href": "posts/Anomaly-Outlier Detection/index.html#data-exploration-and-analysis",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Data Exploration and Analysis",
    "text": "Data Exploration and Analysis\nI applied the Isolation Forest algorithm to our scaled numerical data and identify the anomalies. Letâ€™s proceed with this analysis.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\n# Loading the dataset\ndf = pd.read_csv('anomaly_detection_dataset.csv')\n\n# Data Exploration: Descriptive statistics\ndescriptive_stats = df.describe()\n# Display basic statistics\n\n# Visualization\n# Histograms for numerical data\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\ndf[['Age', 'Income', 'Transaction Amount']].hist(bins=15, ax=axes, color='skyblue')\nplt.suptitle('Histograms of Numerical Features')\n\n# Boxplot for numerical data to check for outliers\nfig, axes = plt.subplots(nrows=1, ncols=3, figsize=(18, 5))\nsns.boxplot(data=df[['Age', 'Income', 'Transaction Amount']], ax=axes[0])\nsns.boxplot(data=df[['Income']], ax=axes[1])\nsns.boxplot(data=df[['Transaction Amount']], ax=axes[2])\nplt.suptitle('Boxplots of Numerical Features')\n\n# Preparing data for anomaly detection\n# Standardizing the numerical data\nscaler = StandardScaler()\nscaled_numerical_data = scaler.fit_transform(df[['Age', 'Income', 'Transaction Amount']])\n\n# Applying PCA for dimensionality reduction (2D visualization)\npca = PCA(n_components=2)\npca_results = pca.fit_transform(scaled_numerical_data)\npca_df = pd.DataFrame(data=pca_results, columns=['PC1', 'PC2'])\n\n# Scatter plot of PCA results\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='PC1', y='PC2', data=pca_df)\nplt.title('PCA of Scaled Numerical Data')\n\nplt.show()\n\ndescriptive_stats, pca_df.head()\n\n\n\n\n\n\n\n\n\n\n(               Age         Income  Transaction Amount\n count  1000.000000    1000.000000         1000.000000\n mean     43.267000   51518.424999          200.855857\n std      15.242311   18506.474035          197.923861\n min      18.000000     988.660890            0.381117\n 25%      30.000000   39745.904804           79.905356\n 50%      43.000000   50483.467494          162.361081\n 75%      56.000000   60698.045016          264.145550\n max      69.000000  181196.443031         1999.137390,\n         PC1       PC2\n 0 -0.301194 -1.080589\n 1 -0.282737 -0.824039\n 2 -0.474600  2.067434\n 3  0.344241  0.305833\n 4 -0.009123  0.894454)\n\n\nData Visualization\nNext, we visualize the data to identify any obvious outliers or patterns.\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Histograms for numerical features\ndf.hist(bins=15, figsize=(15, 6))\nplt.show()\n\n# Box plots for numerical features to identify outliers\ndf.plot(kind='box', subplots=True, layout=(2,3), figsize=(15, 8))\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#analysis-results-anomaly-detection-with-isolation-forest",
    "href": "posts/Anomaly-Outlier Detection/index.html#analysis-results-anomaly-detection-with-isolation-forest",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Analysis Results: Anomaly Detection with Isolation Forest",
    "text": "Analysis Results: Anomaly Detection with Isolation Forest\nAnomaly Detection\nThe Isolation Forest algorithm was applied to the scaled numerical data, and it identified 50 anomalies in our dataset. This is consistent with the initial contamination rate set to 5% of the data.\nVisualization of Detected Anomalies\nThe scatter plot based on the PCA results with anomalies highlighted shows:\nA. Normal data points in blue. B. Anomalies marked in red.\nThese anomalies represent unusual patterns in terms of age, income, and transaction amounts, as identified by the Isolation Forest algorithm.\nInterpretation\nThe visualization clearly shows that the anomalies are distinct from the bulk of the data, signifying their outlier status. These could represent unusual financial transactions or demographic anomalies that would be of interest in real-world scenarios like fraud detection or targeted marketing."
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#anomaly-detection-using-isolation-forest",
    "href": "posts/Anomaly-Outlier Detection/index.html#anomaly-detection-using-isolation-forest",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Anomaly Detection Using Isolation Forest",
    "text": "Anomaly Detection Using Isolation Forest\nWe apply the Isolation Forest algorithm to detect anomalies in the dataset. This method is effective for high-dimensional datasets and does not require prior knowledge of the number of anomalies.\n\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.preprocessing import StandardScaler\n\n# Standardizing the data\nscaler = StandardScaler()\nscaled_data = scaler.fit_transform(df.select_dtypes(include=['float64', 'int64']))\n\n# Applying Isolation Forest\niso_forest = IsolationForest(n_estimators=100, contamination='auto', random_state=0)\npredictions = iso_forest.fit_predict(scaled_data)\n\n# Add a column for anomaly (1 for normal, -1 for anomaly)\ndf['anomaly'] = predictions\n\n# Count the number of anomalies\nanomaly_count = df['anomaly'].value_counts()\nprint(anomaly_count)\n\nanomaly\n 1    835\n-1    165\nName: count, dtype: int64\n\n\nVisualizing the Anomalies\nWe can visualize the anomalies in the context of two principal components.\n\nfrom sklearn.decomposition import PCA\n\n# PCA for dimensionality reduction for visualization\npca = PCA(n_components=2)\npca_result = pca.fit_transform(scaled_data)\n\ndf['pca1'] = pca_result[:, 0]\ndf['pca2'] = pca_result[:, 1]\n\n# Scatter plot of the PCA results colored by anomaly\nsns.scatterplot(x='pca1', y='pca2', hue='anomaly', data=df, palette={1: 'blue', -1: 'red'})\nplt.title('Anomalies in the PCA Plane')\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#k-means-clustering-for-anomaly-detection",
    "href": "posts/Anomaly-Outlier Detection/index.html#k-means-clustering-for-anomaly-detection",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "K-Means Clustering for Anomaly Detection",
    "text": "K-Means Clustering for Anomaly Detection\nK-Means can be used for anomaly detection by identifying small clusters as anomalies.\n\nfrom sklearn.cluster import KMeans\nimport numpy as np\n\n# K-Means Clustering with explicit n_init parameter\nkmeans = KMeans(n_clusters=5, n_init=10, random_state=0).fit(scaled_data)\ndf['cluster'] = kmeans.labels_\n\n# Detecting outliers as the points farthest from the centroids\ndistances = kmeans.transform(scaled_data)\ndf['distance_to_centroid'] = np.min(distances, axis=1)\noutlier_threshold = np.percentile(df['distance_to_centroid'], 95)\ndf['outlier'] = df['distance_to_centroid'] &gt; outlier_threshold\n\nK-Means Clustering Visualization\nAfter running the K-Means algorithm, we can plot the data points and color them by their cluster. Points classified as outliers will be highlighted.\n\nimport matplotlib.pyplot as plt\n\n# K-Means Clustering Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='cluster', data=df, palette='Set1')\nsns.scatterplot(x=df[df['outlier']]['pca1'], y=df[df['outlier']]['pca2'], color='black', s=100, label='Outlier')\nplt.title('K-Means Clustering')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#dbscan-for-anomaly-detection",
    "href": "posts/Anomaly-Outlier Detection/index.html#dbscan-for-anomaly-detection",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "DBSCAN for Anomaly Detection",
    "text": "DBSCAN for Anomaly Detection\nDBSCAN is effective in identifying regions of high density and isolating outliers as points in low-density areas.\n\nfrom sklearn.cluster import DBSCAN\n\n# DBSCAN Clustering\ndbscan = DBSCAN(eps=0.5, min_samples=10).fit(scaled_data)\ndf['dbscan_labels'] = dbscan.labels_\n\n# Anomalies are points with label -1\ndf['dbscan_outlier'] = df['dbscan_labels'] == -1\n\nDBSCAN Visualization\nFor DBSCAN, we will plot the data points and color them based on their cluster, highlighting the anomalies.\n\n# DBSCAN Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='dbscan_labels', data=df, palette='Set2')\nsns.scatterplot(x=df[df['dbscan_outlier']]['pca1'], y=df[df['dbscan_outlier']]['pca2'], color='black', s=100, label='Outlier')\nplt.title('DBSCAN Clustering')\nplt.legend()\nplt.show()"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#local-outlier-factor-lof",
    "href": "posts/Anomaly-Outlier Detection/index.html#local-outlier-factor-lof",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Local Outlier Factor (LOF)",
    "text": "Local Outlier Factor (LOF)\nLOF computes a score reflecting the degree of abnormality of the data, considering local density.\n\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# LOF for anomaly detection\nlof = LocalOutlierFactor(n_neighbors=20, contamination=0.05)\nlof_labels = lof.fit_predict(scaled_data)\n\ndf['lof_outlier'] = lof_labels == -1\n\nLOF Visualization\nLocal Outlier Factor results can be visualized by highlighting the points that are identified as outliers.\n\n# LOF Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='pca1', y='pca2', hue='lof_outlier', data=df)\nplt.title('Local Outlier Factor')\nplt.show()\n\n\n\n\nT-SNE for Visualization of High-Dimensional Data\nT-SNE is effective for visualizing high-dimensional data and its anomalies in a lower-dimensional space.\n\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Make sure to replace 'scaled_data' with your actual scaled data variable name\ntsne = TSNE(n_components=2, random_state=0)\ntsne_results = tsne.fit_transform(scaled_data)\n\n# Adding T-SNE results to your DataFrame\ndf['tsne1'] = tsne_results[:, 0]\ndf['tsne2'] = tsne_results[:, 1]\n\n# Use 'dbscan_labels' or another column of your choice for coloring the points\ncolumn_for_coloring = 'dbscan_labels'  # Replace with your chosen column\n\n# T-SNE Plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='tsne1', y='tsne2', hue=column_for_coloring, data=df, palette='Set2', legend=\"full\")\nplt.title('T-SNE Visualization')\nplt.show()\n\n\n\n\nT-SNE results can be plotted to show how the data is distributed in the reduced-dimensional space, highlighting the anomalies detected.\nThese techniques represent a deeper dive into anomaly detection, leveraging different approaches from clustering, neural networks, and neighborhood-based methods. Each method has its strengths and can be combined or compared for a comprehensive anomaly detection strategy."
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#final-check-for-anomaly-detection",
    "href": "posts/Anomaly-Outlier Detection/index.html#final-check-for-anomaly-detection",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Final check for Anomaly detection",
    "text": "Final check for Anomaly detection\n\n# Applying Isolation Forest for anomaly detection\niso_forest = IsolationForest(n_estimators=100, contamination=0.05, random_state=0)\nanomalies = iso_forest.fit_predict(scaled_numerical_data)\n\n# Adding the anomaly predictions to the PCA DataFrame\npca_df['Anomaly'] = anomalies\n\n# Visualizing the identified anomalies on the PCA plot\nplt.figure(figsize=(10, 6))\nsns.scatterplot(x='PC1', y='PC2', hue='Anomaly', data=pca_df, palette={1: 'blue', -1: 'red'})\nplt.title('PCA of Scaled Numerical Data with Anomalies Highlighted')\nplt.legend(title='Anomaly', loc='upper right', labels=['Normal', 'Anomaly'])\n\nplt.show()\n\n# Counting the number of detected anomalies\nnum_anomalies_detected = (anomalies == -1).sum()\n\nnum_anomalies_detected, pca_df.head()\n\n\n\n\n(50,\n         PC1       PC2  Anomaly\n 0 -0.301194 -1.080589        1\n 1 -0.282737 -0.824039        1\n 2 -0.474600  2.067434        1\n 3  0.344241  0.305833        1\n 4 -0.009123  0.894454        1)"
  },
  {
    "objectID": "posts/Anomaly-Outlier Detection/index.html#conclusion",
    "href": "posts/Anomaly-Outlier Detection/index.html#conclusion",
    "title": "Anomaly/Outlier Detection in a Financial Dataset",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the analysis of anomalies and outliers in financial datasets is of paramount importance for safeguarding financial systems and ensuring data quality. In this report, we delved into the application of various techniques to detect anomalies in a financial dataset containing Age, Income, Transaction Amount, Transaction Type, and Location. The results of this analysis can guide decision-makers in identifying potential issues, such as fraudulent transactions or data entry errors.\nBy leveraging advanced analytics and machine learning algorithms, financial institutions can proactively mitigate risks, enhance customer trust, and streamline their operations. The continuous monitoring and refinement of anomaly detection methods are critical in the ever-evolving landscape of finance.\nAs financial data continues to grow in complexity and volume, the ability to detect anomalies accurately becomes increasingly crucial. It is essential for organizations to stay vigilant and adapt their anomaly detection strategies to stay ahead of emerging threats and challenges in the financial sector."
  },
  {
    "objectID": "CV_Project/CV_project.html",
    "href": "CV_Project/CV_project.html",
    "title": "Fire/Smoke Detection Algorithm using Deep Learning and Machine Vision",
    "section": "",
    "text": "Click on this link here to access the report/website"
  },
  {
    "objectID": "CV_Project/CV_project.html#real-time-forest-fire-detection-system-using-machine-vision-and-deep-learning",
    "href": "CV_Project/CV_project.html#real-time-forest-fire-detection-system-using-machine-vision-and-deep-learning",
    "title": "Fire/Smoke Detection Algorithm using Deep Learning and Machine Vision",
    "section": "",
    "text": "Click on this link here to access the report/website"
  },
  {
    "objectID": "control_systems.html",
    "href": "control_systems.html",
    "title": "Applied Control Systems",
    "section": "",
    "text": "No matching items\n\n Back to top"
  },
  {
    "objectID": "Control Systems/Full_State_Feedback_Control_Design.html",
    "href": "Control Systems/Full_State_Feedback_Control_Design.html",
    "title": "Full State Feedback Control Design",
    "section": "",
    "text": "clear;\nset(0,'defaultlinelinewidth',2);\nformat short;\nsympref('FloatingPointOutput',true);"
  },
  {
    "objectID": "Control Systems/Full_State_Feedback_Control_Design.html#for-cases-6-7-and-8-you-will-design-and-simulate-another-output-feedback-controller-i.e.-a-state-estimator-with-full-state-feedback-control-but-now-you-must-also-include-integral-control-for-reference-tracking-as-defined-in-the-notes.",
    "href": "Control Systems/Full_State_Feedback_Control_Design.html#for-cases-6-7-and-8-you-will-design-and-simulate-another-output-feedback-controller-i.e.-a-state-estimator-with-full-state-feedback-control-but-now-you-must-also-include-integral-control-for-reference-tracking-as-defined-in-the-notes.",
    "title": "Full State Feedback Control Design",
    "section": "For cases 6, 7, and 8, you will design and simulate another Output Feedback Controller (i.e.Â a state estimator with full state feedback control), but now you must also include Integral Control for reference tracking as defined in the notes.",
    "text": "For cases 6, 7, and 8, you will design and simulate another Output Feedback Controller (i.e.Â a state estimator with full state feedback control), but now you must also include Integral Control for reference tracking as defined in the notes."
  },
  {
    "objectID": "Control Systems/Full_State_Feedback_Control_Design.html#you-must-expect-to-iterate-on-cases-6-7-and-8-until-you-achieve-one-acceptable-design.",
    "href": "Control Systems/Full_State_Feedback_Control_Design.html#you-must-expect-to-iterate-on-cases-6-7-and-8-until-you-achieve-one-acceptable-design.",
    "title": "Full State Feedback Control Design",
    "section": "You must expect to iterate on CASES 6, 7, and 8 until you achieve one acceptable design.",
    "text": "You must expect to iterate on CASES 6, 7, and 8 until you achieve one acceptable design."
  },
  {
    "objectID": "Control Systems/Full_State_Feedback_Control_Design.html#simulation-result",
    "href": "Control Systems/Full_State_Feedback_Control_Design.html#simulation-result",
    "title": "Full State Feedback Control Design",
    "section": "SIMULATION RESULT",
    "text": "SIMULATION RESULT"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Anomaly/Outlier Detection in a Financial Dataset\n\n\n\nMachine Learning\n\n\n\nDetecting anomalies in financial data involves using specialized algorithms to identify irregularities, enhancing risk management and fraud prevention\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Classification techniques with Model Prediction Analysis\n\n\n\nMachine Learning\n\n\n\nImplementing advanced classification techniques for precise model prediction analysis to enhance accuracy and efficiency\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Clustering and Prediction techniques\n\n\n\nMachine Learning\n\n\n\nEmploying sophisticated clustering techniques with retail customer data and comparing further with prediction models\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced analysis on Linear and Non-Linear Regression models\n\n\n\nMachine Learning\n\n\n\nA Comprehensive Analysis on Housing Market. Utilizing regression methodologies for the purpose of analyzing trends within the housing market\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "The sound of the Gion Shoja bells echoes the impermanence of all things; the color of the sala flowers reveals the truth that the prosperous must decline. However, we humans are the exception."
  },
  {
    "objectID": "about.html#mahindra-and-mahindra-limited-automotive-product-development-mrv-chennai-tamil-nadu-india---603001",
    "href": "about.html#mahindra-and-mahindra-limited-automotive-product-development-mrv-chennai-tamil-nadu-india---603001",
    "title": "About Me",
    "section": "Mahindra and Mahindra Limited [Automotive Product Development] | MRV Chennai, Tamil Nadu, India - 603001",
    "text": "Mahindra and Mahindra Limited [Automotive Product Development] | MRV Chennai, Tamil Nadu, India - 603001\n\nDepartment - Electrical and Electronics (Vehicle Electronics Software)"
  },
  {
    "objectID": "about.html#product-development-engineer",
    "href": "about.html#product-development-engineer",
    "title": "About Me",
    "section": "Product Development Engineer",
    "text": "Product Development Engineer\nAugust 2022 - July 2023\n\nWorked on projects focussed on critical issues regarding CDC (Cockpit Domain Controller) Vehicles on a production level at mentioned original equipment manufacturer (OEM).\nMy work comprised of certification, debugging and testing processes of Wireless Android Auto and Carplay for the project by colloborating with our 3PL partners and vendors: Visteon Corporation, Aptiv and Harmann International.\nDebugged Infotainment system software in C++ on a firmware scale and traditional Python on a development scale leading to a stabilized initial software release for the products.\nCommute work for bench-level and vehicle-level environmental testing to test WiFi, media playback and Bluetooth features of the infotainment system simultaneously with Navigation (GPS) implementation.\nLearnt & understood the hardware architecture, and performed unit testing on the Silver Box for 3 major projects Mahindra XUV 400 EV, the 2023 Electric Vehicle, Mahindra XUV 700 and Mahindra Scorpio N 2023.\nSupported the Principal Engineers with Standard Operating Procedures (SOPs) and Software Requirement Documents (SRDs), and gained exposure on how to write the set of work instructions for each of them.\nImplemented TCP/IP, UART, LIN (obsolete) and CAN protocols upon Vehicle Electronic Control Units (ECUs) using various development tools and tested them using diagnostic tools which we will discuss about below.\nReceived Green light approval from Google for Android Auto certification for the project XUV 400 EV, the 2023 Electric Vehicle and Scorpio N 2023 leading to USD $1 Million p.a. in CAPEX approvals - $100K more than the ask.\nAndroid Auto software testing processes included tests: PCTS Verifier, QSuite, UX, GPS/Navigation, sensor logging. Consistent stable releases lead to a 15% revenue growth in sales in 2 Quarter cycles, due to on-site customer engineer support.\nIntegrated OTA updates, GPS benchmarking and connectivity features with cross-functional teams like ADAS, HMI and wiring harness for client satisfaction.\nDemonstrated expertise in Android and QNX OS, achieving 93% bug fixes in Android Autoâ€™s initial release.\nDiagnostics tools like Vector CANalyzer v16, CANoe and Garuda 2.0 tool for CAN BUS analysis of Electronic Control Units (ECUs). Loggers and CAPL were utilized as well for log storage and debugging processes.\nPerformed signal monitoring of ECU response using CANalyzer and CANoe to measure and check the overshooting of the serial bus data, both on bench and production vehicles for our customers.\n\n\nMy career includes over 1.5 years of vehicle product development and software validation-testing in cockpit electronic systems and cross-functional teams such as ADAS, HMI, wiring harness and Displays. I have worked on several projects such as:"
  },
  {
    "objectID": "about.html#projects",
    "href": "about.html#projects",
    "title": "About Me",
    "section": "PROJECTS:",
    "text": "PROJECTS:\n\nMAHINDRA XUV 700\n\n\n\n\n\nMahindra XUV 700 AX7 variant\n\n\n\n\n\n\n\nInstrument Panel\n\n\n\n\nAdrenoX OS (For the In-vehicle Infotainment System)\n\nAdrenoX is developed to transform the complete vehicle experience with premium features and technology in partnership with Amazon Alexa, Sony, Visteon and Bosch.\nIt brought voice AI capabilities to the XUV700, with Indiaâ€™s first integration of Amazon Alexa in production vehicles XUV700.\nAdrenoX is the soul of this project as it is the go-to software used for the entirety of the vehicle electronic system. It has different features for each of the variants: MX, AX3, AX5 and AX7\nThe Adrenox connect mobile app provides information in your mobile phone about location & status of the vehicle and allows to control certain vehicle features in a secured way for both Android and iOS platform.\nWe use the ADRENOX connect mobile app for connected car related features.\nThe app comprises of several features and requirements for the owner like:\n\nKYC ( Know Your Customer) Registration (Requirement)\nLogin & Registration (Requirement)\nOver Speed Alert (Feature)\nHigh Engine Temperature Alert (Feature)\nVehicle Start/Stop Alert (Feature)\nEcosenseâ€‹ (Feature)\nFind my car (Feature)\nShare my car location (Feature)\n\nAnd many more features.\n\n\n\n\n\nAdrenoX in XUV 700\n\n\n\n\n\nIN-VEHICLE INFOTAINMENT SYSTEM\n\nOffered by AdrenoX, is through integrated 10.25-inch dual screens with a wide display in the segment and SmartCoreâ„¢ cockpit domain controller technology with the third-generation QualcommÂ® Snapdragonâ„¢ Automotive Cockpit Platforms developed in collaboration with Visteon.\nThe infotainment architecture I have worked on for XUV 700:\n\n\n\n\n\nInfotainment System\n\n\n\n\n\nQNX OS (For the Instrument Cluster)\n\nWe selected BlackBerry QNX to power Cockpit Domain Controller For Next-Gen XUV700 SUV, especially implemented for the Instrument Cluster.\nCombining the latest in chip technology with the highly reliable QNX Hypervisor, the XUV700 allows for design flexibility and scalability to consolidate multiple systems with mixed criticality and different operating environments onto a single hardware platform, reducing both the initial development and long-term costs of ownership while providing the highest standards of functional safety and security.\nThe CDC levarages the QNX Neutrino Realtime Operating System (RTOS) andQNX Hypervisor which represent key ingredients of the vehicle that serve as secure and trusted foundation of the intelligent infotainment system which includes the Instrument Cluster.\n\n\n\nINSTRUMENT CLUSTER\n\nThe digital instrument cluster architecture I have worked on for XUV 700 are of two types, each used in respective particular variants:\n\n\n\n\n\nType 1 Cluster\n\n\n\n\n\nType 2 Cluster\n\n\n\n\n\nADAS - Adaptive Cruise control\n\nCollaborated with the ADAS team to modify and calibrate the cruise control implementation in their SRDs as well as tested their features.\n\n\n\n\n\nAdaptive Cruise control\n\n\n\n\nMahindra Scorpio N\n\n\n\n\n\nMahindra Scorpio N Z8L (6S) variant\n\n\n\n\n\nSony Surround System (OEM) in Mahindra Scorpio N\n\n\n\n\n\nAdrenoX and QNX OS\n\nAdrenoX and QNX OS are used in latest variants of Scorpio N as well (Z6+) for both in-vehicle infotainment system and instrument cluster, supporting Android OS and QNX Hypervisor for a secure software establishment for the cockpit domain.\n\n\n\nIN-VEHICLE INFOTAINMENT SYSTEM\n\nOffered by AdrenoX, the vehicle has a 8-inch touchscreen infotainment system and SmartCoreâ„¢ cockpit domain controller technology with the third-generation QualcommÂ® Snapdragonâ„¢ Automotive Cockpit Platforms developed in collaboration with Visteon.\nThe infotainment architecture I have worked on for Scorpio N:\n\n\n\n\n\nInfotainment System for Mahindra Scorpio N Latest Variants\n\n\n\n\n\nINSTRUMENT CLUSTER\n\nThe semi-digital instrument cluster featuring a 7-inch screen. The cluster architecture I have worked on for Scorpio N, are for 3 variants, Z6, Z8 and Z8L:\n\n\n\n\n\nScorpio N Instrument Cluster showcasing Drive Modes that makes the vehicle adapt to different terrains\n\n\n\n\n\nDETAILS AND ACHIEVEMENTS:\n\nWith a 20.3 cm infotainment screen, Daddyâ€™s den is equipped with GPS navigation, Bluetooth, USB, Apple CarPlay & Android Auto. With Daddy connectivity and entertainment, are at your fingertips.\nSuccessful Android Auto certification post-development and testing phase of 4 months with commute to vendor locations, confirming green light with Google and performing OTA software release with minimal bugs and user errors.\nCalibrated and fine-tuned the cockpit domain controller using CAN tools and logging capabilities, thoroughly testing the live deployed production vehicles of our consumers.\n\n\n\nProtcols followed:\n\nCAN over Ethernet: PDU Transport (e.g., Zone Architecture)\nDiagnostics (UDS over CAN and DoIP)\nSOME/IP\n\n\n\nTools\n\nVector CANalyzer v16.0\nI have performed analysis on network data traffic in serial bus systems, CAN buses in the Electronic Control Units (ECUs) in the vehicle in our product using vector CANalyzer v16.0. We used CANalyzer to check whether and what type of communication is occurring on the network of a particular component. In addition to sending or recording data, interactive ECU diagnosis is also possible.\n\n\n\n\nCANalyzer System Flow Diagram\n\n\n\nThe physical tool that consists of all the components of CANalyzer inlcuding a OBD connection, is shown below:\n\n\n\n\nCANalyzer Tool\n\n\n\n\n\n\n\nCANalyzer landing window which includes the manipulation of BUS channels, stress logging features, CAN Statistics and datasheet I/O\n\n\n\nSignal interference analysis of car speed, idle engine running, engine speed, temperature and gear status are some examples of data traffic testing I have performed to verify and check whether the Instrument Cluster is receiving the data (in the form of binary during transmission and testing) accurately from the required ECUs whose functionalities are being tested to display accurate information on instruments like Speedometer, Odometer, Tachometer, Fuel-Level Gauge and Temperature Gauge.\n\n\n\n\nNetwork BUS signal analysis\n\n\n\nTrace analysis of signal overlapping and cross-function in features like ABS (Anti-lock Braking System), and Gear box, dashboard and engine control.\n\n\n\n\nTrace analysis\n\n\n\n\n\nCANoe\nCANoe is the comprehensive software tool for development, test and analysis of individual ECUs and entire ECU networks in the automotive and various other industries. It supports network designers, development and test engineers throughout the entire development process â€“ from planning to system-level test.\n\n\n\n\nCANoe System Flow Diagram\n\n\n\n\n\n\nStimulation Types and SUT integration\n\n\n\n\n\n\nTest and analysis of mentioned ECUs above on CANoe pro\n\n\n\n\nVehicle Safety (Crash Tests - GLOBAL NCAP)\nThe OEMâ€™s flagship SUV XUV700 also secured five-star rating in the adult occupant protection program and four-star rating in child occupant protection program. However, with an overall safety score of 57.69 points, it now sits slightly lower than the Scorpio-N.\nMainly, my work revolved around the electrical architecture of the vehicle and especially, cockpit electronics, I supported the Functional Safety CAE team with testing configurations during crash tests during the minimization of time frame between the impact and airbag deployment.\n\n\n\n\nTesting example for functional safety\n\n\n\n\n\n\nOBD II Scanner\nIt can be plugged into almost every modern vehicleâ€™s OBD port to get valuable information and real-time data about vehicle systems. A wireless scanner can connect to your cellphone through Bluetooth, and let you access your carâ€™s data through a mobile app.\n\n\nGaruda II Auto Diagnostic Tool\nGaruda is a fully indigenous product from the GlobalEdge stable, provides vehicle network monitoring, re-programming, and diagnostics for a variety of applications, including passenger cars, trucks, buses, off-highway vehicles, and DG set applications. Its compact size, plug-and-play model, and PC connectivity through USB 2.0 and Wi-Fi make it exceptionally user-friendly, even in remote or rugged conditions. Despite its cost-effectiveness, Garuda supports a wide range of protocols and is compatible with host applications such as DiagRA D and Silver Scan.\nEquipped with SAE J2534 and RP1210B interfaces, Garuda serves as a superior diagnostic tool for vehicle engineers, allowing the collection of data from diverse network types, including CAN, using standard interface devices. This capability ensures rapid identification and prompt resolution of vehicular malfunctions. To enhance usage optimization and efficient service, Garuda comes with multiple accessories, including the USB cable and OBDII to DB-15 male cable.\n\n\n\n\nGaruda 2.0 Auto Diagnostic Tool\n\n\n\nKey Applications include:\n\nNetwork analysis and data processing tools\nDiagnostics testers / ECU flashing software\nProduction / EOL test tools\nEngine test bed software\nKey programming software\nJ1939 network analysis tools\nField firmware upgrades\nMobile applications for R&D, production, and after-sales vehicle service\n\nKey Features:\n\nOBD II connectivity between vehicle and PC\nWide input voltage range (9 â€“ 32 VDC)\nPC connectivity through USB 2.0 and Wi-Fi\nVehicle Battery Read (VBATT)\nSupports SAE J2534 and RP1210B API for reprogramming and pass-through diagnostics\nMulti-protocol support - CAN (ISO 11898-2, J2284, J1939, ISO 15765, UDS) and KWP2000 / ISO 14230\n\nProduct Specification:\n\nPC Connection: USB 2.0 / Wi-Fi\nVehicle Connection: J1962 cable / Integrated J1962\nEnclosure:\nSize: 130 X 76 X 30 mm\nColor: Blue\nWeight: 170 g\nHumidity: 20% to 85 % RH\nInput Voltage: 9 â€“ 32 V\nPower Consumption (Nominal):\nUSB mode: 80 mA\nWi-Fi mode: 140 mA\nOperational Temperature Range: 0Â° to 65Â° C\nShock Endurance: 1.5 meter drop\n\n\n\nWireshark\nWireshark is used in the automotive industry for:\n\nCAN Bus Analysis: Capturing and analyzing communication on the Controller Area Network (CAN) bus for debugging and monitoring.\nDiagnostic Communication: Analyzing OBD-II and other diagnostic protocols to diagnose vehicle issues and monitor sensor data.\nEthernet in Automotive: Capturing and analyzing Ethernet frames for high-bandwidth applications like infotainment and advanced driver assistance systems.\nSecurity Analysis: Monitoring network traffic for security vulnerabilities and potential cyber threats in automotive systems.\nProtocol Development and Testing: Supporting the development and testing of new automotive communication protocols and systems.\nTelematics and Fleet Management: Capturing and analyzing communication between vehicles and backend servers for fleet management and telematics applications.\n\nMy contribution:\nAs it is a proprietary protocol, I had to reverse engineer the data. It might or might not be encrypted so one might need luck in being able to decipher the messages. In my experience, a lot of protocols have some sort of PDU structure with a length and/or sequence number in them, that is where I expanded my domain on PDUâ€™s and continuing to learn further currently."
  },
  {
    "objectID": "about.html#oracle-cerner-cerner-corporation-bangalore-karnataka-india---560103",
    "href": "about.html#oracle-cerner-cerner-corporation-bangalore-karnataka-india---560103",
    "title": "About Me",
    "section": "Oracle Cerner [Cerner Corporation] | Bangalore, Karnataka, India - 560103",
    "text": "Oracle Cerner [Cerner Corporation] | Bangalore, Karnataka, India - 560103\n\nDepartment - CareAware (CCTA)"
  },
  {
    "objectID": "about.html#software-developer-engineer-intern",
    "href": "about.html#software-developer-engineer-intern",
    "title": "About Me",
    "section": "Software Developer Engineer Intern",
    "text": "Software Developer Engineer Intern\nAugust 2022 - July 2023\n\nWorked on a scalable healthcare IT web application using Java, Spring Boot, Python, React, and JavaScript, focusing on patient-bed-nurse allocation.\nCreated REST API controllers tested with POSTMAN, achieving 95% unit test coverage with JUNIT, Mockito, and Jest/enzyme.\nConducted code reviews and merged around 20 commits/week reducing software defects by 20% alongside optimizing RDBMS MySQL database size by 7% using scripts.\nEmployed Agile scrums and machine learning to cluster suppliers using medical device data, saving the team 10 hours/week.\nDevOps: Utilized Jenkins for CI/CD and pipeline staging script-based automation, integration and deployment processes.\n\nand software engineering for Oracle Cerner and automotive software development and management for Mahindra & Mahindra. I use the MATLAB, JAVA, Javascript and Python programming languages on a daily basis but am much more advanced with my MATLAB work.\nI aim to help bridge the gap between business needs and engineering to develop and deploy automation solutions, predictive algorithms, perform vehicle control in cockpit systems as well as cruise-control (ADAS), and perform control design, system ID and stabilization of various live projects in my career."
  },
  {
    "objectID": "about.html#programming-analytics-skills",
    "href": "about.html#programming-analytics-skills",
    "title": "About Me",
    "section": "Programming & Analytics Skills",
    "text": "Programming & Analytics Skills\n\nTechnical: MATLAB, Python, Java, C++, SQL, HTML, CSS, React, JavaScript, Linux bash, Git, CAN, LIN\nFrameworks and Tools: RESTful API, GitHub, Jenkins, Microservices, Vector CANalyzer, CANoe, Wireshark, Android, OpenCV, POSTMAN, Android Auto PCTS Verifier, ROS2, Gazebo, MySQL, Scikit-learn, Tableau, Power BI, Spring, JUNIT5, Jupyter, RTOS, Control System Toolbox, Simulink\nOther: Agile, JIRA, Product & Project Management, eLMS (Electronic Life Management System), Microsoft Office Suite"
  },
  {
    "objectID": "Control Systems/Bode_freqresp.html",
    "href": "Control Systems/Bode_freqresp.html",
    "title": "Bode Inverse Estimation",
    "section": "",
    "text": "Analysis on Frequency Responses using Transfer Functions of a given System\nclear, clc\n\nset(0,'defaultlinelinewidth',2);\ns= tf('s')\ns =\n \n  s\n \nContinuous-time transfer function.\nModel Properties\n\n\nReview of Transfer Functions and Frequency Responses\nIn a classical feedback control, we should know how to sketch a Bode diagram given a Laplace-domain transfer function. One of the first steps in the sketching procedure was to factor the transfer function into first-order and second-order building blocks. The list below shows the standard seven building blocks in normalized form (normalized is not the only form, but these seven are the only building blocks). Any transfer function can be factored into products of these seven terms.\nGain: \\(K\\)\nFirst-order Pole at Origin: \\(\\left(\\frac{1}{s}\\right)\\) First-order Zero at Origin: \\(\\left(\\frac{s}{1}\\right)\\)\nFirst-order Real Pole: \\(\\left(\\frac{p}{s+p}\\right)\\) First-order Real Zero: \\(\\left(\\frac{s+z}{z}\\right)\\)\nSecond-order Pole: \\(\\left(\\frac{\\omega_p^2 }{s^2 +2\\zeta_p \\omega_p +\\omega_p^2 }\\right)\\) Second-order Zero: \\(\\left(\\frac{s^2 +2\\zeta_z \\omega_z +\\omega_z^2 }{\\omega_z^2 }\\right)\\)\nIf one understands how to sketch a Bode diagram for a transfer function, they should also understand how to do the inverse problem of estimating a mathematical transfer function from a given Bode diagram (for relatively simple systems).\nThus, here we are applying our knowledge of sketching Bode diagrams to accurately estimate the Laplace-domain transfer functions associated with each frequency response below.\nNote: The term â€œaccurately estimateâ€ means that we are expected to extract numerical information from the frequency response plots (e.g.Â slopes, frequencies, gains) to determine numerical values for the transfer function parameters based on the normalized building blocks above. After we have generated our estimated transfer function, we must compute and plot the frequency response in a properly annotated figure (like the ones provided below). Thus in this verification process, our plots should have the same grid lines, ranges, labels, etc, as the provided plots initially.\n\n\nCase 1\n\nDesired Magnitude and Phase\n\noptions = bodeoptions;\noptions.FreqUnits = 'Hz'; % or 'rad/second', 'rpm', etc.\noptions.MagScale = 'linear';\noptions.MagLowerLimMode = 'manual';\noptions.MagLowerLim = -55;\noptions.XLimMode = 'manual';\noptions.XLim = [10^0 10^4];\n\noptions.Grid = 'on';\nfigure(1)\n\npole = 3e2*2*pi;\ng0 = 10^(-20/20);\n\ntf = g0 * 1/(s/pole + 1);\nbode(tf, options)\n\n\nOutput:\n\n\n\n\nCase 2\n\nDesired Magnitude and Phase\n\noptions = bodeoptions;\noptions.FreqUnits = 'Hz'; % or 'rad/second', 'rpm', etc.\noptions.MagScale = 'linear';\noptions.XLimMode = 'manual';\noptions.XLim = [10^1 10^5];\noptions.Grid = 'on';\nfigure(2)\n\nzero = 900*2*pi;\ng0 = 10^(75/20);\n\ntf = g0* 1 / s * (s+zero)/zero;\nbode(tf, options)\n\n\nOutput:\n\n\n\n\nCase 3\n\nDesired Magnitude and Phase\n\noptions = bodeoptions;\noptions.FreqUnits = 'Hz'; % or 'rad/second', 'rpm', etc.\noptions.MagScale = 'linear';\noptions.XLimMode = 'manual';\noptions.XLim = [10^(-2) 10^4];\noptions.Grid = 'on';\nfigure(3);\n\nzero = 2*pi;\npole = 3e2*2*pi;\n\ng0= 10^(-20/20);\n\ntf= g0 * (s/zero + 1)/(s/pole + 1);\nbode(tf, options)\n\n\nOutput:\n\n\n\n\nCase 4\n\nDesired Magnitude and Phase\n\noptions = bodeoptions;\noptions.FreqUnits = 'Hz'; % or 'rad/second', 'rpm', etc.\noptions.MagScale = 'linear';\noptions.MagLowerLimMode = 'manual';\noptions.MagLowerLim = -140;\noptions.XLimMode = 'manual';\noptions.XLim = [10^2 10^7];\noptions.Grid = 'on';\noptions.GridColor = 'red';\nfigure(4)\n\npole = 4e3*2*pi;\nzero = 2e5*2*pi;\n\nQdB=5;\nQ=10^(QdB/20);\nzeta= 1/(2*Q);\n\ng0=10^(-20/20);\n\ntf = g0 * pole^2/(s^2 + 2*zeta*pole*s + pole^2) * (s+zero)/zero;\n\nbode(tf,options)\n\n\nOutput:\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Control Systems/Linearized_LTI_Modeling_Feedback_Control.html",
    "href": "Control Systems/Linearized_LTI_Modeling_Feedback_Control.html",
    "title": "Linearized_LTI_Modeling_Feedback_Control",
    "section": "",
    "text": "clear;\nset(0,'defaultlinelinewidth',2);\nformat short;\nsympref('FloatingPointOutput',true);"
  },
  {
    "objectID": "Control Systems/Linearized_LTI_Modeling_Feedback_Control.html#background",
    "href": "Control Systems/Linearized_LTI_Modeling_Feedback_Control.html#background",
    "title": "Linearized_LTI_Modeling_Feedback_Control",
    "section": "Background",
    "text": "Background\nA small startup company is exploring an aerial concept for delivering packages (Payload). This concept uses an open-top bin (gray) supported by drone motors and propellors (blue-forward and green-aft).\nPrior to developing a full 3D prototype, they have commissioned you to develop and validate a full state feedback control system for a 2D (i.e.Â planar) version as shown in Figure 1.\n\nFigure 1. 2D-Coordinate System Definition for Prototype Aerial Package Delivery System\nThe following definitions refer to Figure 1:\nÂ Â Â Â Â Â Â Â  \\[ \\begin{array}{l} \\textrm{CG}=\\textrm{center}\\;\\textrm{of}\\;\\textrm{gravity}\\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{system}\\;\\textrm{with}\\;\\textrm{the}\\;\\textrm{current}\\;\\textrm{payload}\\;\\textrm{position}\\\\ O=\\textrm{origin}\\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{world}\\;\\textrm{coordinate}\\;\\textrm{system}\\\\ x\\left(t\\right)=\\textrm{horizontal}\\;\\textrm{position}\\;\\left\\lbrack m\\right\\rbrack \\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{CG}\\;\\textrm{in}\\;\\textrm{the}\\;\\textrm{world}\\;\\textrm{coordinate}\\;\\textrm{system}\\\\ y\\left(t\\right)=\\textrm{vertical}\\;\\textrm{position}\\;\\left\\lbrack m\\right\\rbrack \\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{CG}\\;\\textrm{in}\\;\\textrm{the}\\;\\textrm{world}\\;\\textrm{coordinate}\\;\\textrm{system}\\\\ \\theta \\left(t\\right)=\\textrm{angular}\\;\\textrm{rotation}\\;\\left\\lbrack \\textrm{rad}\\right\\rbrack \\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{open}-\\textrm{top}\\;\\textrm{bin}\\\\ F_1 \\left(t\\right)=\\textrm{bipolar}\\;\\textrm{control}\\;\\textrm{thrust}\\;\\textrm{force}\\;\\left\\lbrack N\\right\\rbrack \\;\\textrm{at}\\;\\textrm{the}\\;\\textrm{front}\\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{bin}\\\\ F_2 \\left(t\\right)=\\textrm{bipolar}\\;\\textrm{control}\\;\\textrm{thrust}\\;\\textrm{force}\\;\\left\\lbrack N\\right\\rbrack \\;\\textrm{at}\\;\\textrm{the}\\;\\textrm{rear}\\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{bin} \\end{array} \\]\nThe equations of motion for this system are non-linear; however, the following linearized equations are a reasonable approximation for the purposes of this project:\n\nForce balance, y-direction: \\(M\\ddot{\\mathrm{y}} \\left(t\\right)=F_1 \\left(t\\right)+F_2 \\left(t\\right)-\\alpha \\dot{y} \\left(t\\right)-M\\;g\\;\\)\nMoment balance about CG: \\(J\\ddot{\\theta} \\left(t\\right)=d_1 F_1 \\left(t\\right)-d_2 F_2 \\left(t\\right)-\\beta \\dot{\\theta} \\left(t\\right)\\)\nForce balance, x-direction: \\(M\\ddot{x} \\left(t\\right)=-\\left({\\bar{F} }_1 +{\\bar{F} }_2 \\right)\\theta \\left(t\\right)-\\gamma \\dot{x} \\left(t\\right)\\)\n\nThe parameters associated with these dynamic equations are defined as:\nÂ Â Â Â Â Â Â Â  \\[ \\begin{array}{l} g=\\textrm{gravity}\\;\\left\\lbrack \\frac{m}{s^2 }\\right\\rbrack \\\\ M=\\textrm{total}\\;\\textrm{mass}\\;\\left\\lbrack \\textrm{kg}\\right\\rbrack \\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{system}\\;\\textrm{with}\\;\\textrm{payload}\\\\ J=\\textrm{total}\\;\\textrm{rotational}\\;\\textrm{moment}\\;\\textrm{of}\\;\\textrm{inertia}\\;\\left\\lbrack \\textrm{kg}\\cdot m^2 \\right\\rbrack \\;\\textrm{about}\\;\\textrm{the}\\;\\textrm{CG}\\;\\textrm{of}\\;\\textrm{the}\\;\\textrm{system}\\;\\textrm{with}\\;\\textrm{payload}\\\\ \\alpha =\\textrm{vertical}\\;\\textrm{aerodynamic}\\;\\textrm{damping}\\;\\textrm{coefficient}\\;\\;\\left\\lbrack \\frac{N\\cdot s}{\\;m}\\right\\rbrack \\\\ \\beta =\\textrm{rotational}\\;\\textrm{aerodynamic}\\;\\textrm{damping}\\;\\textrm{coefficient}\\;\\left(\\frac{N\\cdot m\\cdot s}{\\textrm{rad}}\\right)\\\\ \\gamma =\\textrm{horizontal}\\;\\textrm{aerodynamic}\\;\\textrm{damping}\\;\\textrm{coefficient}\\;\\left(\\frac{N\\cdot s}{m}\\right)\\\\ d_1 =\\textrm{distance}\\;\\left\\lbrack m\\right\\rbrack \\;\\textrm{from}\\;\\textrm{the}\\;\\textrm{CG}\\;\\textrm{to}\\;\\textrm{the}\\;\\textrm{front}\\;\\textrm{control}\\;\\textrm{thrust}\\\\ d_2 =\\textrm{distance}\\;\\left\\lbrack m\\right\\rbrack \\;\\textrm{from}\\;\\textrm{the}\\;\\textrm{CG}\\;\\textrm{to}\\;\\textrm{the}\\;\\textrm{rear}\\;\\textrm{control}\\;\\textrm{thrust} \\end{array} \\]"
  },
  {
    "objectID": "cv/cv.html",
    "href": "cv/cv.html",
    "title": "Curriculum Vitae",
    "section": "",
    "text": "nishantkb@vt.edu"
  },
  {
    "objectID": "cv/cv.html#summary",
    "href": "cv/cv.html#summary",
    "title": "Curriculum Vitae",
    "section": "Summary",
    "text": "Summary\nMy goal is to learn and apply lead the practical control methods in the field of robotics and autonomous vehicles to integrate substantive expertise from diverse fields with machine intelligence and vision.\nThrough research and projects, I am constantly improving my ability to obtain, tidy, explore, transform, visualize, model, and communicate data. My preferred tools are MATLAB, Simulink, Java, JavaScript and Python, open source programming languages kept on the cutting edge by their energetic communities.\nIn addition to their utility in common data science tasks, these languages serve as accessible interfaces to deep learning libraries like TensorFlow.\nI aim to leverage my background in science and engineering to help shape the future of autonomy, controls and machine vision and deliver on the promise of machine learning applications."
  },
  {
    "objectID": "cv/cv.html#education",
    "href": "cv/cv.html#education",
    "title": "Curriculum Vitae",
    "section": "EDUCATION",
    "text": "EDUCATION\n\nVirginia Polytechnic Institute and State University, Blacksburg, Virginia 24060, USA\n\nAnticipated Graduation: May, 2025\nMaster of Science in Computer Engineering\n\nVellore Institute of Technology, Vellore, Tamil Nadu, India\n\n2018 â€“ 2022\nBachelor of Technology in Electronics and Communication Engineering\n\nCGPA: 3.33 / 4.00"
  },
  {
    "objectID": "cv/cv.html#work-experience",
    "href": "cv/cv.html#work-experience",
    "title": "Curriculum Vitae",
    "section": "WORK EXPERIENCE",
    "text": "WORK EXPERIENCE"
  },
  {
    "objectID": "cv/cv.html#product-development-engineer-mahindra-mahindra-limited-chennai-tn-india",
    "href": "cv/cv.html#product-development-engineer-mahindra-mahindra-limited-chennai-tn-india",
    "title": "Curriculum Vitae",
    "section": "Product Development Engineer | Mahindra & Mahindra Limited | Chennai, TN, India",
    "text": "Product Development Engineer | Mahindra & Mahindra Limited | Chennai, TN, India\nAugust 2022 â€“ July 2023\nR&D Engineer - Electrical & Electronics - CAN BUS Analysis of Electronic Control Units [Automotive Product Development]\n\nProjects: Worked on the infotainment and cluster software architecture for ICE & BEV vehicles; Mahindra Scorpio N Z6/Z8 variants, XUV700 & XUV400 EV, providing bench-level as well as vehicle level environmental testing and production.\nCollaborated with cross-functional teams like ADAS, HMI and wiring harness to incorporate architecture changes.\nLed and completed 2 core Android Auto certifications for mentioned projects with software testing on various levels - PCTS Verifier, QSuite, UX, GPS/Navigation, sensor logging and final software validation; resulting in 15% improvement in sales.\nPrepared SRD (Software Requirements Definition) as a combined activity with the supplier; Improved data management processes by 12%.\nAuthored technical documentation, design specifications, test plans, & streamline project development; Integrated OTA updates, GPS benchmark and connectivity features for enhanced user experiences.\nExperienced in writing test cases from system requirements and customer use cases.\nCollaborated with tier 1 suppliers and 3PL partners Visteon and Harman International for software validation; Received green light from Google for Android Auto deployments with ~ 93.5% rectification of bugs and issues in the initial release; Efficiency improvements by 2% each cycle.\nStrong foundations in CAN, Diagnostics, Android and QNX architecture; Diagnostics tools like CANalyser, CANoe, Garuda 2.0."
  },
  {
    "objectID": "cv/cv.html#software-engineer-intern-oracle-cerner-bangalore-karnataka-india-jan-2022-july-2022",
    "href": "cv/cv.html#software-engineer-intern-oracle-cerner-bangalore-karnataka-india-jan-2022-july-2022",
    "title": "Curriculum Vitae",
    "section": "Software Engineer Intern | Oracle Cerner | Bangalore, Karnataka, India Jan 2022 â€“ July 2022",
    "text": "Software Engineer Intern | Oracle Cerner | Bangalore, Karnataka, India Jan 2022 â€“ July 2022\n\nCollaborated with cross-functional teams to develop and deploy healthcare software solutions.\nDesigned and implemented secure and scalable software components using Java (spring boot), Python, React, and JavaScript.\nConducted code reviews, debugging, and testing, reducing software defects by 20%.\nEmployed machine learning to cluster suppliers using medical device data, saving the team 10 work hours per week.\nGathered requirements from healthcare professionals and ensured compliance with regulations.\nActively participated in Agile methodologies with practical exposure to machine learning for client data.\nAssignment of beds and nurses to patients in different units done through web application with Spring Security in backend.\n95% average unit test coverage for the mainstream application using Mockito framework and Jest/enzyme framework.\nDevOps: CI/CD of the product upon Jenkins with pipeline staging scripts for automation, integration and deployment.\nEfficiently reduced size of MySQL database by 7% with scripts; improving the product function by ~ 20%."
  },
  {
    "objectID": "cv/cv.html#projects",
    "href": "cv/cv.html#projects",
    "title": "Curriculum Vitae",
    "section": "PROJECTS",
    "text": "PROJECTS"
  },
  {
    "objectID": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib",
    "href": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib",
    "title": "Curriculum Vitae",
    "section": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib",
    "text": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib\n\nPresented Comparative Image analysis between the two data hiding techniques are made on the reconstructed image to conclude which Steganography method achieves better results, Least Significant Bit (LSB) or Discrete Cosine Transform (DST)\nAutomated the generation and evaluation of ~25,000 images, achieving an 87% pose detection accuracy from the model"
  },
  {
    "objectID": "cv/cv.html#idea-repository-api-spring-boot-redux-saga-postman-api-json-git-jenkins-mysql-oracle-database-react-framework",
    "href": "cv/cv.html#idea-repository-api-spring-boot-redux-saga-postman-api-json-git-jenkins-mysql-oracle-database-react-framework",
    "title": "Curriculum Vitae",
    "section": "Idea Repository API | Spring boot, Redux Saga, Postman API, JSON, Git, Jenkins, MySQL, Oracle Database, React Framework",
    "text": "Idea Repository API | Spring boot, Redux Saga, Postman API, JSON, Git, Jenkins, MySQL, Oracle Database, React Framework\n\nAn API for a full-stack web application to establish user security - authorization and authentication at the backend with Redux-based UI focus on frontend.\nPublished as a technical paper - web application improved test user satisfaction ratings by 15%\nEmphasis on using redux-saga middleware instead of Thunk - faster web page response; improved scrolling efficiency by 10%\nGitHub: https://github.com/NishantBharali/projects"
  },
  {
    "objectID": "cv/cv.html#digital-hearing-aid-system-using-matlab-simulation-matlab-gui",
    "href": "cv/cv.html#digital-hearing-aid-system-using-matlab-simulation-matlab-gui",
    "title": "Curriculum Vitae",
    "section": "Digital Hearing Aid System using MATLAB | Simulation, MATLAB (GUI)",
    "text": "Digital Hearing Aid System using MATLAB | Simulation, MATLAB (GUI)\n\nDesigned a digital hearing aid system using MATLAB using Digital Signal Processing. The implementation of this configurable DHA system includes noise reduction filter, frequency shaper function and amplitude compression function.\nThe DHA design is designed to adapt for mild and moderate hearing loss patients since different gain can be set up to map different levels of hearing loss.\nThe code written in MATLAB, loads the input wave signal and takes the sampling frequency and the number of bits of that signal. Then, AWGN (Additive white Gaussian noise) and random noise are added to the signal before they are processed by various MATLAB functions to get an output which is audible to the hearing impaired person."
  },
  {
    "objectID": "cv/cv.html#detection-of-number-plate-and-identification-of-number-using-matlab-platform-simulation-matlab-simulink",
    "href": "cv/cv.html#detection-of-number-plate-and-identification-of-number-using-matlab-platform-simulation-matlab-simulink",
    "title": "Curriculum Vitae",
    "section": "Detection of Number Plate and Identification of Number using MATLAB | Platform Simulation, MATLAB, Simulink",
    "text": "Detection of Number Plate and Identification of Number using MATLAB | Platform Simulation, MATLAB, Simulink\n\nOur project focuses on developing an automated number plate recognition system, streamlining the process of plate detection and information storage. As vehicles enter a secure area, our system automatically captures and stores their number plates, replacing manual data collection for improved accuracy.\nThe project operates on a supervised method, utilizing a reference database for comparison. It comprises three key components: reference creation, plate detection, and alphabet/digit identification."
  },
  {
    "objectID": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib-1",
    "href": "cv/cv.html#information-data-hiding-using-steganography-techniques-opencv-pillow-scikit-image-numpy-matplotlib-1",
    "title": "Curriculum Vitae",
    "section": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib",
    "text": "Information Data Hiding using Steganography Techniques | OpenCV, Pillow, SciKit-Image, NumPy, Matplotlib\n\nPresented Comparative Image analysis between the two data hiding techniques are made on the reconstructed image to conclude which Steganography method achieves better results, Least Significant Bit (LSB) or Discrete Cosine Transform (DST).\nAutomated the generation and evaluation of 35,000 images, achieving an 87% pose detection accuracy from the model."
  },
  {
    "objectID": "cv/cv.html#adaptive-traffic-light-system-using-8051-microcontroller-8051-micrcontroller-at89c51-leds-7-segment-display-ir-sensors-proteus-simulation",
    "href": "cv/cv.html#adaptive-traffic-light-system-using-8051-microcontroller-8051-micrcontroller-at89c51-leds-7-segment-display-ir-sensors-proteus-simulation",
    "title": "Curriculum Vitae",
    "section": "Adaptive Traffic Light System using 8051 Microcontroller | 8051 Micrcontroller (AT89C51), LEDs, 7 Segment Display, IR Sensors, Proteus Simulation",
    "text": "Adaptive Traffic Light System using 8051 Microcontroller | 8051 Micrcontroller (AT89C51), LEDs, 7 Segment Display, IR Sensors, Proteus Simulation\n\nDesigned an adaptive traffic light system, which allots different time frames based on the density of traffic to a certain lane.\nUnder current circumstances, traffic lights are set in different directions with a fixed time delay, following a particular cycle which will be conveyed by the LEDs with the time delay in consideration."
  },
  {
    "objectID": "cv/cv.html#skills",
    "href": "cv/cv.html#skills",
    "title": "Curriculum Vitae",
    "section": "SKILLS",
    "text": "SKILLS\n\nTechnical Skills: Python, Keras, SciKit-Image (sci-kit learn), Java, MATLAB, SQL, CAN, LIN, HTML, CSS, React, CI/CD, Linux bash, RESTful API, Redux-saga, Spring Framework, Jupyter, Microcontroller 8051, Simulink, SOLIDWORKS, Verilog\nFramework and Tools: OpenCV, Matplotlib (Pillow), Vector CANalyzer, CANoe, Wireshark, Android and QNX architecture, Postman, Git Jenkins, Microservices, ROS, Visual Studio, Gazebo, MySQL\nOther: Agile, Scrum,JIRA, Product& Project Management, MicrosoftOffice Suite, eLMS"
  },
  {
    "objectID": "cv/cv.html#others-and-organizations",
    "href": "cv/cv.html#others-and-organizations",
    "title": "Curriculum Vitae",
    "section": "OTHERS AND ORGANIZATIONS",
    "text": "OTHERS AND ORGANIZATIONS\n\nSAE Autodrive Challenge (Fall 2023) : Participating in the Vehicle Control and testing sub-team under working on Q23-24 cycle learning through training and workshops on topics like Machine Vision, ROS2 and MATLAB GUIs\nUndergraduate Teaching Assistant for the course Digital Logic Design (ECE2003), VIT Vellore (2019-2021) : Secured Silver Rank in IoT - Domain Specialist conforming to National Skills Qualifications Framework Level 8, 2021\nAssistant Web Developer at IEEE IAS, VIT Vellore, 2020\nCore Committee Member, IEEE - Circuits and Systems Society, 2019-2020\nCore Committee Member, IEEE Computer Society, 2019-2022"
  },
  {
    "objectID": "cv/cv.html#publications",
    "href": "cv/cv.html#publications",
    "title": "Curriculum Vitae",
    "section": "PUBLICATION(S)",
    "text": "PUBLICATION(S)\nFull Stack WebDevelopment of Redux-based Applications with Dynamic Microservices(Case Study - IDEA REPOSITORY), 2022-2023\n\nSuccessfully certified and published the technical research paper in a peer-reviewed journal with an acceptable impact factor where the synopsis of the paper was web development strategy to develop applications based on redux using redux-saga middleware instead of its native Thunk middleware for faster web page response and to improve scrolling efficiency by 10%.\nUse of the MySQL server and spring framework was incorporated for dynamic usage of the backend while the saga middleware at the frontend uses the data stored in redux store dynamically and asynchronously to facilitate faster and responsive web page."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Nishant Bharali",
    "section": "",
    "text": "Greetings! My name is Nishant Bharali. \nIâ€™m an engineer with experiences in software modeling, analytics, vehicle controls, machine vision and machine learning. For a living, I get to solve problems with data, currently pursuing my Masterâ€™s in Computer Engineering.\nLife is not an experience without obstacles worth thriving against!\n\nProgramming\nI use Python, Java, MATLAB and C++ to develop and deploy packages, web apps, automation pipelines, machine learning workflows, and websites (this site was built with Quarto). My daily toolkit also includes Python.\n\n\nWeb App Development\nMy framework of choice for web app development is Java + Spring + React + Jenkins. With Spring, React and Jenkins, I can build enterprise-grade UIs on top of Bootstrap 5 that can be infinitely styled with HTML, Sass, CSS, JavaScript, and more.\n\n\nData Science Communities\nThere are a plethora of online and in-person data science communities to learn from and share your own experiences. Online communities for open-source languages and data science are incredibly welcoming and below are my favorites:\n\nData Science Hangout\nThe Ravit Show\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "posts/Classification/index.html",
    "href": "posts/Classification/index.html",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "",
    "text": "Implementing advanced classification techniques for precise model prediction analysis to enhance accuracy and efficiency"
  },
  {
    "objectID": "posts/Classification/index.html#advanced-classification-techniques-with-model-prediction-analysis",
    "href": "posts/Classification/index.html#advanced-classification-techniques-with-model-prediction-analysis",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "",
    "text": "Implementing advanced classification techniques for precise model prediction analysis to enhance accuracy and efficiency"
  },
  {
    "objectID": "posts/Classification/index.html#introduction",
    "href": "posts/Classification/index.html#introduction",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "Introduction",
    "text": "Introduction\nMachine learning is a fascinating field that empowers computers to learn and make predictions or decisions without being explicitly programmed. One of the fundamental tasks in machine learning is classification, where the goal is to categorize data points into predefined classes or labels. In this blog post, we dive deep into the world of classification, exploring advanced techniques and their application on a real-world dataset.\nThe Iris dataset is a well-known benchmark in the machine learning community. It consists of measurements of four features from three different species of iris flowers. This seemingly simple dataset serves as an excellent playground for understanding and implementing classification algorithms. However, we wonâ€™t stop at the basics; weâ€™ll explore advanced classification techniques, model tuning, and even dive into ensemble methods and neural networks.\nData Loading and Preliminary Analysis\n\n# Importing essential libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Load dataset\niris_df = pd.read_csv('Iris_dataset.csv')\n\n# Basic dataset information\nprint(iris_df.head())\nprint(iris_df.describe())\nprint(iris_df.info())\n\n# Visualizing the distribution of classes\nsns.countplot(x='species', data=iris_df)\nplt.show()\n\n# Pairplot to explore relationships between features\nsns.pairplot(iris_df, hue='species')\nplt.show()\n\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0           6.370695          2.771952           5.118790          1.542084   \n1           5.979286          2.612095           5.086595          1.560228   \n2           6.741563          2.804321           4.758669          1.443702   \n3           6.346538          2.796799           5.601084          2.114922   \n4           5.558280          2.831451           4.876331          2.045125   \n\n      species  \n0   virginica  \n1  versicolor  \n2  versicolor  \n3   virginica  \n4   virginica  \n       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount        3000.000000       3000.000000        3000.000000   \nmean            5.844003          3.055776           3.756746   \nstd             0.825203          0.435899           1.761100   \nmin             4.180767          1.918769           0.907839   \n25%             5.118694          2.780525           1.559288   \n50%             5.777010          3.025634           4.347984   \n75%             6.416866          3.342169           5.098831   \nmax             7.963257          4.516746           7.046886   \n\n       petal width (cm)  \ncount       3000.000000  \nmean           1.199022  \nstd            0.760822  \nmin           -0.048888  \n25%            0.310048  \n50%            1.326659  \n75%            1.818514  \nmax            2.601413  \n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 3000 entries, 0 to 2999\nData columns (total 5 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  3000 non-null   float64\n 1   sepal width (cm)   3000 non-null   float64\n 2   petal length (cm)  3000 non-null   float64\n 3   petal width (cm)   3000 non-null   float64\n 4   species            3000 non-null   object \ndtypes: float64(4), object(1)\nmemory usage: 117.3+ KB\nNone\n\n\n\n\n\n\n\n\nData Preprocessing\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Encoding categorical data\nencoder = LabelEncoder()\niris_df['species'] = encoder.fit_transform(iris_df['species'])\n\n# Splitting dataset into features and target variable\nX = iris_df.drop('species', axis=1)\ny = iris_df['species']\n\n# Splitting dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nExploratory Data Analysis (EDA)\n\n# Correlation heatmap\nplt.figure(figsize=(10, 8))\nsns.heatmap(iris_df.corr(), annot=True, cmap='viridis')\nplt.show()\n\n# Advanced pairplot with distribution and regression\nsns.pairplot(iris_df, kind='reg', hue='species')\nplt.show()"
  },
  {
    "objectID": "posts/Classification/index.html#model-building-and-evaluation-for-classification",
    "href": "posts/Classification/index.html#model-building-and-evaluation-for-classification",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "Model Building and Evaluation For Classification",
    "text": "Model Building and Evaluation For Classification\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Function to train and evaluate models\ndef train_evaluate_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    print(f'Model: {model.__class__.__name__}')\n    print(classification_report(y_test, predictions))\n    # Confusion matrix\n    cm = confusion_matrix(y_test, predictions)\n    sns.heatmap(cm, annot=True)\n    plt.show()\n\n# Decision Tree Classifier\ntrain_evaluate_model(DecisionTreeClassifier(), X_train, y_train, X_test, y_test)\n\n# RandomForestClassifier\ntrain_evaluate_model(RandomForestClassifier(), X_train, y_train, X_test, y_test)\n\n# GradientBoostingClassifier\ntrain_evaluate_model(GradientBoostingClassifier(), X_train, y_train, X_test, y_test)\n\n# Support Vector Machine (SVC)\ntrain_evaluate_model(SVC(), X_train, y_train, X_test, y_test)\n\n# K-Nearest Neighbors (KNN)\ntrain_evaluate_model(KNeighborsClassifier(), X_train, y_train, X_test, y_test)\n\n# Logistic Regression\ntrain_evaluate_model(LogisticRegression(), X_train, y_train, X_test, y_test)\n\nModel: DecisionTreeClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.98      0.99       299\n           2       0.98      0.99      0.99       312\n\n    accuracy                           0.99       900\n   macro avg       0.99      0.99      0.99       900\nweighted avg       0.99      0.99      0.99       900\n\nModel: RandomForestClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\nModel: GradientBoostingClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\nModel: SVC\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.96      0.98       299\n           2       0.97      0.99      0.98       312\n\n    accuracy                           0.98       900\n   macro avg       0.99      0.98      0.98       900\nweighted avg       0.98      0.98      0.98       900\n\nModel: KNeighborsClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       1.00      1.00      1.00       299\n           2       1.00      1.00      1.00       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\nModel: LogisticRegression\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.98      0.95      0.97       299\n           2       0.96      0.98      0.97       312\n\n    accuracy                           0.98       900\n   macro avg       0.98      0.98      0.98       900\nweighted avg       0.98      0.98      0.98       900\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn this code, we introduce different types of classification models, including Random Forest Classifier, Gradient Boosting Classifier, Support Vector Classifier (SVC), K-Nearest Neighbors Classifier (KNN), and Logistic Regression.\nFor each model, we train it on the training data and evaluate its performance using accuracy and a classification report that includes precision, recall, and F1-score. This allows you to compare the performance of various classification algorithms on the Iris dataset.\nAdvanced Model Tuning and Analysis\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for RandomForestClassifier\nparam_grid = {'n_estimators': [10, 50, 100], 'max_features': ['sqrt', 'log2', None]}\ngrid_search = GridSearchCV(RandomForestClassifier(), param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\nbest_rf = grid_search.best_estimator_\n\n# Evaluating the tuned model\ntrain_evaluate_model(best_rf, X_train, y_train, X_test, y_test)\n\nModel: RandomForestClassifier\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900\n\n\n\n\n\n\nHyperparameter Tuning for the Classification Model\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Proper Hyperparameter tuning for Random Forest Classifier\nparam_grid_rf = {\n    'n_estimators': [50, 100, 200],\n    'max_depth': [None, 10, 20],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search_rf = GridSearchCV(RandomForestClassifier(), param_grid_rf, cv=5)\ngrid_search_rf.fit(X_train, y_train)\n\nbest_rf_classifier = grid_search_rf.best_estimator_\n\n# Evaluating the tuned Random Forest Classifier\ntuned_rf_predictions = best_rf_classifier.predict(X_test)\nprint(\"Tuned Random Forest Classifier - Model Evaluation\")\nprint(\"Accuracy:\", accuracy_score(y_test, tuned_rf_predictions))\nprint(\"Classification Report:\")\nprint(classification_report(y_test, tuned_rf_predictions))\n\nTuned Random Forest Classifier - Model Evaluation\nAccuracy: 0.9955555555555555\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       0.99      0.99      0.99       299\n           2       0.99      0.99      0.99       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900"
  },
  {
    "objectID": "posts/Classification/index.html#roc-curve-analysis-for-multiple-models",
    "href": "posts/Classification/index.html#roc-curve-analysis-for-multiple-models",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "ROC Curve Analysis for Multiple Models",
    "text": "ROC Curve Analysis for Multiple Models\nComparing the performance of the various classification models using ROC Curve analysis, here we discuss the plots of the ROC Curve for each model. This will involve calculating the True Positive Rate (TPR) and False Positive Rate (FPR) for each model and plotting them.\n\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nimport matplotlib.pyplot as plt\n\n# Binarize the output classes for ROC analysis\ny_bin = label_binarize(y, classes=[0, 1, 2])\nn_classes = y_bin.shape[1]\n\n# Splitting the data again for multiclass ROC analysis\nX_train, X_test, y_train_bin, y_test_bin = train_test_split(X, y_bin, test_size=0.3, random_state=42)\n\n# Classifier list\nclassifiers = [\n    OneVsRestClassifier(DecisionTreeClassifier()),\n    OneVsRestClassifier(RandomForestClassifier()),\n    OneVsRestClassifier(GradientBoostingClassifier()),\n    OneVsRestClassifier(SVC(probability=True)),\n    OneVsRestClassifier(KNeighborsClassifier()),\n    OneVsRestClassifier(LogisticRegression())\n]\n\n# Plotting ROC Curves\nplt.figure(figsize=(10, 8))\n\n# Compute ROC curve and ROC area for each class\nfor classifier in classifiers:\n    classifier.fit(X_train, y_train_bin)\n    y_score = classifier.predict_proba(X_test)\n\n    # Compute ROC curve and ROC area for each class\n    for i in range(n_classes):\n        fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_score[:, i])\n        roc_auc = auc(fpr, tpr)\n        plt.plot(fpr, tpr, label=f'{classifier.estimator.__class__.__name__} (area = {roc_auc:.2f})')\n\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic for Multi-class')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\nThis code will generate ROC curves for each of the classifiers used, providing a visual comparison of their performance in terms of the trade-off between the True Positive Rate and False Positive Rate. The area under the curve (AUC) is also displayed as a measure of the modelâ€™s performance, with a higher AUC indicating a better model. This analysis is crucial for understanding the performance of classification models, especially in multi-class settings.\nDimensionality Reduction and Visualization\n\nfrom sklearn.decomposition import PCA\n\n# PCA for dimensionality reduction\npca = PCA(n_components=2)\nX_train_pca = pca.fit_transform(X_train)\nX_test_pca = pca.transform(X_test)\n\n# Visualizing PCA results\nplt.figure(figsize=(8, 6))\nplt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', label='Train set')\nplt.scatter(X_test_pca[:, 0], X_test_pca[:, 1], c=y_test, cmap='plasma', label='Test set', marker='x')\nplt.xlabel('First principal component')\nplt.ylabel('Second principal component')\nplt.legend()\nplt.title('PCA of Iris Dataset')\nplt.show()"
  },
  {
    "objectID": "posts/Classification/index.html#additional-advanced-analysis",
    "href": "posts/Classification/index.html#additional-advanced-analysis",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "Additional Advanced Analysis",
    "text": "Additional Advanced Analysis\n1. Cross-Validation\nCross-validation is a technique used to evaluate the generalizability of a model by training and testing it on different subsets of the dataset.\n\nfrom sklearn.model_selection import cross_val_score\n\n# Example using RandomForestClassifier\nrf_classifier = RandomForestClassifier()\n\n# Performing 10-fold cross-validation\ncv_scores = cross_val_score(rf_classifier, X, y, cv=10)\n\nprint(\"Cross-Validation Scores for RandomForestClassifier:\", cv_scores)\nprint(\"Average Score:\", np.mean(cv_scores))\n\nCross-Validation Scores for RandomForestClassifier: [0.99666667 1.         0.99333333 0.99333333 1.         0.99666667\n 0.99666667 0.99333333 1.         1.        ]\nAverage Score: 0.9969999999999999\n\n\n2. Ensemble Methods\nEnsemble methods combine multiple models to improve the overall performance. Here, I will use an ensemble of different classifiers.\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import VotingClassifier\n\n# Update Logistic Regression in the ensemble\nclassifiers = [\n    ('Decision Tree', DecisionTreeClassifier()),\n    ('Random Forest', RandomForestClassifier()),\n    ('Gradient Boosting', GradientBoostingClassifier()),\n    ('SVC', SVC(probability=True)),\n    ('KNN', KNeighborsClassifier()),\n    ('Logistic Regression', LogisticRegression(max_iter=1000))\n]\n\nensemble = VotingClassifier(estimators=classifiers, voting='soft')\nensemble.fit(X_train, y_train)\nensemble_predictions = ensemble.predict(X_test)\n\nprint(\"Ensemble Model Classification Report:\")\nprint(classification_report(y_test, ensemble_predictions))\n\nEnsemble Model Classification Report:\n              precision    recall  f1-score   support\n\n           0       1.00      1.00      1.00       289\n           1       1.00      0.99      0.99       299\n           2       0.99      1.00      1.00       312\n\n    accuracy                           1.00       900\n   macro avg       1.00      1.00      1.00       900\nweighted avg       1.00      1.00      1.00       900"
  },
  {
    "objectID": "posts/Classification/index.html#prediction",
    "href": "posts/Classification/index.html#prediction",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "PREDICTION",
    "text": "PREDICTION\nPrediction: Data Preprocessing with Visualization\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Encoding categorical data\nencoder = LabelEncoder()\niris_df['species'] = encoder.fit_transform(iris_df['species'])\n\n# Visualizing the distribution of the target variable\nplt.figure(figsize=(8, 5))\nsns.countplot(x='species', data=iris_df)\nplt.title('Distribution of Target Variable (Species)')\nplt.show()\n\n# Splitting dataset into features and target variable\nX = iris_df.drop('species', axis=1)\ny = iris_df['species']\n\n# Splitting dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# Feature scaling\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# Pairplot to explore relationships between features\npairplot_df = iris_df.copy()\npairplot_df['species'] = encoder.inverse_transform(pairplot_df['species'])\n\nplt.figure(figsize=(10, 8))\nsns.pairplot(pairplot_df, hue='species')\nplt.title('Pairplot of Features with Species as Hue')\nplt.show()\n\n\n\n\n&lt;Figure size 960x768 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/Classification/index.html#model-building-evaluation-and-visualization-for-prediction",
    "href": "posts/Classification/index.html#model-building-evaluation-and-visualization-for-prediction",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "Model Building, Evaluation, and Visualization for Prediction",
    "text": "Model Building, Evaluation, and Visualization for Prediction\n\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\n\n# Function to train and evaluate regression models\ndef train_evaluate_regression_model(model, X_train, y_train, X_test, y_test):\n    model.fit(X_train, y_train)\n    predictions = model.predict(X_test)\n    \n    # Model Evaluation\n    print(f'Model: {model.__class__.__name__}')\n    print(f'Mean Squared Error: {mean_squared_error(y_test, predictions)}')\n    print(f'R-squared (R2) Score: {r2_score(y_test, predictions)}')\n    \n    # Visualization\n    plt.figure(figsize=(8, 5))\n    plt.scatter(y_test, predictions)\n    plt.xlabel('True Values')\n    plt.ylabel('Predictions')\n    plt.title(f'{model.__class__.__name__} - True vs. Predicted Values')\n    plt.show()\n\n# Linear Regression\ntrain_evaluate_regression_model(LinearRegression(), X_train, y_train, X_test, y_test)\n\n# Decision Tree Regressor\ntrain_evaluate_regression_model(DecisionTreeRegressor(), X_train, y_train, X_test, y_test)\n\n# Random Forest Regressor\ntrain_evaluate_regression_model(RandomForestRegressor(), X_train, y_train, X_test, y_test)\n\nModel: LinearRegression\nMean Squared Error: 0.047703003079178956\nR-squared (R2) Score: 0.9284946222241109\nModel: DecisionTreeRegressor\nMean Squared Error: 0.0077777777777777776\nR-squared (R2) Score: 0.9883413432623143\nModel: RandomForestRegressor\nMean Squared Error: 0.006041444444444443\nR-squared (R2) Score: 0.990944055102883\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Model Tuning and Analysis for Prediction\n\nfrom sklearn.model_selection import GridSearchCV\n\n# Hyperparameter tuning for Random Forest Regressor\nparam_grid = {\n    'n_estimators': [10, 50, 100],\n    'max_depth': [None, 10, 20, 30],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(RandomForestRegressor(), param_grid, cv=5, scoring='neg_mean_squared_error')\ngrid_search.fit(X_train, y_train)\n\nbest_rf = grid_search.best_estimator_\n\n# Evaluating the tuned model\npredictions = best_rf.predict(X_test)\nprint(\"Tuned Random Forest Regressor - Model Evaluation\")\nprint(f'Mean Squared Error: {mean_squared_error(y_test, predictions)}')\nprint(f'R-squared (R2) Score: {r2_score(y_test, predictions)}')\n\nTuned Random Forest Regressor - Model Evaluation\nMean Squared Error: 0.0072306666666666665\nR-squared (R2) Score: 0.9891614464876909\n\n\nFeature Selection and Importance Analysis\nFeature selection is crucial for improving model performance and reducing overfitting. Here, we use techniques like Recursive Feature Elimination (RFE) and feature importance analysis to select the most relevant features for classification or prediction.\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# Using Recursive Feature Elimination (RFE) with Logistic Regression\nmodel = LogisticRegression()\nrfe = RFE(model, n_features_to_select=3)  # Select the top 3 features\nfit = rfe.fit(X_train, y_train)\n\n# List of selected features\nselected_features = [feature for idx, feature in enumerate(X.columns) if fit.support_[idx]]\nprint(\"Selected Features:\", selected_features)\n\nSelected Features: ['sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n\n\nAdditionally, we can analyze feature importance for tree-based models like Random Forest or Gradient Boosting to understand which features contribute the most to predictions.\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Feature Importance Analysis for Random Forest Classifier\nrf_model = RandomForestClassifier()\nrf_model.fit(X_train, y_train)\n\n# Plot feature importance\nfeature_importance = pd.Series(rf_model.feature_importances_, index=X.columns)\nfeature_importance.nlargest(5).plot(kind='barh')\nplt.title(\"Feature Importance (Random Forest)\")\nplt.show()\n\n\n\n\nHandling Class Imbalance\nIn real-world datasets, class imbalance is common, where one class has significantly fewer samples than others. Techniques like oversampling, undersampling, and Synthetic Minority Over-sampling Technique (SMOTE) can be employed to address this issue.\n\nfrom imblearn.over_sampling import SMOTE\n\n# Using SMOTE to handle class imbalance\nsmote = SMOTE(sampling_strategy='auto')\nX_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
  },
  {
    "objectID": "posts/Classification/index.html#conclusion",
    "href": "posts/Classification/index.html#conclusion",
    "title": "Advanced Classification techniques with Model Prediction Analysis",
    "section": "Conclusion",
    "text": "Conclusion\nIn this journey through the Iris dataset and the realm of classification, weâ€™ve covered a wide range of topics. Starting with data loading and preprocessing, we explored the relationships between features, ensuring that we understood our data thoroughly. We then introduced a variety of classification models, from decision trees to support vector machines, and compared their performance using robust evaluation metrics.\nBut we didnâ€™t stop there. We delved into advanced techniques, including cross-validation to ensure the generalizability of our models, ensemble methods that combined the strengths of multiple classifiers, and even a taste of neural networks for classification tasks.\nOur exploration of ROC curves allowed us to visualize and compare the trade-offs between true positive and false positive rates across different models, providing valuable insights into their performance.\nIn the end, classification is a powerful tool in the machine learning toolkit, with applications ranging from medical diagnosis to spam email filtering. The Iris dataset served as an ideal playground to learn and experiment with these techniques, but the knowledge gained can be applied to more complex and real-world classification problems.\nAs you continue your journey in machine learning, remember that classification is just the tip of the iceberg. The world of machine learning is vast and ever-evolving, and there are countless exciting challenges and opportunities awaiting those who dare to explore it further."
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html",
    "href": "posts/Linear and Non Linear Regression/index.html",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "",
    "text": "Utilizing linear and non-linear regression methodologies for the purpose of analyzing trends within the housing market"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#a-comprehensive-analysis-on-housing-market",
    "href": "posts/Linear and Non Linear Regression/index.html#a-comprehensive-analysis-on-housing-market",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "",
    "text": "Utilizing linear and non-linear regression methodologies for the purpose of analyzing trends within the housing market"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#introduction",
    "href": "posts/Linear and Non Linear Regression/index.html#introduction",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Introduction",
    "text": "Introduction\nIn machine learning, linear and nonlinear regression are fundamental techniques used to model relationships between variables, such as predicting housing prices based on various features in a dataset.\nLinear Regression is a straightforward method that assumes a linear relationship between the input features (e.g., square footage, number of bedrooms) and the target variable (housing price). It aims to find the best-fit line that minimizes the difference between predicted and actual values. Linear regression is interpretable and works well when the relationship is approximately linear.\nNonlinear Regression, on the other hand, allows for more complex relationships. It can capture curves, bends, and nonlinear patterns in the data. This is particularly useful when housing prices may depend on interactions between features or exhibit nonlinear behavior.\nThe housing dataset you are using provides a rich source of information to apply both linear and nonlinear regression techniques. By utilizing these methods, you can build predictive models that estimate housing prices accurately, taking into account the specific relationships between features and target variables, whether they are linear or nonlinear in nature. These models can guide real estate decisions, investment strategies, and market analyses more effectively, ultimately benefiting both buyers and sellers in the housing market.\nWe will start by loading the dataset and performing basic preprocessing, including encoding categorical variables and feature scaling.\nExploring the Dataset and Data Preprocessing\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import SimpleImputer\nimport seaborn as sns\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\n\n\n# Load the dataset\nhousing_df = pd.read_csv('modified_housing_data.csv')\n\n# Handling missing values if any\nimputer = SimpleImputer(strategy='mean')\nhousing_df[['Size', 'GardenArea']] = imputer.fit_transform(housing_df[['Size', 'GardenArea']])\n\n# One-hot encoding and Scaling\ncategorical_features = ['Neighborhood']\nnumerical_features = ['Size', 'Bedrooms', 'Bathrooms', 'Age', 'GarageSize', 'GardenArea']\n\n# Create transformers\none_hot = OneHotEncoder()\nscaler = StandardScaler()\n\n# Column transformer\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', scaler, numerical_features),\n        ('cat', one_hot, categorical_features)\n    ])\n\n# Preprocessing pipeline\nprep_pipeline = Pipeline(steps=[('preprocessor', preprocessor)])\n\n# Splitting the data\nX = housing_df.drop(['HouseID', 'Price'], axis=1)\ny = housing_df['Price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\nX_train_prep = prep_pipeline.fit_transform(X_train)\nX_test_prep = prep_pipeline.transform(X_test)\n\nVisualizing Market Trend\n\n#Visualizing the distributions and relationships\nsns.pairplot(housing_df)\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-linear-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-linear-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Linear Regression Analysis",
    "text": "Advanced Linear Regression Analysis\nFor the linear regression model, weâ€™ll include feature importance analysis and cross-validation.\n\n# Linear Regression Model\nlinear_model = LinearRegression()\nlinear_model.fit(X_train_prep, y_train)\n\n# Cross-Validation\ncv_scores = cross_val_score(linear_model, X_train_prep, y_train, cv=5, scoring='neg_mean_squared_error')\nprint(\"CV MSE for Linear Regression:\", -np.mean(cv_scores))\n\n# Predictions and Evaluation\ny_pred_linear = linear_model.predict(X_test_prep)\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression Test MSE:\", mse_linear)\nprint(\"Linear Regression Test R2:\", r2_linear)\n\nCV MSE for Linear Regression: 2618960221.1247973\nLinear Regression Test MSE: 2520672652.9396286\nLinear Regression Test R2: 0.9111757572070075"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Non-Linear Regression Analysis",
    "text": "Advanced Non-Linear Regression Analysis\nWeâ€™ll apply a more complex non-linear model, such as a Random Forest Regressor, and perform hyperparameter tuning using GridSearchCV.\n\n# Non-Linear Model - Random Forest Regressor\nrf_model = RandomForestRegressor(random_state=42)\n\n# Hyperparameter Grid\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_features': ['sqrt', 'log2', None],  # Removed 'auto' and added None\n    'max_depth': [10, 20, 30, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\n# Grid Search\ngrid_search = GridSearchCV(estimator=rf_model, param_grid=param_grid, cv=3, n_jobs=-1, scoring='neg_mean_squared_error', verbose=2)\ngrid_search.fit(X_train_prep, y_train)\n\n# Best Model\nbest_rf_model = grid_search.best_estimator_\n\n# Predictions and Evaluation\ny_pred_rf = best_rf_model.predict(X_test_prep)\nmse_rf = mean_squared_error(y_test, y_pred_rf)\nr2_rf = r2_score(y_test, y_pred_rf)\nprint(\"Random Forest Test MSE:\", mse_rf)\nprint(\"Random Forest Test R2:\", r2_rf)\n\nFitting 3 folds for each of 324 candidates, totalling 972 fits\nRandom Forest Test MSE: 3815896212.7555285\nRandom Forest Test R2: 0.865534268688408"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Regression Analysis",
    "text": "Advanced Regression Analysis\n1. Exploring Feature Interactions\nFeature interactions can reveal complex relationships that might not be captured by individual features alone.\n\n# Adding interaction terms\npoly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\nX_train_poly = poly.fit_transform(X_train_prep)\nX_test_poly = poly.transform(X_test_prep)\n\n# Re-training Linear Regression with Interaction Terms\nlinear_model_interact = LinearRegression()\nlinear_model_interact.fit(X_train_poly, y_train)\n\n# Evaluating the model with interaction terms\ny_pred_interact = linear_model_interact.predict(X_test_poly)\nmse_interact = mean_squared_error(y_test, y_pred_interact)\nprint(\"MSE with Interaction Terms:\", mse_interact)\n\nMSE with Interaction Terms: 2983950454.9349637\n\n\n2. Model Diagnostics for Linear Regression\nChecking assumptions and diagnostics of linear regression to ensure the validity of the model.\n\n# Model Diagnostics\nX_train_sm = sm.add_constant(X_train_prep)  # Adding a constant\nmodel = sm.OLS(y_train, X_train_sm).fit()\nprint(model.summary())\n\n# Residuals plot\nplt.scatter(model.predict(X_train_sm), model.resid)\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.axhline(y=0, color='red', linestyle='--')\nplt.title('Residuals vs Predicted')\nplt.show()\n\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                  Price   R-squared:                       0.917\nModel:                            OLS   Adj. R-squared:                  0.914\nMethod:                 Least Squares   F-statistic:                     283.3\nDate:                Tue, 09 Jan 2024   Prob (F-statistic):          3.91e-119\nTime:                        04:02:04   Log-Likelihood:                -2932.4\nNo. Observations:                 240   AIC:                             5885.\nDf Residuals:                     230   BIC:                             5920.\nDf Model:                           9                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst       4.798e+05   2602.372    184.377      0.000    4.75e+05    4.85e+05\nx1           1.56e+05   3262.250     47.819      0.000     1.5e+05    1.62e+05\nx2          5.035e+04   3267.184     15.410      0.000    4.39e+04    5.68e+04\nx3          2.194e+04   3304.761      6.640      0.000    1.54e+04    2.85e+04\nx4         -1.011e+04   3260.647     -3.100      0.002   -1.65e+04   -3684.133\nx5         -2624.2257   3281.530     -0.800      0.425   -9089.930    3841.478\nx6           -41.7240   3263.210     -0.013      0.990   -6471.330    6387.882\nx7          1.215e+05   5467.126     22.218      0.000    1.11e+05    1.32e+05\nx8           1.26e+05   5609.374     22.456      0.000    1.15e+05    1.37e+05\nx9          1.184e+05   6164.082     19.216      0.000    1.06e+05    1.31e+05\nx10         1.139e+05   5567.918     20.463      0.000    1.03e+05    1.25e+05\n==============================================================================\nOmnibus:                        0.912   Durbin-Watson:                   2.215\nProb(Omnibus):                  0.634   Jarque-Bera (JB):                0.697\nSkew:                          -0.123   Prob(JB):                        0.706\nKurtosis:                       3.097   Cond. No.                     6.20e+15\n==============================================================================\n\nNotes:\n[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n[2] The smallest eigenvalue is 7.83e-30. This might indicate that there are\nstrong multicollinearity problems or that the design matrix is singular.\n\n\n\n\n\n3. Learning Curves\nUnderstanding how model performance changes as the training set size increases.\n\n# Learning curve function\ndef plot_learning_curve(estimator, X, y, title):\n    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=5, n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5))\n    train_scores_mean = np.mean(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n\n    plt.figure()\n    plt.title(title)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n    plt.legend(loc=\"best\")\n    plt.grid()\n    plt.show()\n\n# Plotting learning curve for Linear Regression\nplot_learning_curve(linear_model, X_train_prep, y_train, \"Linear Regression Learning Curve\")\n\n\n\n\n4. Ensemble Methods\nCombining multiple regression models to improve predictive performance.\n\n# Gradient Boosting Regressor\ngb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\ngb_reg.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_gb = gb_reg.predict(X_test_prep)\nmse_gb = mean_squared_error(y_test, y_pred_gb)\nprint(\"Gradient Boosting Regressor MSE:\", mse_gb)\n\nGradient Boosting Regressor MSE: 3673304675.984175"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#deeper-regression-analysis",
    "href": "posts/Linear and Non Linear Regression/index.html#deeper-regression-analysis",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Deeper Regression Analysis",
    "text": "Deeper Regression Analysis\nFeature Importance-Based Selection\nFirst, letâ€™s use a model to identify the most important features and then retrain our models using only these features.\n\n# Feature Importance with Random Forest\nrf_for_importance = RandomForestRegressor()\nrf_for_importance.fit(X_train_prep, y_train)\n\n# Get feature importances and corresponding feature names\nimportances = rf_for_importance.feature_importances_\nfeature_names = prep_pipeline.get_feature_names_out()\n\n# Creating a DataFrame for visualization\nimportance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances}).sort_values(by='Importance', ascending=False)\n\n# Adjusting the threshold for feature selection\ntop_features = importance_df[importance_df['Importance'].cumsum() &lt;= 0.90]['Feature']\n\n# Ensure that top_features is not empty\nif len(top_features) == 0:\n    raise ValueError(\"No features selected. Consider loosening the feature selection criterion.\")\n\nX_train_top = X_train_prep[:, [feature_names.tolist().index(feat) for feat in top_features]]\nX_test_top = X_test_prep[:, [feature_names.tolist().index(feat) for feat in top_features]]\n\n# Re-train models with top features\nlinear_model_top = LinearRegression()\nlinear_model_top.fit(X_train_top, y_train)\n\n# Evaluation\ny_pred_top = linear_model_top.predict(X_test_top)\nmse_top = mean_squared_error(y_test, y_pred_top)\nprint(\"Top Features Linear Regression MSE:\", mse_top)\n\nTop Features Linear Regression MSE: 5967275811.385474"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-models",
    "href": "posts/Linear and Non Linear Regression/index.html#advanced-non-linear-models",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Advanced Non-Linear Models",
    "text": "Advanced Non-Linear Models\nIncorporating more complex non-linear models such as Support Vector Regression and Neural Networks.\n\n# Support Vector Regression\nsvr = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_svr = svr.predict(X_test_prep)\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nprint(\"Support Vector Regression MSE:\", mse_svr)\n\nnn_reg = MLPRegressor(hidden_layer_sizes=(100, 50), \n                      max_iter=5000, \n                      learning_rate_init=0.001, \n                      solver='adam', \n                      early_stopping=True, \n                      n_iter_no_change=10,\n                      random_state=42)\nnn_reg.fit(X_train_prep, y_train)\n\n# Evaluation\ny_pred_nn = nn_reg.predict(X_test_prep)\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nprint(\"Neural Network Regression MSE:\", mse_nn)\n\nSupport Vector Regression MSE: 28406556463.671677\nNeural Network Regression MSE: 397201687873.44415\n\n\nBuilding and Evaluating the Linear Regression Model\nA. Final Model Training\nWeâ€™ll train the final models using the best parameters found from previous steps.\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport matplotlib.pyplot as plt\n\n# Linear Regression\nlinear_model_final = LinearRegression()\nlinear_model_final.fit(X_train_prep, y_train)\n\n# Neural Network\nnn_model_final = MLPRegressor(hidden_layer_sizes=(100, 50), \n                              max_iter=5000, \n                              learning_rate_init=0.001, \n                              solver='adam', \n                              early_stopping=True, \n                              n_iter_no_change=10,\n                              random_state=42)\nnn_model_final.fit(X_train_prep, y_train)\n\n# Predictions\ny_pred_linear = linear_model_final.predict(X_test_prep)\ny_pred_nn = nn_model_final.predict(X_test_prep)\n\nB. Evaluation Metrics\nCalculating and printing evaluation metrics for both models. Weâ€™ll train the final models using the best parameters found from previous steps.\n\n# Evaluation for Linear Regression\nmse_linear = mean_squared_error(y_test, y_pred_linear)\nr2_linear = r2_score(y_test, y_pred_linear)\nprint(\"Linear Regression - MSE:\", mse_linear, \"R2:\", r2_linear)\n\n# Evaluation for Neural Network\nmse_nn = mean_squared_error(y_test, y_pred_nn)\nr2_nn = r2_score(y_test, y_pred_nn)\nprint(\"Neural Network - MSE:\", mse_nn, \"R2:\", r2_nn)\n\nLinear Regression - MSE: 2520672652.9396286 R2: 0.9111757572070075\nNeural Network - MSE: 397201687873.44415 R2: -12.996715964015438\n\n\nC. Plotting Model Performance\nVisualizing the performance of the models using scatter plots and residual plots.\nI. Scatter Plot for Predictions.\n\nplt.figure(figsize=(12, 6))\n\n# Scatter plot for Linear Regression\nplt.subplot(1, 2, 1)\nplt.scatter(y_test, y_pred_linear, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Predictions')\n\n# Scatter plot for Neural Network\nplt.subplot(1, 2, 2)\nplt.scatter(y_test, y_pred_nn, alpha=0.5, color='red')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Neural Network Predictions')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nII. Residual Plot\n\nplt.figure(figsize=(12, 6))\n\n# Residual plot for Linear Regression\nplt.subplot(1, 2, 1)\nplt.scatter(y_pred_linear, y_test - y_pred_linear, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred_linear.min(), xmax=y_pred_linear.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\n# Residual plot for Neural Network\nplt.subplot(1, 2, 2)\nplt.scatter(y_pred_nn, y_test - y_pred_nn, alpha=0.5, color='red')\nplt.hlines(y=0, xmin=y_pred_nn.min(), xmax=y_pred_nn.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Neural Network Residuals')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nBuilding and Evaluating Non-Linear Model (Support Vector Regression)\nA. Training the Support Vector Regressor\n\nfrom sklearn.svm import SVR\n\n# Support Vector Regression\nsvr_model_final = SVR(kernel='rbf', C=1.0, gamma='scale')\nsvr_model_final.fit(X_train_prep, y_train)\n\n# Predictions\ny_pred_svr = svr_model_final.predict(X_test_prep)\n\nB. Evaluation Metrics for SVR\n\n# Evaluation for Support Vector Regression\nmse_svr = mean_squared_error(y_test, y_pred_svr)\nr2_svr = r2_score(y_test, y_pred_svr)\nprint(\"Support Vector Regression - MSE:\", mse_svr, \"R2:\", r2_svr)\n\nSupport Vector Regression - MSE: 28406556463.671677 R2: -0.0009990251211158263\n\n\nC. Plotting Comparisons\nVisualizing the performance of the Linear Regression, Neural Network, and Support Vector Regression models.\n\nplt.figure(figsize=(18, 6))\n\n# Linear Regression\nplt.subplot(1, 3, 1)\nplt.scatter(y_test, y_pred_linear, alpha=0.5)\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Linear Regression Predictions')\n\n# Neural Network\nplt.subplot(1, 3, 2)\nplt.scatter(y_test, y_pred_nn, alpha=0.5, color='red')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Neural Network Predictions')\n\n# Support Vector Regression\nplt.subplot(1, 3, 3)\nplt.scatter(y_test, y_pred_svr, alpha=0.5, color='green')\nplt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'k--', lw=2)\nplt.xlabel('Actual')\nplt.ylabel('Predicted')\nplt.title('Support Vector Regression Predictions')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\nD. Residual Plot for All Models\n\nplt.figure(figsize=(18, 6))\n\n# Linear Regression Residuals\nplt.subplot(1, 3, 1)\nplt.scatter(y_pred_linear, y_test - y_pred_linear, alpha=0.5)\nplt.hlines(y=0, xmin=y_pred_linear.min(), xmax=y_pred_linear.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Linear Regression Residuals')\n\n# Neural Network Residuals\nplt.subplot(1, 3, 2)\nplt.scatter(y_pred_nn, y_test - y_pred_nn, alpha=0.5, color='red')\nplt.hlines(y=0, xmin=y_pred_nn.min(), xmax=y_pred_nn.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Neural Network Residuals')\n\n# SVR Residuals\nplt.subplot(1, 3, 3)\nplt.scatter(y_pred_svr, y_test - y_pred_svr, alpha=0.5, color='green')\nplt.hlines(y=0, xmin=y_pred_svr.min(), xmax=y_pred_svr.max(), colors='black', linestyles='dashed')\nplt.xlabel('Predicted')\nplt.ylabel('Residuals')\nplt.title('Support Vector Regression Residuals')\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "posts/Linear and Non Linear Regression/index.html#conclusion",
    "href": "posts/Linear and Non Linear Regression/index.html#conclusion",
    "title": "Advanced analysis on Linear and Non-Linear Regression models",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, the fields of random processes and probability theory, as well as linear and nonlinear regression, are vital components of machine learning when applied to diverse datasets such as weather and housing data. These foundational concepts empower us to model and make predictions in the face of inherent uncertainty, allowing for more accurate forecasts, informed decision-making, and improved insights.\nFor weather data analysis, random processes and probability theory enable us to understand and quantify the stochastic nature of weather patterns. Leveraging machine learning techniques on this data helps us provide accurate forecasts and anticipate extreme events, benefiting numerous sectors that rely on weather information.\nIn the case of housing data analysis, linear and nonlinear regression techniques enable us to model complex relationships between housing features and prices. Whether itâ€™s linear relationships for straightforward cases or nonlinear models to capture intricate patterns, these tools empower us to make more informed decisions in real estate, investments, and market analysis.\nIn both domains, machine learning applied to these fundamental concepts provides us with the means to extract valuable insights and make data-driven decisions, ultimately enhancing our understanding and predictive capabilities, and offering practical solutions that can improve the quality of life and the efficiency of various industries."
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "Projects",
    "section": "",
    "text": "Machine Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplied Control Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRobotics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Vision\n\n\n\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "Resume/Resume.html",
    "href": "Resume/Resume.html",
    "title": "Resume",
    "section": "",
    "text": "It appears you donâ€™t have a PDF plugin for this browser. You can click here to download the PDF file.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Robotics/Dynamics.html",
    "href": "Robotics/Dynamics.html",
    "title": "Robot Dynamics",
    "section": "",
    "text": "Here we will simulate the dynamics of the planar robot shown above. We have the dynamics of this robot mentioned in the code.\nNow we combine our simulation with the dynamics we have for the specified robot. We will need to get the (ğ‘¥, ğ‘¦) position of each link to plot the robot. We are using the given simulation parameters and frame rates.\n\nMake a simulation where \\(\\normalsize \\tau = [0, 0]^{ğ‘‡}\\) and the robot has no friction.\nMake a simulation where \\(\\normalsize \\tau = [0, 0]^{ğ‘‡}\\) and the robot has viscous friction ğµ = ğ¼.\nMake a simulation where \\(\\normalsize \\tau = [20, 5]^{ğ‘‡}\\) and the robot has viscous friction ğµ = ğ¼.\n\nWe can play with the parameters (such as mass, inertia, friction, and ğœ), and see how these parameters affect the simulation. We can see the implementations below for the simulation codes I am responsible for.\n\nEnvironment 1: Make a simulation where \\(\\normalsize \\tau = [0, 0]^{ğ‘‡}\\) and the robot has no friction.\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\nfrom scipy.spatial.transform import Rotation as R\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\n# System parameters\nL1 = 1\nL2 = 1\nm1 = 1\nm2 = 1\nI1 = 0.1\nI2 = 0.1\ng = 9.81\ntau = np.array([0, 0])  # Case 1\n\n# Initial conditions\ntheta = np.array([0.0, 0.0])\nthetadot = np.array([0.0, 0.0])\nthetadotdot = np.array([0.0, 0.0])\n\nomega = np.array([0, 0, 1])\nq1 = np.array([0, 0, 0])\nq2 = np.array([L1, 0, 0])\nq3 = np.array([L1 + L2, 0, 0])\n\nS1 = np.hstack((omega, -np.cross(omega, q1)))\nS2 = np.hstack((omega, -np.cross(omega, q2)))\nS_eq1 = np.column_stack((S1, np.zeros(6)))\nS_eq2 = np.column_stack((S1, S2))\n\nM1 = np.vstack((np.hstack((np.eye(3), q2[:, None])), [0, 0, 0, 1]))\nM2 = np.vstack((np.hstack((np.eye(3), np.array([[L1 + L2, 0, 0]]).T)), [0, 0, 0, 1]))\n\n# Plot setup\nmax_reach = L1 + L2\nfig, ax = plt.subplots()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.grid()\n\n# Animation setup\nframes = 1000\ndeltaT = 0.01\nlines, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\n\n\ndef animate(idx):\n    global theta, thetadot, thetadotdot\n    p0 = np.array([0,0])\n    T1 = fk(M1, S_eq2[:, 0:1], theta[0:1])\n    p1 = T1[0:2, 3]\n    T2 = fk(M2, S_eq2, theta)\n    p2 = T2[0:2, 3]\n    P = np.column_stack((p0, p1, p2))\n\n    lines.set_data(P[0, :], P[1, :])\n\n    thetadot += deltaT * thetadotdot\n    theta += deltaT * thetadot\n\n    # Dynamics and integration\n    Mass_Matrix = np.array([\n        [I1 + I2 + L1**2*m1 + L1**2*m2 + L2**2*m2 + 2*L1*L2*m2*np.cos(theta[1]), m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2],\n        [m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2, m2*L2**2 + I2]\n    ])\n\n    Coriolis_Matrix = np.array([\n        [-L1*L2*m2*thetadot[1]*np.sin(theta[1]), -L1*L2*m2*np.sin(theta[1])*(thetadot[0] + thetadot[1])],\n        [L1*L2*m2*thetadot[0]*np.sin(theta[1]), 0]\n    ])\n\n    gravity_vector = np.array([\n        g*(m1 + m2)*L1*np.cos(theta[0]) + g*m2*L2*np.cos(np.sum(theta)),\n        g*m2*L2*np.cos(np.sum(theta))\n    ])\n\n    B = np.array([[0, 0], [0, 0]])  # Case 1\n\n    thetadotdot = np.linalg.inv(Mass_Matrix) @ (tau - Coriolis_Matrix @ thetadot - B @ thetadot - gravity_vector)\n \n\n    return lines,\n\nani = animation.FuncAnimation(fig, animate, frames=frames, interval=deltaT*1000, blit=False)\n\n# Save as GIF\nani.save('Pendulum_Python_1.gif', writer=PillowWriter(fps=33))\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n\n% create figure\nfigure\naxis([-2, 2, -2, 2])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Pendulum_Case_1.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nL1 = 1;\nL2 = 1;\nm1 = 1;\nm2 = 1;\nI1 = 0.1;\nI2 = 0.1;\ng = 9.81;\ntau = [0;0]; % Case 1\n\n% Initial conditions\ntheta = [0;0]; % joint position\nthetadot = [0;0]; % joint velocity\nthetadotdot = [0;0]; % joint acceleration\n\nmasses = [m1,m2];\nomega = [0;0;1];\n\nInertia_1 = [0 0 0;0 0 0;0 0 I1];\nInertia_2 = [0 0 0;0 0 0;0 0 I2];\nq1 = [0;0;0]; % Position of Joint 1\nq2 = [L1;0;0]; % Position of Joint 2\nq3 = [L1+L2;0;0]; % end effector position\n\nS1 = [omega; -cross(omega,q1)];\nS2 = [omega;-cross(omega,q2)];\nS_eq1 = [S1,[0;0;0;0;0;0]];\nS_eq2 = [S1, S2]; \n\nM1 = [eye(3),q2; 0 0 0 1];\nM2 = [eye(3), [L1+L2;0;0]; 0 0 0 1];\n\ngravity_vector = (zeros(length(theta),1));\nCoriolis_Matrix = (zeros(2,2));\nMass_Matrix = [I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n\nfor idx = 1:1000\n\n    % plot the robot\n    % 1. get the position of each link\n    p0 = [0; 0];\n    T1 = fk(M1,S_eq2(:,1:1),theta(1:1,:));\n    p1 = T1(1:2,4); % position of link 1 (location of joint 2)\n    T2 = fk(M2,S_eq2,theta);\n    p2 = T2(1:2,4); % position of link 2 (the end-effector)\n    P = [p0, p1, p2];\n    % 2. draw the robot and save the frame\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4);\n    drawnow;\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n\n    % integrate to update velocity and position\n    % your code here\n    deltaT = 0.01;\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n\n    Mass_Matrix =[I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n    Coriolis_Matrix = [-L1*L2*m2*thetadot(2)*sin(theta(2)),  -L1*L2*m2*sin(theta(2))*(thetadot(1) + thetadot(2));L1*L2*m2*thetadot(1)*sin(theta(2)), 0] ;\n    \n    gravity_vector = [(g*(m1+m2)*L1*cos(theta(1))) + g*m2*L2*cos(theta(1) + theta(2)); g*m2*L2*cos(theta(1) + theta(2))];\n    \n    B = [[0 0]\n        [0 0]]; % Case 1\n\n    thetadotdot = (inv(Mass_Matrix)) * (tau - Coriolis_Matrix * thetadot -B*thetadot - gravity_vector);\n\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nEnvironment 2: Make a simulation where \\(\\normalsize \\tau = [0, 0]^ğ‘‡\\) and the robot has viscous friction \\(\\normalsize ğµ = ğ¼\\).\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\nfrom scipy.spatial.transform import Rotation as R\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\n# System parameters\nL1 = 1\nL2 = 1\nm1 = 1\nm2 = 1\nI1 = 0.1\nI2 = 0.1\ng = 9.81\ntau = np.array([0, 0])  # Case 2\n\n# Initial conditions\ntheta = np.array([0.0, 0.0])\nthetadot = np.array([0.0, 0.0])\nthetadotdot = np.array([0.0, 0.0])\n\nomega = np.array([0, 0, 1])\nq1 = np.array([0, 0, 0])\nq2 = np.array([L1, 0, 0])\nq3 = np.array([L1 + L2, 0, 0])\n\nS1 = np.hstack((omega, -np.cross(omega, q1)))\nS2 = np.hstack((omega, -np.cross(omega, q2)))\nS_eq1 = np.column_stack((S1, np.zeros(6)))\nS_eq2 = np.column_stack((S1, S2))\n\nM1 = np.vstack((np.hstack((np.eye(3), q2[:, None])), [0, 0, 0, 1]))\nM2 = np.vstack((np.hstack((np.eye(3), np.array([[L1 + L2, 0, 0]]).T)), [0, 0, 0, 1]))\n\n# Plot setup\nmax_reach = L1 + L2\nfig, ax = plt.subplots()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.grid()\n\n# Animation setup\nframes = 1000\ndeltaT = 0.01\nlines, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\n\n\ndef animate(idx):\n    global theta, thetadot, thetadotdot\n    p0 = np.array([0,0])\n    T1 = fk(M1, S_eq2[:, 0:1], theta[0:1])\n    p1 = T1[0:2, 3]\n    T2 = fk(M2, S_eq2, theta)\n    p2 = T2[0:2, 3]\n    P = np.column_stack((p0, p1, p2))\n\n    lines.set_data(P[0, :], P[1, :])\n\n    thetadot += deltaT * thetadotdot\n    theta += deltaT * thetadot\n\n    # Dynamics and integration\n    Mass_Matrix = np.array([\n        [I1 + I2 + L1**2*m1 + L1**2*m2 + L2**2*m2 + 2*L1*L2*m2*np.cos(theta[1]), m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2],\n        [m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2, m2*L2**2 + I2]\n    ])\n\n    Coriolis_Matrix = np.array([\n        [-L1*L2*m2*thetadot[1]*np.sin(theta[1]), -L1*L2*m2*np.sin(theta[1])*(thetadot[0] + thetadot[1])],\n        [L1*L2*m2*thetadot[0]*np.sin(theta[1]), 0]\n    ])\n\n    gravity_vector = np.array([\n        g*(m1 + m2)*L1*np.cos(theta[0]) + g*m2*L2*np.cos(np.sum(theta)),\n        g*m2*L2*np.cos(np.sum(theta))\n    ])\n\n    B = np.array([[1, 0], [0, 1]])  # Case 2\n\n    thetadotdot = np.linalg.inv(Mass_Matrix) @ (tau - Coriolis_Matrix @ thetadot - B @ thetadot - gravity_vector)\n \n\n    return lines,\n\nani = animation.FuncAnimation(fig, animate, frames=frames, interval=deltaT*1000, blit=False)\n\n# Save as GIF\nani.save('Pendulum_Python_2.gif', writer=PillowWriter(fps=33))\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-2, 2, -2, 2])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Pendulum_Case_2.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nL1 = 1;\nL2 = 1;\nm1 = 1;\nm2 = 1;\nI1 = 0.1;\nI2 = 0.1;\ng = 9.81;\ntau = [0;0]; % Case 2\n\n% Initial conditions\ntheta = [0;0]; % joint position\nthetadot = [0;0]; % joint velocity\nthetadotdot = [0;0]; % joint acceleration\n\nmasses = [m1,m2];\nomega = [0;0;1];\n\nInertia_1 = [0 0 0;0 0 0;0 0 I1];\nInertia_2 = [0 0 0;0 0 0;0 0 I2];\nq1 = [0;0;0]; % Position of Joint 1\nq2 = [L1;0;0]; % Position of Joint 2\nq3 = [L1+L2;0;0]; % end effector position\n\nS1 = [omega; -cross(omega,q1)];\nS2 = [omega;-cross(omega,q2)];\nS_eq1 = [S1,[0;0;0;0;0;0]];\nS_eq2 = [S1, S2]; \n\nM1 = [eye(3),q2; 0 0 0 1];\nM2 = [eye(3), [L1+L2;0;0]; 0 0 0 1];\n\ngravity_vector = (zeros(length(theta),1));\nCoriolis_Matrix = (zeros(2,2));\nMass_Matrix = [I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n\nfor idx = 1:1000\n\n    % plot the robot\n    % 1. get the position of each link\n    p0 = [0; 0];\n    T1 = fk(M1,S_eq2(:,1:1),theta(1:1,:));\n    p1 = T1(1:2,4); % position of link 1 (location of joint 2)\n    T2 = fk(M2,S_eq2,theta);\n    p2 = T2(1:2,4); % position of link 2 (the end-effector)\n    P = [p0, p1, p2];\n    % 2. draw the robot and save the frame\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4);\n    drawnow;\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n\n    % integrate to update velocity and position\n    % your code here\n    deltaT = 0.01;\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n\n    Mass_Matrix =[I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n    Coriolis_Matrix = [-L1*L2*m2*thetadot(2)*sin(theta(2)),  -L1*L2*m2*sin(theta(2))*(thetadot(1) + thetadot(2));L1*L2*m2*thetadot(1)*sin(theta(2)), 0] ;\n    \n    gravity_vector = [(g*(m1+m2)*L1*cos(theta(1))) + g*m2*L2*cos(theta(1) + theta(2)); g*m2*L2*cos(theta(1) + theta(2))];\n    \n    B = eye(2); % Case 2\n\n    thetadotdot = (inv(Mass_Matrix)) * (tau - Coriolis_Matrix * thetadot -B*thetadot - gravity_vector);\n\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nEnvironment 3: Make a simulation where \\(\\normalsize \\tau = [20, 5]^ğ‘‡\\) and the robot has viscous friction \\(\\normalsize ğµ = ğ¼\\).\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\nfrom scipy.spatial.transform import Rotation as R\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\n# System parameters\nL1 = 1\nL2 = 1\nm1 = 1\nm2 = 1\nI1 = 0.1\nI2 = 0.1\ng = 9.81\ntau = np.array([20, 5])  # Case 3\n\n# Initial conditions\ntheta = np.array([0.0, 0.0])\nthetadot = np.array([0.0, 0.0])\nthetadotdot = np.array([0.0, 0.0])\n\nomega = np.array([0, 0, 1])\nq1 = np.array([0, 0, 0])\nq2 = np.array([L1, 0, 0])\nq3 = np.array([L1 + L2, 0, 0])\n\nS1 = np.hstack((omega, -np.cross(omega, q1)))\nS2 = np.hstack((omega, -np.cross(omega, q2)))\nS_eq1 = np.column_stack((S1, np.zeros(6)))\nS_eq2 = np.column_stack((S1, S2))\n\nM1 = np.vstack((np.hstack((np.eye(3), q2[:, None])), [0, 0, 0, 1]))\nM2 = np.vstack((np.hstack((np.eye(3), np.array([[L1 + L2, 0, 0]]).T)), [0, 0, 0, 1]))\n\n# Plot setup\nmax_reach = L1 + L2\nfig, ax = plt.subplots()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.grid()\n\n# Animation setup\nframes = 1000\ndeltaT = 0.01\nlines, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\n\n\ndef animate(idx):\n    global theta, thetadot, thetadotdot\n    p0 = np.array([0,0])\n    T1 = fk(M1, S_eq2[:, 0:1], theta[0:1])\n    p1 = T1[0:2, 3]\n    T2 = fk(M2, S_eq2, theta)\n    p2 = T2[0:2, 3]\n    P = np.column_stack((p0, p1, p2))\n\n    lines.set_data(P[0, :], P[1, :])\n\n    thetadot += deltaT * thetadotdot\n    theta += deltaT * thetadot\n\n    # Dynamics and integration\n    Mass_Matrix = np.array([\n        [I1 + I2 + L1**2*m1 + L1**2*m2 + L2**2*m2 + 2*L1*L2*m2*np.cos(theta[1]), m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2],\n        [m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2, m2*L2**2 + I2]\n    ])\n\n    Coriolis_Matrix = np.array([\n        [-L1*L2*m2*thetadot[1]*np.sin(theta[1]), -L1*L2*m2*np.sin(theta[1])*(thetadot[0] + thetadot[1])],\n        [L1*L2*m2*thetadot[0]*np.sin(theta[1]), 0]\n    ])\n\n    gravity_vector = np.array([\n        g*(m1 + m2)*L1*np.cos(theta[0]) + g*m2*L2*np.cos(np.sum(theta)),\n        g*m2*L2*np.cos(np.sum(theta))\n    ])\n\n    B = np.array([[1, 0], [0, 1]])  # Case 3\n\n    thetadotdot = np.linalg.inv(Mass_Matrix) @ (tau - Coriolis_Matrix @ thetadot - B @ thetadot - gravity_vector)\n \n\n    return lines,\n\nani = animation.FuncAnimation(fig, animate, frames=frames, interval=deltaT*1000, blit=False)\n\n# Save as GIF\nani.save('Pendulum_Python_3.gif', writer=PillowWriter(fps=33))\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n\n% create figure\nfigure\naxis([-2, 2, -2, 2])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Pendulum_Python_3.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nL1 = 1;\nL2 = 1;\nm1 = 1;\nm2 = 1;\nI1 = 0.1;\nI2 = 0.1;\ng = 9.81;\ntau = [20;5]; % Case 3\n\n% Initial conditions\ntheta = [0;0]; % joint position\nthetadot = [0;0]; % joint velocity\nthetadotdot = [0;0]; % joint acceleration\n\nmasses = [m1,m2];\nomega = [0;0;1];\n\nInertia_1 = [0 0 0;0 0 0;0 0 I1];\nInertia_2 = [0 0 0;0 0 0;0 0 I2];\nq1 = [0;0;0]; % Position of Joint 1\nq2 = [L1;0;0]; % Position of Joint 2\nq3 = [L1+L2;0;0]; % end effector position\n\nS1 = [omega; -cross(omega,q1)];\nS2 = [omega;-cross(omega,q2)];\nS_eq1 = [S1,[0;0;0;0;0;0]];\nS_eq2 = [S1, S2]; \n\nM1 = [eye(3),q2; 0 0 0 1];\nM2 = [eye(3), [L1+L2;0;0]; 0 0 0 1];\n\ngravity_vector = (zeros(length(theta),1));\nCoriolis_Matrix = (zeros(2,2));\nMass_Matrix = [I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n\nfor idx = 1:1000\n\n    % plot the robot\n    % 1. get the position of each link\n    p0 = [0; 0];\n    T1 = fk(M1,S_eq2(:,1:1),theta(1:1,:));\n    p1 = T1(1:2,4); % position of link 1 (location of joint 2)\n    T2 = fk(M2,S_eq2,theta);\n    p2 = T2(1:2,4); % position of link 2 (the end-effector)\n    P = [p0, p1, p2];\n    % 2. draw the robot and save the frame\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4);\n    drawnow;\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n\n    % integrate to update velocity and position\n    % your code here\n    deltaT = 0.01;\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n\n    Mass_Matrix =[I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n    Coriolis_Matrix = [-L1*L2*m2*thetadot(2)*sin(theta(2)),  -L1*L2*m2*sin(theta(2))*(thetadot(1) + thetadot(2));L1*L2*m2*thetadot(1)*sin(theta(2)), 0] ;\n    \n    gravity_vector = [(g*(m1+m2)*L1*cos(theta(1))) + g*m2*L2*cos(theta(1) + theta(2)); g*m2*L2*cos(theta(1) + theta(2))];\n    \n    B = eye(2); % Case 3\n\n    thetadotdot = (inv(Mass_Matrix)) * (tau - Coriolis_Matrix * thetadot -B*thetadot - gravity_vector);\n\nend\n\nclose(v);\nclose all"
  },
  {
    "objectID": "Robotics/Dynamics.html#robot-dynamics",
    "href": "Robotics/Dynamics.html#robot-dynamics",
    "title": "Robot Dynamics",
    "section": "",
    "text": "Here we will simulate the dynamics of the planar robot shown above. We have the dynamics of this robot mentioned in the code.\nNow we combine our simulation with the dynamics we have for the specified robot. We will need to get the (ğ‘¥, ğ‘¦) position of each link to plot the robot. We are using the given simulation parameters and frame rates.\n\nMake a simulation where \\(\\normalsize \\tau = [0, 0]^{ğ‘‡}\\) and the robot has no friction.\nMake a simulation where \\(\\normalsize \\tau = [0, 0]^{ğ‘‡}\\) and the robot has viscous friction ğµ = ğ¼.\nMake a simulation where \\(\\normalsize \\tau = [20, 5]^{ğ‘‡}\\) and the robot has viscous friction ğµ = ğ¼.\n\nWe can play with the parameters (such as mass, inertia, friction, and ğœ), and see how these parameters affect the simulation. We can see the implementations below for the simulation codes I am responsible for.\n\nEnvironment 1: Make a simulation where \\(\\normalsize \\tau = [0, 0]^{ğ‘‡}\\) and the robot has no friction.\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\nfrom scipy.spatial.transform import Rotation as R\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\n# System parameters\nL1 = 1\nL2 = 1\nm1 = 1\nm2 = 1\nI1 = 0.1\nI2 = 0.1\ng = 9.81\ntau = np.array([0, 0])  # Case 1\n\n# Initial conditions\ntheta = np.array([0.0, 0.0])\nthetadot = np.array([0.0, 0.0])\nthetadotdot = np.array([0.0, 0.0])\n\nomega = np.array([0, 0, 1])\nq1 = np.array([0, 0, 0])\nq2 = np.array([L1, 0, 0])\nq3 = np.array([L1 + L2, 0, 0])\n\nS1 = np.hstack((omega, -np.cross(omega, q1)))\nS2 = np.hstack((omega, -np.cross(omega, q2)))\nS_eq1 = np.column_stack((S1, np.zeros(6)))\nS_eq2 = np.column_stack((S1, S2))\n\nM1 = np.vstack((np.hstack((np.eye(3), q2[:, None])), [0, 0, 0, 1]))\nM2 = np.vstack((np.hstack((np.eye(3), np.array([[L1 + L2, 0, 0]]).T)), [0, 0, 0, 1]))\n\n# Plot setup\nmax_reach = L1 + L2\nfig, ax = plt.subplots()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.grid()\n\n# Animation setup\nframes = 1000\ndeltaT = 0.01\nlines, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\n\n\ndef animate(idx):\n    global theta, thetadot, thetadotdot\n    p0 = np.array([0,0])\n    T1 = fk(M1, S_eq2[:, 0:1], theta[0:1])\n    p1 = T1[0:2, 3]\n    T2 = fk(M2, S_eq2, theta)\n    p2 = T2[0:2, 3]\n    P = np.column_stack((p0, p1, p2))\n\n    lines.set_data(P[0, :], P[1, :])\n\n    thetadot += deltaT * thetadotdot\n    theta += deltaT * thetadot\n\n    # Dynamics and integration\n    Mass_Matrix = np.array([\n        [I1 + I2 + L1**2*m1 + L1**2*m2 + L2**2*m2 + 2*L1*L2*m2*np.cos(theta[1]), m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2],\n        [m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2, m2*L2**2 + I2]\n    ])\n\n    Coriolis_Matrix = np.array([\n        [-L1*L2*m2*thetadot[1]*np.sin(theta[1]), -L1*L2*m2*np.sin(theta[1])*(thetadot[0] + thetadot[1])],\n        [L1*L2*m2*thetadot[0]*np.sin(theta[1]), 0]\n    ])\n\n    gravity_vector = np.array([\n        g*(m1 + m2)*L1*np.cos(theta[0]) + g*m2*L2*np.cos(np.sum(theta)),\n        g*m2*L2*np.cos(np.sum(theta))\n    ])\n\n    B = np.array([[0, 0], [0, 0]])  # Case 1\n\n    thetadotdot = np.linalg.inv(Mass_Matrix) @ (tau - Coriolis_Matrix @ thetadot - B @ thetadot - gravity_vector)\n \n\n    return lines,\n\nani = animation.FuncAnimation(fig, animate, frames=frames, interval=deltaT*1000, blit=False)\n\n# Save as GIF\nani.save('Pendulum_Python_1.gif', writer=PillowWriter(fps=33))\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n\n% create figure\nfigure\naxis([-2, 2, -2, 2])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Pendulum_Case_1.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nL1 = 1;\nL2 = 1;\nm1 = 1;\nm2 = 1;\nI1 = 0.1;\nI2 = 0.1;\ng = 9.81;\ntau = [0;0]; % Case 1\n\n% Initial conditions\ntheta = [0;0]; % joint position\nthetadot = [0;0]; % joint velocity\nthetadotdot = [0;0]; % joint acceleration\n\nmasses = [m1,m2];\nomega = [0;0;1];\n\nInertia_1 = [0 0 0;0 0 0;0 0 I1];\nInertia_2 = [0 0 0;0 0 0;0 0 I2];\nq1 = [0;0;0]; % Position of Joint 1\nq2 = [L1;0;0]; % Position of Joint 2\nq3 = [L1+L2;0;0]; % end effector position\n\nS1 = [omega; -cross(omega,q1)];\nS2 = [omega;-cross(omega,q2)];\nS_eq1 = [S1,[0;0;0;0;0;0]];\nS_eq2 = [S1, S2]; \n\nM1 = [eye(3),q2; 0 0 0 1];\nM2 = [eye(3), [L1+L2;0;0]; 0 0 0 1];\n\ngravity_vector = (zeros(length(theta),1));\nCoriolis_Matrix = (zeros(2,2));\nMass_Matrix = [I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n\nfor idx = 1:1000\n\n    % plot the robot\n    % 1. get the position of each link\n    p0 = [0; 0];\n    T1 = fk(M1,S_eq2(:,1:1),theta(1:1,:));\n    p1 = T1(1:2,4); % position of link 1 (location of joint 2)\n    T2 = fk(M2,S_eq2,theta);\n    p2 = T2(1:2,4); % position of link 2 (the end-effector)\n    P = [p0, p1, p2];\n    % 2. draw the robot and save the frame\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4);\n    drawnow;\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n\n    % integrate to update velocity and position\n    % your code here\n    deltaT = 0.01;\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n\n    Mass_Matrix =[I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n    Coriolis_Matrix = [-L1*L2*m2*thetadot(2)*sin(theta(2)),  -L1*L2*m2*sin(theta(2))*(thetadot(1) + thetadot(2));L1*L2*m2*thetadot(1)*sin(theta(2)), 0] ;\n    \n    gravity_vector = [(g*(m1+m2)*L1*cos(theta(1))) + g*m2*L2*cos(theta(1) + theta(2)); g*m2*L2*cos(theta(1) + theta(2))];\n    \n    B = [[0 0]\n        [0 0]]; % Case 1\n\n    thetadotdot = (inv(Mass_Matrix)) * (tau - Coriolis_Matrix * thetadot -B*thetadot - gravity_vector);\n\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nEnvironment 2: Make a simulation where \\(\\normalsize \\tau = [0, 0]^ğ‘‡\\) and the robot has viscous friction \\(\\normalsize ğµ = ğ¼\\).\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\nfrom scipy.spatial.transform import Rotation as R\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\n# System parameters\nL1 = 1\nL2 = 1\nm1 = 1\nm2 = 1\nI1 = 0.1\nI2 = 0.1\ng = 9.81\ntau = np.array([0, 0])  # Case 2\n\n# Initial conditions\ntheta = np.array([0.0, 0.0])\nthetadot = np.array([0.0, 0.0])\nthetadotdot = np.array([0.0, 0.0])\n\nomega = np.array([0, 0, 1])\nq1 = np.array([0, 0, 0])\nq2 = np.array([L1, 0, 0])\nq3 = np.array([L1 + L2, 0, 0])\n\nS1 = np.hstack((omega, -np.cross(omega, q1)))\nS2 = np.hstack((omega, -np.cross(omega, q2)))\nS_eq1 = np.column_stack((S1, np.zeros(6)))\nS_eq2 = np.column_stack((S1, S2))\n\nM1 = np.vstack((np.hstack((np.eye(3), q2[:, None])), [0, 0, 0, 1]))\nM2 = np.vstack((np.hstack((np.eye(3), np.array([[L1 + L2, 0, 0]]).T)), [0, 0, 0, 1]))\n\n# Plot setup\nmax_reach = L1 + L2\nfig, ax = plt.subplots()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.grid()\n\n# Animation setup\nframes = 1000\ndeltaT = 0.01\nlines, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\n\n\ndef animate(idx):\n    global theta, thetadot, thetadotdot\n    p0 = np.array([0,0])\n    T1 = fk(M1, S_eq2[:, 0:1], theta[0:1])\n    p1 = T1[0:2, 3]\n    T2 = fk(M2, S_eq2, theta)\n    p2 = T2[0:2, 3]\n    P = np.column_stack((p0, p1, p2))\n\n    lines.set_data(P[0, :], P[1, :])\n\n    thetadot += deltaT * thetadotdot\n    theta += deltaT * thetadot\n\n    # Dynamics and integration\n    Mass_Matrix = np.array([\n        [I1 + I2 + L1**2*m1 + L1**2*m2 + L2**2*m2 + 2*L1*L2*m2*np.cos(theta[1]), m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2],\n        [m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2, m2*L2**2 + I2]\n    ])\n\n    Coriolis_Matrix = np.array([\n        [-L1*L2*m2*thetadot[1]*np.sin(theta[1]), -L1*L2*m2*np.sin(theta[1])*(thetadot[0] + thetadot[1])],\n        [L1*L2*m2*thetadot[0]*np.sin(theta[1]), 0]\n    ])\n\n    gravity_vector = np.array([\n        g*(m1 + m2)*L1*np.cos(theta[0]) + g*m2*L2*np.cos(np.sum(theta)),\n        g*m2*L2*np.cos(np.sum(theta))\n    ])\n\n    B = np.array([[1, 0], [0, 1]])  # Case 2\n\n    thetadotdot = np.linalg.inv(Mass_Matrix) @ (tau - Coriolis_Matrix @ thetadot - B @ thetadot - gravity_vector)\n \n\n    return lines,\n\nani = animation.FuncAnimation(fig, animate, frames=frames, interval=deltaT*1000, blit=False)\n\n# Save as GIF\nani.save('Pendulum_Python_2.gif', writer=PillowWriter(fps=33))\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n% create figure\nfigure\naxis([-2, 2, -2, 2])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Pendulum_Case_2.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nL1 = 1;\nL2 = 1;\nm1 = 1;\nm2 = 1;\nI1 = 0.1;\nI2 = 0.1;\ng = 9.81;\ntau = [0;0]; % Case 2\n\n% Initial conditions\ntheta = [0;0]; % joint position\nthetadot = [0;0]; % joint velocity\nthetadotdot = [0;0]; % joint acceleration\n\nmasses = [m1,m2];\nomega = [0;0;1];\n\nInertia_1 = [0 0 0;0 0 0;0 0 I1];\nInertia_2 = [0 0 0;0 0 0;0 0 I2];\nq1 = [0;0;0]; % Position of Joint 1\nq2 = [L1;0;0]; % Position of Joint 2\nq3 = [L1+L2;0;0]; % end effector position\n\nS1 = [omega; -cross(omega,q1)];\nS2 = [omega;-cross(omega,q2)];\nS_eq1 = [S1,[0;0;0;0;0;0]];\nS_eq2 = [S1, S2]; \n\nM1 = [eye(3),q2; 0 0 0 1];\nM2 = [eye(3), [L1+L2;0;0]; 0 0 0 1];\n\ngravity_vector = (zeros(length(theta),1));\nCoriolis_Matrix = (zeros(2,2));\nMass_Matrix = [I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n\nfor idx = 1:1000\n\n    % plot the robot\n    % 1. get the position of each link\n    p0 = [0; 0];\n    T1 = fk(M1,S_eq2(:,1:1),theta(1:1,:));\n    p1 = T1(1:2,4); % position of link 1 (location of joint 2)\n    T2 = fk(M2,S_eq2,theta);\n    p2 = T2(1:2,4); % position of link 2 (the end-effector)\n    P = [p0, p1, p2];\n    % 2. draw the robot and save the frame\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4);\n    drawnow;\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n\n    % integrate to update velocity and position\n    % your code here\n    deltaT = 0.01;\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n\n    Mass_Matrix =[I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n    Coriolis_Matrix = [-L1*L2*m2*thetadot(2)*sin(theta(2)),  -L1*L2*m2*sin(theta(2))*(thetadot(1) + thetadot(2));L1*L2*m2*thetadot(1)*sin(theta(2)), 0] ;\n    \n    gravity_vector = [(g*(m1+m2)*L1*cos(theta(1))) + g*m2*L2*cos(theta(1) + theta(2)); g*m2*L2*cos(theta(1) + theta(2))];\n    \n    B = eye(2); % Case 2\n\n    thetadotdot = (inv(Mass_Matrix)) * (tau - Coriolis_Matrix * thetadot -B*thetadot - gravity_vector);\n\nend\n\nclose(v);\nclose all\n\n\n\n\n\n\nEnvironment 3: Make a simulation where \\(\\normalsize \\tau = [20, 5]^ğ‘‡\\) and the robot has viscous friction \\(\\normalsize ğµ = ğ¼\\).\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nfrom matplotlib.animation import PillowWriter\nfrom scipy.spatial.transform import Rotation as R\n\ndef expm(A):\n    return np.linalg.matrix_power(np.eye(A.shape[0]) + A / 16, 16)\n\ndef bracket_s(s):\n    return np.array([[0, -s[2], s[1], s[3]], [s[2], 0, -s[0], s[4]], [-s[1], s[0], 0, s[5]], [0, 0, 0, 0]])\n\ndef fk(M, S, theta):\n    T = np.eye(4)\n    for i in range(len(theta)):\n        T = np.dot(T, expm(bracket_s(S[:, i]) * theta[i]))\n    return np.dot(T, M)\n\n# System parameters\nL1 = 1\nL2 = 1\nm1 = 1\nm2 = 1\nI1 = 0.1\nI2 = 0.1\ng = 9.81\ntau = np.array([20, 5])  # Case 3\n\n# Initial conditions\ntheta = np.array([0.0, 0.0])\nthetadot = np.array([0.0, 0.0])\nthetadotdot = np.array([0.0, 0.0])\n\nomega = np.array([0, 0, 1])\nq1 = np.array([0, 0, 0])\nq2 = np.array([L1, 0, 0])\nq3 = np.array([L1 + L2, 0, 0])\n\nS1 = np.hstack((omega, -np.cross(omega, q1)))\nS2 = np.hstack((omega, -np.cross(omega, q2)))\nS_eq1 = np.column_stack((S1, np.zeros(6)))\nS_eq2 = np.column_stack((S1, S2))\n\nM1 = np.vstack((np.hstack((np.eye(3), q2[:, None])), [0, 0, 0, 1]))\nM2 = np.vstack((np.hstack((np.eye(3), np.array([[L1 + L2, 0, 0]]).T)), [0, 0, 0, 1]))\n\n# Plot setup\nmax_reach = L1 + L2\nfig, ax = plt.subplots()\nax.set_xlim(-3, 3)\nax.set_ylim(-3, 3)\nax.grid()\n\n# Animation setup\nframes = 1000\ndeltaT = 0.01\nlines, = ax.plot([], [], 'o-', color=[1, 0.5, 0], linewidth=4)\n\n\ndef animate(idx):\n    global theta, thetadot, thetadotdot\n    p0 = np.array([0,0])\n    T1 = fk(M1, S_eq2[:, 0:1], theta[0:1])\n    p1 = T1[0:2, 3]\n    T2 = fk(M2, S_eq2, theta)\n    p2 = T2[0:2, 3]\n    P = np.column_stack((p0, p1, p2))\n\n    lines.set_data(P[0, :], P[1, :])\n\n    thetadot += deltaT * thetadotdot\n    theta += deltaT * thetadot\n\n    # Dynamics and integration\n    Mass_Matrix = np.array([\n        [I1 + I2 + L1**2*m1 + L1**2*m2 + L2**2*m2 + 2*L1*L2*m2*np.cos(theta[1]), m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2],\n        [m2*L2**2 + L1*m2*np.cos(theta[1])*L2 + I2, m2*L2**2 + I2]\n    ])\n\n    Coriolis_Matrix = np.array([\n        [-L1*L2*m2*thetadot[1]*np.sin(theta[1]), -L1*L2*m2*np.sin(theta[1])*(thetadot[0] + thetadot[1])],\n        [L1*L2*m2*thetadot[0]*np.sin(theta[1]), 0]\n    ])\n\n    gravity_vector = np.array([\n        g*(m1 + m2)*L1*np.cos(theta[0]) + g*m2*L2*np.cos(np.sum(theta)),\n        g*m2*L2*np.cos(np.sum(theta))\n    ])\n\n    B = np.array([[1, 0], [0, 1]])  # Case 3\n\n    thetadotdot = np.linalg.inv(Mass_Matrix) @ (tau - Coriolis_Matrix @ thetadot - B @ thetadot - gravity_vector)\n \n\n    return lines,\n\nani = animation.FuncAnimation(fig, animate, frames=frames, interval=deltaT*1000, blit=False)\n\n# Save as GIF\nani.save('Pendulum_Python_3.gif', writer=PillowWriter(fps=33))\n\n\n\n\n\n\n\nCode\n\n\nclose all\nclear\nclc\n\n\n% create figure\nfigure\naxis([-2, 2, -2, 2])\ngrid on\nhold on\n\n% save as a video file\nv = VideoWriter('Pendulum_Python_3.mp4', 'MPEG-4');\nv.FrameRate = 100;\nopen(v);\n\n% pick your system parameters\nL1 = 1;\nL2 = 1;\nm1 = 1;\nm2 = 1;\nI1 = 0.1;\nI2 = 0.1;\ng = 9.81;\ntau = [20;5]; % Case 3\n\n% Initial conditions\ntheta = [0;0]; % joint position\nthetadot = [0;0]; % joint velocity\nthetadotdot = [0;0]; % joint acceleration\n\nmasses = [m1,m2];\nomega = [0;0;1];\n\nInertia_1 = [0 0 0;0 0 0;0 0 I1];\nInertia_2 = [0 0 0;0 0 0;0 0 I2];\nq1 = [0;0;0]; % Position of Joint 1\nq2 = [L1;0;0]; % Position of Joint 2\nq3 = [L1+L2;0;0]; % end effector position\n\nS1 = [omega; -cross(omega,q1)];\nS2 = [omega;-cross(omega,q2)];\nS_eq1 = [S1,[0;0;0;0;0;0]];\nS_eq2 = [S1, S2]; \n\nM1 = [eye(3),q2; 0 0 0 1];\nM2 = [eye(3), [L1+L2;0;0]; 0 0 0 1];\n\ngravity_vector = (zeros(length(theta),1));\nCoriolis_Matrix = (zeros(2,2));\nMass_Matrix = [I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n\nfor idx = 1:1000\n\n    % plot the robot\n    % 1. get the position of each link\n    p0 = [0; 0];\n    T1 = fk(M1,S_eq2(:,1:1),theta(1:1,:));\n    p1 = T1(1:2,4); % position of link 1 (location of joint 2)\n    T2 = fk(M2,S_eq2,theta);\n    p2 = T2(1:2,4); % position of link 2 (the end-effector)\n    P = [p0, p1, p2];\n    % 2. draw the robot and save the frame\n    cla;\n    plot(P(1,:), P(2,:), 'o-', 'color',[1, 0.5, 0],'linewidth',4);\n    drawnow;\n    frame = getframe(gcf);\n    writeVideo(v,frame);\n\n    % integrate to update velocity and position\n    % your code here\n    deltaT = 0.01;\n    thetadot = thetadot + deltaT * thetadotdot;\n    theta = theta + deltaT * thetadot;\n\n    Mass_Matrix =[I1 + I2 + L1^2*m1 + L1^2*m2 + L2^2*m2 + 2*L1*L2*m2*cos(theta(2)), m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2; m2*L2^2 + L1*m2*cos(theta(2))*L2 + I2, m2*L2^2 + I2];\n    \n    Coriolis_Matrix = [-L1*L2*m2*thetadot(2)*sin(theta(2)),  -L1*L2*m2*sin(theta(2))*(thetadot(1) + thetadot(2));L1*L2*m2*thetadot(1)*sin(theta(2)), 0] ;\n    \n    gravity_vector = [(g*(m1+m2)*L1*cos(theta(1))) + g*m2*L2*cos(theta(1) + theta(2)); g*m2*L2*cos(theta(1) + theta(2))];\n    \n    B = eye(2); % Case 3\n\n    thetadotdot = (inv(Mass_Matrix)) * (tau - Coriolis_Matrix * thetadot -B*thetadot - gravity_vector);\n\nend\n\nclose(v);\nclose all"
  },
  {
    "objectID": "Robotics/Materials/Materials/Module_1-Basics/Homework/My HW1_Problem_4.html",
    "href": "Robotics/Materials/Materials/Module_1-Basics/Homework/My HW1_Problem_4.html",
    "title": "Portfolio",
    "section": "",
    "text": "import numpy as np\n\n# Define the angles of rotation\ntheta_z1 = np.pi / 4  # Ï€/4 radians\ntheta_y = -np.pi / 3  # -Ï€/3 radians\ntheta_z2 = np.pi / 2  # Ï€/2 radians\n\n# Create the rotation matrices\nR1 = np.array([[np.cos(theta_z1), -np.sin(theta_z1), 0],\n               [np.sin(theta_z1), np.cos(theta_z1), 0],\n               [0, 0, 1]])\n\nR2 = np.array([[np.cos(theta_y), 0, np.sin(theta_y)],\n               [0, 1, 0],\n               [-np.sin(theta_y), 0, np.cos(theta_y)]])\n\nR3 = np.array([[np.cos(theta_z2), -np.sin(theta_z2), 0],\n               [np.sin(theta_z2), np.cos(theta_z2), 0],\n               [0, 0, 1]])\n\n# Multiply the rotation matrices\nR = np.dot(R1, np.dot(R2, R3))\n\n# Multiply the transpose of R to R itself to prove R is a rotational matrix as the result is an identity matrix\nresult_prove_rotational = R.T @ R\ndeterminant = np.linalg.det(R)\nR_inverse = np.linalg.inv(R)\n\nprint(\"Below is the rotational matrix R: \")\nprint(\"\\n\", R)\nprint(\"\\nDeterminant of rotational matrix R:\")\nprint(\"\\n\", determinant)\nprint(\"\\nR is a rotational matrix as result is an identity matrix after multiplying R with R transpose: \")\nprint(\"\\n\", result_prove_rotational)\nprint(\"\\n R.T is the transpose of R to equate it with R inverse below: \")\nprint(\"\\n\", R.T)\nprint(\"\\n R_inverse is the inverse of R to equate it with R transpose above: \")\nprint(\"\\n\", R_inverse)\n\nBelow is the rotational matrix R: \n\n [[-7.07106781e-01 -3.53553391e-01 -6.12372436e-01]\n [ 7.07106781e-01 -3.53553391e-01 -6.12372436e-01]\n [ 5.30287619e-17 -8.66025404e-01  5.00000000e-01]]\n\nDeterminant of rotational matrix R:\n\n 1.0\n\nR is a rotational matrix as result is an identity matrix after multiplying R with R transpose: \n\n [[1.00000000e+00 1.46601644e-17 1.01628368e-17]\n [1.46601644e-17 1.00000000e+00 1.48741681e-17]\n [1.01628368e-17 1.48741681e-17 1.00000000e+00]]\n\n R.T is the transpose of R to equate it with R inverse below: \n\n [[-7.07106781e-01  7.07106781e-01  5.30287619e-17]\n [-3.53553391e-01 -3.53553391e-01 -8.66025404e-01]\n [-6.12372436e-01 -6.12372436e-01  5.00000000e-01]]\n\n R_inverse is the inverse of R to equate it with R transpose above: \n\n [[-7.07106781e-01  7.07106781e-01 -7.85046229e-17]\n [-3.53553391e-01 -3.53553391e-01 -8.66025404e-01]\n [-6.12372436e-01 -6.12372436e-01  5.00000000e-01]]\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "Robotics/RRT.html",
    "href": "Robotics/RRT.html",
    "title": "RRT Algorithm Simulation",
    "section": "",
    "text": "The path plan should actually work on a robot. If the path plan makes the robot turn at sharp angles but the robot canâ€™t move at sharp angles (like a car), that path plan shouldnâ€™t be allowed.\nThe path plan should be as close to optimal as possible. While itâ€™s nice to find any path plan that gets the robot from a start location to a goal location, that isnâ€™t enough unfortunately. Weâ€™d like something thatâ€™s somewhat efficient. Not only will it help the robot complete its task as fast as possible, but itâ€™ll also conserve its precious battery life.\nThe path plan should avoid colliding with walls. This obviously goes without saying. Robots can be pretty expensive, and crashing is never a good thing. My little robot alone cost me well over a thousand bucks.\n\nOne of the most popular algorithms for coming up with a path plan that tries to satisfies these conditions is called Rapidly-exploring Random Trees (RRT). Since a picture is worth a thousand words, check out the diagram below. Letâ€™s suppose the robot has to go from a start location (the red dot) to a goal location (the green dot) in a simple map without any obstacles. Basically, weâ€™ll start off with a tree that has a root node representing the start position of the robot. After that, weâ€™ll build the tree up gradually. How? Weâ€™ll take a bunch of random samples of the map, make a new node for each random sample, and insert each new node into the tree somehow. Once the tree has a node thatâ€™s close enough to the goal position of the robot, weâ€™re done.\nHere we are using the RRT algorithm to perform motion planning in 2-DoF environments. As before, the mobile robotâ€™s position is \\(\\normalsize \\theta = [ğ‘¥, ğ‘¦]^ğ‘‡\\).\nImplementing the RRT algorithm below. This code should be able to work with an arbitrary number of circular obstacles with 2 conditions:\nâ€¢ The bounds of the workspace are \\(\\normalsize ğ‘¥ âˆˆ [0, 1], \\, ğ‘¦ âˆˆ [0, 1]\\) â€¢ The motion plan must end within \\(\\normalsize \\epsilon = 0.1\\) units of the goal\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Define start and goal positions\ntheta_start = {'coord': np.array([0, 0]), 'parent': None}\ntheta_goal = np.array([1, 1])\n\n# Workspace bounds\nx_bounds = [0, 1]\ny_bounds = [0, 1]\n\n# Define obstacles\nobstacles = np.array([\n    [0.5, 0.3, 0.2],\n    [0.7, 0.7, 0.2],\n    [0.6, 0.4, 0.1],\n    [0.4, 0.3, 0.2],\n    [0.1, 0.75, 0.3]\n])\n\n# RRT parameters\nepsilon = 0.1  # Goal threshold\ndelta = 0.05   # Step size\nN = 1000       # Number of iterations\n\n# Initialize tree\nG = [theta_start]\n\n# Visualize environment setup\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(x_bounds)\nax.set_ylim(y_bounds)\nax.set_aspect('equal')\n\n# Plot obstacles\nfor obstacle in obstacles:\n    circle = Circle(obstacle[0:2], obstacle[2], color='gray', fill=False)\n    ax.add_patch(circle)\n\n# Plot start and goal\nstart_plot, = ax.plot(theta_start['coord'][0], theta_start['coord'][1], 'ko', markerfacecolor='k')\ngoal_plot, = ax.plot(theta_goal[0], theta_goal[1], 'ko', markerfacecolor='k')\n\n# This flag is used to stop the animation once the goal is reached\ngoal_reached = False\n\n# Animation update function\ndef update(frame):\n    global G, goal_reached\n\n    if goal_reached:\n        return\n\n    theta_rand = theta_goal if np.random.rand() &lt; 0.2 else np.random.rand(2)\n    distances = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(distances)\n    theta_near = G[theta_near_index]\n    vec_to_rand = theta_rand - theta_near['coord']\n    vec_to_rand = delta * vec_to_rand / np.linalg.norm(vec_to_rand)\n    theta_new = {'coord': theta_near['coord'] + vec_to_rand, 'parent': theta_near_index}\n\n    if not is_collision(theta_new['coord'], obstacles):\n        G.append(theta_new)\n        ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n        ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color='gray', markerfacecolor='gray')\n\n        if np.linalg.norm(theta_new['coord'] - theta_goal) &lt; epsilon:\n            # Draw the final path in orange\n            current = theta_new\n            while current['parent'] is not None:\n                parent = G[current['parent']]\n                ax.plot([current['coord'][0], parent['coord'][0]], [current['coord'][1], parent['coord'][1]], color='orange', linewidth=3)\n                current = parent\n            goal_reached = True\n\n# Collision checking function\ndef is_collision(coord, obstacles):\n    return any(np.linalg.norm(coord - obstacle[0:2]) &lt; obstacle[2] for obstacle in obstacles)\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation as a GIF\ngif_path = \"RRT_main.gif\"\nwriter = PillowWriter(fps=20)\nani.save(gif_path, writer=writer)\n\n# Close the plot\nplt.close()\n\n\n\n\n\n\n\nCode\n\nclear\nclose all\n\n% Define start and goal positions\ntheta_start.coord = [0; 0];\ntheta_goal = [1; 1];\n\n% Workspace bounds\nx_bounds = [0, 1];\ny_bounds = [0, 1];\n\n% Define obstacles\n% Each row is an obstacle with format: [center_x, center_y, radius]\nobstacles = [\n    0.5, 0.3, 0.2;\n    0.7, 0.7, 0.2;\n    0.6, 0.4, 0.1;\n    0.4, 0.3, 0.2;\n    0.1, 0.75, 0.3;\n    % Add as many obstacles required, satisfying the condition of arbitrary\n    % number of obstacles' inclusion\n];\n\n% RRT parameters\nepsilon = 0.1; % Goal threshold\ndelta = 0.05;  % Step size\nN = 1000;      % Number of iterations\n\n% Visualize environment\nfigure\nhold on\ngrid on\naxis([x_bounds, y_bounds])\naxis equal\n\n% Plot obstacles\nfor i = 1:size(obstacles, 1)\n    viscircles(obstacles(i, 1:2), obstacles(i, 3), 'Color', [0.5, 0.5, 0.5]);\nend\n\n% Initialize tree\ntheta_start.parent = 0;\nG(1) = theta_start;\n\n% Main RRT loop\nfor idx = 1:N\n    if norm(G(end).coord - theta_goal) &lt; epsilon\n        break\n    end\n\n    % Random sample\n    theta_rand = rand(2,1);\n\n    % Nearest node\n    [min_dist, theta_near_index] = min(vecnorm([G.coord] - theta_rand));\n    theta_near = G(theta_near_index);\n\n    % Step towards random sample\n    vec_to_rand = theta_rand - theta_near.coord;\n    if norm(vec_to_rand) &gt; delta\n        vec_to_rand = delta * vec_to_rand / norm(vec_to_rand);\n    end\n    theta_new.coord = theta_near.coord + vec_to_rand;\n\n    % Collision check with all obstacles\n    if isCollision(theta_new.coord, obstacles)\n        continue;\n    end\n\n    % Add new node to tree\n    theta_new.parent = theta_near_index;\n    G = [G, theta_new];\n\n    % Plotting\n    plot(theta_new.coord(1), theta_new.coord(2), 'o', 'Color', [0.5, 0.5, 0.5], ...\n    'MarkerFaceColor', [0.5, 0.5, 0.5]);\n    line([theta_near.coord(1), theta_new.coord(1)], [theta_near.coord(2), theta_new.coord(2)], 'Color', 'k', 'LineWidth', 2);\nend\n\n% Trace back path\nchild_theta = G(end);\nwhile child_theta.parent ~= 0\n    parent_theta = G(child_theta.parent);\n    line([child_theta.coord(1), parent_theta.coord(1)], [child_theta.coord(2), parent_theta.coord(2)], 'Color', [1, 0.5, 0], 'LineWidth', 3);\n    child_theta = parent_theta;\nend\n\n% Plot start and goal\nplot(theta_start.coord(1), theta_start.coord(2), 'ko', 'MarkerFaceColor', 'k');\nplot(theta_goal(1), theta_goal(2), 'ko', 'MarkerFaceColor', 'k');\n\n% Collision checking function\n\nfunction collision = isCollision(coord, obstacles)\n    collision = any(arrayfun(@(idx) norm(coord - obstacles(idx, 1:2)') &lt; obstacles(idx, 3), 1:size(obstacles, 1)));\nend\n\nResult\n\nNotice that centers is a matrix where each column corresponds to an obstacle center. Similarly, radii is a vector where the \\(\\normalsize ğ‘–^{th}\\) entry corresponds to the radius of the \\(\\normalsize ğ‘–^{th}\\) obstacle.\n\nEnvironment 1: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.55, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\).\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.5], [0.5]])\nradii = np.array([0.3])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\n\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    theta_rand = np.random.rand(2)\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"RRT_1_obstacles.gif\", writer=writer)\n\n\nResult:\n\n\nEnvironment 2: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.7]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.5, 0.5], [0.3, 0.7]])\nradii = np.array([0.3, 0.2])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\n\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    theta_rand = np.random.rand(2)\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"RRT_2_obstacles.gif\", writer=writer)\n\n\nResult:\n\n\nEnvironment 3: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.2, 0.35]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.2\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\). A third obstacle with \\(\\normalsize center \\; ğ‘_3 = [0.7, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_3 = 0.2\\)\n\nHere we are declaring it a Baseline Algorithm for 3 obstacles. Later below we will discuss the differences between a baseline and Goal-bias algorithm and their pros-cons.\n\n\n\n\n\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.2, 0.5, 0.7], [0.35, 0.3, 0.5]])\nradii = np.array([0.2, 0.2, 0.2])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\n\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    theta_rand = np.random.rand(2)\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"Baseline_RRT.gif\", writer=writer)\n\n\n\n\n\n\n\nCode\n\nclear\nclose all\n\n% environment\ntheta_start.coord = [0; 0];\ntheta_goal.coord = [1; 1];\ncenters = [0.2, 0.5, 0.7; 0.35, 0.3, 0.5];\nradii = [0.2, 0.2, 0.2];\n\n% parameters\nepsilon = 0.1;\ndelta = 0.1;\nN = 1000;\n\n% visualize environment\nfigure\ngrid on\nhold on\naxis([0, 1, 0, 1])\naxis equal\n\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\n\nplot(0, 0, 'ko', 'MarkerFaceColor', 'k')\nplot(1, 1, 'ko', 'MarkerFaceColor', 'k')\n\n% initialize tree\ntheta_start.parent = 0;\nG(1) = theta_start;\n\nfor idx = 1:N\n % stop if theta_new is close to theta_goal\n if norm(G(end).coord - theta_goal.coord) &lt; epsilon\n break\n end\n\n % sample random joint position\n theta_rand = rand(2,1);\n\n % find node in G nearest to theta_rand\n dist = zeros(length(G), 1);\n for jdx = 1:1:length(G)\n dist(jdx) = norm(G(jdx).coord - theta_rand);\n end\n [~, theta_near_index] = min(dist);\n theta_near = G(theta_near_index);\n\n % take a step from theta_near towards theta_rand\n vec_to_rand = theta_rand - theta_near.coord;\n dist_to_rand = norm(vec_to_rand);\n\n if dist_to_rand &lt; delta\n theta_new.coord = theta_rand;\n else\n theta_new.coord = theta_near.coord + delta * ...\n vec_to_rand/dist_to_rand;\n end\n\n % check if theta_new is collision free\n collision = false;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_new.coord - center) &lt; radius\n collision = true;\n end\n end\n\n if collision\n continue\n end\n\n % if collision free, add theta_new to tree with parent theta_near\n theta_new.parent = theta_near_index;\n G = [G, theta_new];\n\n % plot node and edge\n plot(theta_new.coord(1), theta_new.coord(2), 'o', 'Color', [0.5, ...\n 0.5, 0.5], 'MarkerFaceColor', [0.5, 0.5, 0.5])\n line([theta_near.coord(1), theta_new.coord(1)], ...\n [theta_near.coord(2), theta_new.coord(2)], 'Color', 'k', 'LineWidth', ...\n 2);\nend\n\n% work backwards from the final node to the root of the tree\nnext_theta = G(end);\nwhile next_theta.parent ~= 0\n prev_theta_idx = next_theta.parent;\n prev_theta = G(prev_theta_idx);\n line([next_theta.coord(1), prev_theta.coord(1)], ...\n [next_theta.coord(2), prev_theta.coord(2)], 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n next_theta = prev_theta;\nend\n\nResult:\n\nWe can see the trajectory plots in the above figures. Here the black lines and gray dots show the tree ğº, while the orange line is the final motion plan from ğœƒğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ to a point close to the goal \\(\\normalsize (\\epsilon â‰¤ 0.1)\\). Each time we run your RRT code you should get a different solution: RRT builds the tree through random sampling.\n\n\n\n\n\nThis version will sample the goal more frequently (letâ€™s refer to this as goal bias). For goal bias, with probability \\(\\normalsize 0.2\\) set \\(\\normalsize \\theta_{rand}\\) as \\(\\normalsize \\theta_{goal}\\). Otherwise sample randomly as normal. Then we run our code 10 times for baseline and 10 times for goal bias.\nThus we write down how many samples it takes on average to find a motion plan. Which approach is more sample-efficient: baseline or goal bias?\nLet us check the implementation and results.\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.2, 0.5, 0.7], [0.35, 0.3, 0.5]])\nradii = np.array([0.2, 0.2, 0.2])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    if np.random.rand() &lt; 0.2:\n        theta_rand = theta_goal['coord']\n    else:\n        theta_rand = np.random.rand(2)\n\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"Goal_bias_RRT.gif\", writer=writer)\n\n\n\n\n\n\n\nCode\n\nclear\nclose all\n% Environment initiation\ntheta_start.coord = [0; 0];\ntheta_goal = [1; 1];\n\n% Obstacle Parameters\n% First obstacle center and radius\ncenter = [0.2; 0.35];\nradius = 0.2;\n% Second obstacle center and radius\ncenter2 = [0.5; 0.3]; \nradius2 = 0.2;\n% Third obstacle center and radius\ncenter3 = [0.7; 0.5];\nradius3 = 0.2;\n\n% Specifying parameters\nepsilon = 0.1;\ndelta = 0.1;\nN = 1000;\n\n% Visualizing the environment\nfigure\ngrid on\nhold on\naxis([0, 1, 0, 1])\naxis equal\nviscircles(center', radius, 'Color', [0.5, 0.5, 0.5]);\nviscircles(center2', radius2, 'Color', [0.7, 0.3, 0.3]); % Visualizing the second obstacle\nviscircles(center3', radius3, 'Color', [0.3, 0.5, 0.7]); % Visualizing the third obstacle\nplot(0, 0, 'ko', 'MarkerFaceColor', 'k')\nplot(1, 1, 'ko', 'MarkerFaceColor', 'k')\n\n% Initializing the tree\ntheta_start.parent = 0;\nG(1) = theta_start;\n\nfor idx = 1:N\n    \n    % Stop if the last node in G is close to theta_goal\n    if norm(G(end).coord  - theta_goal) &lt; epsilon\n        break\n    end\n\n    % sample random joint position: probability 0.2\n    \n    if rand() &lt; 0.2\n        theta_rand = theta_goal;\n    else\n        theta_rand = rand(2,1);\n    end\n    \n    % find node in G nearest to theta_rand\n    min_dist = inf;\n    theta_near_index = 0;\n\n    for jdx = 1:length(G)\n        coord = G(jdx).coord;\n        dist = norm(theta_rand - coord);\n        if dist &lt; min_dist\n            min_dist = dist;\n            theta_near_index = jdx;\n        end\n    end\n\n    theta_near = G(theta_near_index);\n    % take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near.coord;\n    dist_to_rand = norm(vec_to_rand);\n    if dist_to_rand &lt; delta\n        theta_new.coord = theta_rand;\n    else\n        theta_new.coord = theta_near.coord + delta * ...\n        vec_to_rand/dist_to_rand;\n    end\n    \n    % check if theta_new is collision free with all obstacles\n    dist_to_obs1 = norm(theta_new.coord - center);\n    dist_to_obs2 = norm(theta_new.coord - center2);\n    dist_to_obs3 = norm(theta_new.coord - center3);\n    if dist_to_obs1 &lt; radius || dist_to_obs2 &lt; radius2 || dist_to_obs3 &lt; radius3\n        continue\n    end\n\n    % if collision free, add theta_new to tree with parent theta_near\n    theta_new.parent = theta_near_index;\n    G = [G, theta_new];\n\n    % plot node and edge\n    plot(theta_new.coord(1), theta_new.coord(2), 'o', 'Color', [0.5, 0.5, 0.5], ...\n    'MarkerFaceColor', [0.5, 0.5, 0.5])\n    line([theta_near.coord(1), theta_new.coord(1)], [theta_near.coord(2), ...\n    theta_new.coord(2)], 'Color', 'k', 'LineWidth', 2);\n    drawnow\n\nend\n\n% work backwards from the final node to the root of the tree\nchild_theta = G(end);\nwhile child_theta.parent ~= 0\n\n    parent_theta_index = child_theta.parent;\n    parent_theta = G(parent_theta_index);\n    line([child_theta.coord(1), parent_theta.coord(1)], ...\n        [child_theta.coord(2), parent_theta.coord(2)], ...\n        'Color', [1, 0.5, 0], 'LineWidth', 3);\n    child_theta = parent_theta;\nend\n\nResult:\n\nThe samples ğ‘ across ten runs with baseline and goal bias are tabulated below.\n\n\n\nrun\nbaseline\ngoal bias\n\n\n\n\n1\n254\n225\n\n\n2\n363\n169\n\n\n3\n318\n150\n\n\n4\n590\n143\n\n\n5\n352\n249\n\n\n6\n359\n166\n\n\n7\n202\n251\n\n\n8\n267\n162\n\n\n9\n440\n183\n\n\n10\n226\n313\n\n\n\nOn average, the baseline required 337 samples to reach a valid motion plan, while goal bias needed 201 samples. For Environment 3 the evidence suggests that biasing the samples towards ğœƒğ‘”ğ‘œğ‘ğ‘™ decreases the total number of samples and causes RRT to reach a solution faster. Intuitively, this is because the tree is trying to move in the direction of the goal more frequently.\nThis is particularly advantageous when the robot is in free space (or has passed the obstacles) and should move directly towards the goal. The code snippet required to implement goal bias is shown below:\n\n\n\n\n\n\n# Sample random joint position: probability 0.2\nif np.random.rand() &lt; 0.2:\n    theta_rand = theta_goal['coord']\nelse:\n    theta_rand = np.random.rand(2)\n\n\n\n\n\nCode\n\n% sample random joint position: probability 0.2\n    \nif rand() &lt; 0.2\n    theta_rand = theta_goal;\nelse\n    theta_rand = rand(2,1);\nend"
  },
  {
    "objectID": "Robotics/RRT.html#rrt-algorithm-for-1-obstacle",
    "href": "Robotics/RRT.html#rrt-algorithm-for-1-obstacle",
    "title": "RRT Algorithm Simulation",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.5], [0.5]])\nradii = np.array([0.3])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\n\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    theta_rand = np.random.rand(2)\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"RRT_1_obstacles.gif\", writer=writer)\n\n\nResult:\n\n\nEnvironment 2: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.3\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.7]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\)"
  },
  {
    "objectID": "Robotics/RRT.html#rrt-algorithm-for-2-obstacles",
    "href": "Robotics/RRT.html#rrt-algorithm-for-2-obstacles",
    "title": "RRT Algorithm Simulation",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.5, 0.5], [0.3, 0.7]])\nradii = np.array([0.3, 0.2])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\n\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    theta_rand = np.random.rand(2)\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"RRT_2_obstacles.gif\", writer=writer)\n\n\nResult:\n\n\nEnvironment 3: One obstacle with \\(\\normalsize center \\; ğ‘_1 = [0.2, 0.35]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_1 = 0.2\\). A second obstacle with \\(\\normalsize center \\; ğ‘_2 = [0.5, 0.3]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_2 = 0.2\\). A third obstacle with \\(\\normalsize center \\; ğ‘_3 = [0.7, 0.5]^ğ‘‡ \\; and \\; radius \\; ğ‘Ÿ_3 = 0.2\\)\n\nHere we are declaring it a Baseline Algorithm for 3 obstacles. Later below we will discuss the differences between a baseline and Goal-bias algorithm and their pros-cons."
  },
  {
    "objectID": "Robotics/RRT.html#baseline-rrt-algorithm",
    "href": "Robotics/RRT.html#baseline-rrt-algorithm",
    "title": "RRT Algorithm Simulation",
    "section": "",
    "text": "Code\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.2, 0.5, 0.7], [0.35, 0.3, 0.5]])\nradii = np.array([0.2, 0.2, 0.2])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\n\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    theta_rand = np.random.rand(2)\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"Baseline_RRT.gif\", writer=writer)\n\n\n\n\n\n\n\nCode\n\nclear\nclose all\n\n% environment\ntheta_start.coord = [0; 0];\ntheta_goal.coord = [1; 1];\ncenters = [0.2, 0.5, 0.7; 0.35, 0.3, 0.5];\nradii = [0.2, 0.2, 0.2];\n\n% parameters\nepsilon = 0.1;\ndelta = 0.1;\nN = 1000;\n\n% visualize environment\nfigure\ngrid on\nhold on\naxis([0, 1, 0, 1])\naxis equal\n\nfor idx = 1:length(radii)\n viscircles(centers(:, idx)', radii(idx), 'Color', [0.5, 0.5, ...\n 0.5]);\nend\n\nplot(0, 0, 'ko', 'MarkerFaceColor', 'k')\nplot(1, 1, 'ko', 'MarkerFaceColor', 'k')\n\n% initialize tree\ntheta_start.parent = 0;\nG(1) = theta_start;\n\nfor idx = 1:N\n % stop if theta_new is close to theta_goal\n if norm(G(end).coord - theta_goal.coord) &lt; epsilon\n break\n end\n\n % sample random joint position\n theta_rand = rand(2,1);\n\n % find node in G nearest to theta_rand\n dist = zeros(length(G), 1);\n for jdx = 1:1:length(G)\n dist(jdx) = norm(G(jdx).coord - theta_rand);\n end\n [~, theta_near_index] = min(dist);\n theta_near = G(theta_near_index);\n\n % take a step from theta_near towards theta_rand\n vec_to_rand = theta_rand - theta_near.coord;\n dist_to_rand = norm(vec_to_rand);\n\n if dist_to_rand &lt; delta\n theta_new.coord = theta_rand;\n else\n theta_new.coord = theta_near.coord + delta * ...\n vec_to_rand/dist_to_rand;\n end\n\n % check if theta_new is collision free\n collision = false;\n for jdx = 1:length(radii)\n center = centers(:, jdx);\n radius = radii(jdx);\n if norm(theta_new.coord - center) &lt; radius\n collision = true;\n end\n end\n\n if collision\n continue\n end\n\n % if collision free, add theta_new to tree with parent theta_near\n theta_new.parent = theta_near_index;\n G = [G, theta_new];\n\n % plot node and edge\n plot(theta_new.coord(1), theta_new.coord(2), 'o', 'Color', [0.5, ...\n 0.5, 0.5], 'MarkerFaceColor', [0.5, 0.5, 0.5])\n line([theta_near.coord(1), theta_new.coord(1)], ...\n [theta_near.coord(2), theta_new.coord(2)], 'Color', 'k', 'LineWidth', ...\n 2);\nend\n\n% work backwards from the final node to the root of the tree\nnext_theta = G(end);\nwhile next_theta.parent ~= 0\n prev_theta_idx = next_theta.parent;\n prev_theta = G(prev_theta_idx);\n line([next_theta.coord(1), prev_theta.coord(1)], ...\n [next_theta.coord(2), prev_theta.coord(2)], 'Color', [1, 0.5, ...\n 0], 'LineWidth', 3);\n next_theta = prev_theta;\nend\n\nResult:\n\nWe can see the trajectory plots in the above figures. Here the black lines and gray dots show the tree ğº, while the orange line is the final motion plan from ğœƒğ‘ ğ‘¡ğ‘ğ‘Ÿğ‘¡ to a point close to the goal \\(\\normalsize (\\epsilon â‰¤ 0.1)\\). Each time we run your RRT code you should get a different solution: RRT builds the tree through random sampling."
  },
  {
    "objectID": "Robotics/RRT.html#goal-bias-rrt-algorithm",
    "href": "Robotics/RRT.html#goal-bias-rrt-algorithm",
    "title": "RRT Algorithm Simulation",
    "section": "",
    "text": "This version will sample the goal more frequently (letâ€™s refer to this as goal bias). For goal bias, with probability \\(\\normalsize 0.2\\) set \\(\\normalsize \\theta_{rand}\\) as \\(\\normalsize \\theta_{goal}\\). Otherwise sample randomly as normal. Then we run our code 10 times for baseline and 10 times for goal bias.\nThus we write down how many samples it takes on average to find a motion plan. Which approach is more sample-efficient: baseline or goal bias?\nLet us check the implementation and results.\n\n\n\n\n\n\nCode\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')  # Set the backend to a non-interactive backend\nimport matplotlib.pyplot as plt\nfrom matplotlib.patches import Circle\nfrom matplotlib.animation import PillowWriter, FuncAnimation\n\n# Environment\ntheta_start = {'coord': np.array([0, 0])}\ntheta_goal = {'coord': np.array([1, 1])}\ncenters = np.array([[0.2, 0.5, 0.7], [0.35, 0.3, 0.5]])\nradii = np.array([0.2, 0.2, 0.2])\n\n# Parameters\nepsilon = 0.1\ndelta = 0.1\nN = 1000\n\n# Initialize figure\nfig, ax = plt.subplots()\nax.grid(True)\nax.set_xlim(0, 1)\nax.set_ylim(0, 1)\nax.set_aspect('equal', adjustable='box')\n\n# Draw obstacles\nfor idx in range(len(radii)):\n    circle = Circle(centers[:, idx], radii[idx], color=[0.5, 0.5, 0.5], alpha=0.7)\n    ax.add_patch(circle)\n\n# Plot start and goal\nax.plot(0, 0, 'ko', markerfacecolor='k')\nax.plot(1, 1, 'ko', markerfacecolor='k')\n\n# Initialize tree\ntheta_start['parent'] = None\nG = [theta_start]\nfinal_path_plotted = False\n\ndef update(frame):\n    global G, final_path_plotted\n    if frame == 0 or final_path_plotted:\n        return\n\n    # Sample random joint position\n    if np.random.rand() &lt; 0.2:\n        theta_rand = theta_goal['coord']\n    else:\n        theta_rand = np.random.rand(2)\n\n\n    # Find node in G nearest to theta_rand\n    dist = [np.linalg.norm(node['coord'] - theta_rand) for node in G]\n    theta_near_index = np.argmin(dist)\n    theta_near = G[theta_near_index]\n\n    # Take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near['coord']\n    dist_to_rand = np.linalg.norm(vec_to_rand)\n\n    theta_new = {}\n    if dist_to_rand &lt; delta:\n        theta_new['coord'] = theta_rand\n    else:\n        theta_new['coord'] = theta_near['coord'] + delta * vec_to_rand / dist_to_rand\n\n    # Check if theta_new is collision-free\n    collision = False\n    for jdx in range(len(radii)):\n        center = centers[:, jdx]\n        radius = radii[jdx]\n        if np.linalg.norm(theta_new['coord'] - center) &lt; radius:\n            collision = True\n            break\n\n    if collision:\n        return\n\n    # If collision-free, add theta_new to tree with parent theta_near\n    theta_new['parent'] = theta_near_index\n    G.append(theta_new)\n\n    # Plot node and edge\n    ax.plot(theta_new['coord'][0], theta_new['coord'][1], 'o', color=[0.5, 0.5, 0.5], markerfacecolor=[0.5, 0.5, 0.5])\n    ax.plot([theta_near['coord'][0], theta_new['coord'][0]], [theta_near['coord'][1], theta_new['coord'][1]], 'k-', linewidth=2)\n\n    # If goal is close enough to the last node in G, plot the final path\n    if np.linalg.norm(G[-1]['coord'] - theta_goal['coord']) &lt; epsilon:\n        next_theta = G[-1]\n        while next_theta['parent'] is not None:\n            prev_theta = G[next_theta['parent']]\n            line, = ax.plot([next_theta['coord'][0], prev_theta['coord'][0]], [next_theta['coord'][1], prev_theta['coord'][1]], 'orange', linewidth=3)\n            next_theta = prev_theta\n\n        # Add final path to the animation\n        final_path_plotted = True\n        return line,\n\n\n# Create the animation\nani = FuncAnimation(fig, update, frames=N, repeat=False)\n\n# Save the animation\nwriter = PillowWriter(fps=20)\nani.save(\"Goal_bias_RRT.gif\", writer=writer)\n\n\n\n\n\n\n\nCode\n\nclear\nclose all\n% Environment initiation\ntheta_start.coord = [0; 0];\ntheta_goal = [1; 1];\n\n% Obstacle Parameters\n% First obstacle center and radius\ncenter = [0.2; 0.35];\nradius = 0.2;\n% Second obstacle center and radius\ncenter2 = [0.5; 0.3]; \nradius2 = 0.2;\n% Third obstacle center and radius\ncenter3 = [0.7; 0.5];\nradius3 = 0.2;\n\n% Specifying parameters\nepsilon = 0.1;\ndelta = 0.1;\nN = 1000;\n\n% Visualizing the environment\nfigure\ngrid on\nhold on\naxis([0, 1, 0, 1])\naxis equal\nviscircles(center', radius, 'Color', [0.5, 0.5, 0.5]);\nviscircles(center2', radius2, 'Color', [0.7, 0.3, 0.3]); % Visualizing the second obstacle\nviscircles(center3', radius3, 'Color', [0.3, 0.5, 0.7]); % Visualizing the third obstacle\nplot(0, 0, 'ko', 'MarkerFaceColor', 'k')\nplot(1, 1, 'ko', 'MarkerFaceColor', 'k')\n\n% Initializing the tree\ntheta_start.parent = 0;\nG(1) = theta_start;\n\nfor idx = 1:N\n    \n    % Stop if the last node in G is close to theta_goal\n    if norm(G(end).coord  - theta_goal) &lt; epsilon\n        break\n    end\n\n    % sample random joint position: probability 0.2\n    \n    if rand() &lt; 0.2\n        theta_rand = theta_goal;\n    else\n        theta_rand = rand(2,1);\n    end\n    \n    % find node in G nearest to theta_rand\n    min_dist = inf;\n    theta_near_index = 0;\n\n    for jdx = 1:length(G)\n        coord = G(jdx).coord;\n        dist = norm(theta_rand - coord);\n        if dist &lt; min_dist\n            min_dist = dist;\n            theta_near_index = jdx;\n        end\n    end\n\n    theta_near = G(theta_near_index);\n    % take a step from theta_near towards theta_rand\n    vec_to_rand = theta_rand - theta_near.coord;\n    dist_to_rand = norm(vec_to_rand);\n    if dist_to_rand &lt; delta\n        theta_new.coord = theta_rand;\n    else\n        theta_new.coord = theta_near.coord + delta * ...\n        vec_to_rand/dist_to_rand;\n    end\n    \n    % check if theta_new is collision free with all obstacles\n    dist_to_obs1 = norm(theta_new.coord - center);\n    dist_to_obs2 = norm(theta_new.coord - center2);\n    dist_to_obs3 = norm(theta_new.coord - center3);\n    if dist_to_obs1 &lt; radius || dist_to_obs2 &lt; radius2 || dist_to_obs3 &lt; radius3\n        continue\n    end\n\n    % if collision free, add theta_new to tree with parent theta_near\n    theta_new.parent = theta_near_index;\n    G = [G, theta_new];\n\n    % plot node and edge\n    plot(theta_new.coord(1), theta_new.coord(2), 'o', 'Color', [0.5, 0.5, 0.5], ...\n    'MarkerFaceColor', [0.5, 0.5, 0.5])\n    line([theta_near.coord(1), theta_new.coord(1)], [theta_near.coord(2), ...\n    theta_new.coord(2)], 'Color', 'k', 'LineWidth', 2);\n    drawnow\n\nend\n\n% work backwards from the final node to the root of the tree\nchild_theta = G(end);\nwhile child_theta.parent ~= 0\n\n    parent_theta_index = child_theta.parent;\n    parent_theta = G(parent_theta_index);\n    line([child_theta.coord(1), parent_theta.coord(1)], ...\n        [child_theta.coord(2), parent_theta.coord(2)], ...\n        'Color', [1, 0.5, 0], 'LineWidth', 3);\n    child_theta = parent_theta;\nend\n\nResult:\n\nThe samples ğ‘ across ten runs with baseline and goal bias are tabulated below.\n\n\n\nrun\nbaseline\ngoal bias\n\n\n\n\n1\n254\n225\n\n\n2\n363\n169\n\n\n3\n318\n150\n\n\n4\n590\n143\n\n\n5\n352\n249\n\n\n6\n359\n166\n\n\n7\n202\n251\n\n\n8\n267\n162\n\n\n9\n440\n183\n\n\n10\n226\n313\n\n\n\nOn average, the baseline required 337 samples to reach a valid motion plan, while goal bias needed 201 samples. For Environment 3 the evidence suggests that biasing the samples towards ğœƒğ‘”ğ‘œğ‘ğ‘™ decreases the total number of samples and causes RRT to reach a solution faster. Intuitively, this is because the tree is trying to move in the direction of the goal more frequently.\nThis is particularly advantageous when the robot is in free space (or has passed the obstacles) and should move directly towards the goal. The code snippet required to implement goal bias is shown below:\n\n\n\n\n\n\n# Sample random joint position: probability 0.2\nif np.random.rand() &lt; 0.2:\n    theta_rand = theta_goal['coord']\nelse:\n    theta_rand = np.random.rand(2)\n\n\n\n\n\nCode\n\n% sample random joint position: probability 0.2\n    \nif rand() &lt; 0.2\n    theta_rand = theta_goal;\nelse\n    theta_rand = rand(2,1);\nend"
  },
  {
    "objectID": "tool.html",
    "href": "tool.html",
    "title": "Tools",
    "section": "",
    "text": "Git\n\n\n\nTools/Packages\n\n\n\nDiving deep into the Git tool\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\n\nTools/Packages\n\n\n\nGuide to use Quarto to its full potential\n\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "Tools/quarto/quarto.html",
    "href": "Tools/quarto/quarto.html",
    "title": "Quarto",
    "section": "",
    "text": "Goal\n\n\n\nCreate a Quarto website and publish it on GitHub/GitLab Pages, Netlify, and Quarto Pub.\nQuarto is an open-source software system for turning plain-text source files into outputs like articles, books, blogs, dashboards, presentations, reports, and websites. Announced by Posit CEO JJ Allaire, Quarto is already taking the worldğŸŒby stormâ›ˆï¸!\nI strongly believe that everyone, regardless of their background and current technical skill level, can learn and benefit from Quarto. Getting started with Quarto is easy thanks to its excellent documentation and vibrant community of enthusiastic users and developers.\nRather than repeat the basic information already available elsewhere, I will share some advanced techniques along with the fundamental knowledge needed to understand how they work. The topics I cover are very technical, but my goal is to make the content on my blog as accessible as possible.\nThe first topic I will delve into is creating and publishing a website with Quarto. To follow the Quarto documentation on creating a website, you will need Visual Studio Code (VSCode), VSCodium and RStudio, or a terminal."
  },
  {
    "objectID": "Tools/quarto/quarto.html#sec-inst",
    "href": "Tools/quarto/quarto.html#sec-inst",
    "title": "Quarto",
    "section": "Installing tools",
    "text": "Installing tools\nIf you use macOS, Linux, or the Windows Subsystem for Linux (WSL), you can install all of the aforementioned tools with the Homebrew package manager. To install everything you will need to follow along with this blog post, you can first install Homebrew and then run brew bundle in a directory that contains the Brewfile shown in ExampleÂ 1.\n\nExample 1 Â \n\n\nBrewfile\n\nbrew \"gh\"\nbrew \"git\"\nbrew \"glab\"\ncask \"github\"\ncask \"quarto\"\ncask \"rstudio\"\ncask \"vscodium\"\ncask \"visual-studio-code\"\nvscode \"quarto.quarto\"\nvscode \"REditorSupport.r\"\n\n\nUsing a package manager like Homebrew to install all the requirements with a single shell command like brew bundle is the fastest and easiest way to get ready to follow along with this blog post. If you are curious about how I set up my computer, you can take a look at my Brewfile and other configuration files in my setup repository (repo) on GitHub.\nApart from RStudio, VSCode, and VSCodium, the Brewfile in ExampleÂ 1 will install the Git version control system, the GitHub and GitLab command line interfaces (CLIs), and GitHub Desktop, a Git Graphical User Interface (GUI). For more information on Git, a tool used by 93% of software developers worldwide according to survey results published by StackOverflow, take a look at the â€œGitHub for supporting, reusing, contributing, and failing safelyâ€ post by Allison Horst and Julie Lowndes on the Openscapes blog.\nAs an alternative to installing tools on your computer, you can use the web interface provided by GitHub Codespaces. To set up a Codespace, you can remove the lines that start with cask from the Brewfile provided in ExampleÂ 1 and add the file to a repo called dotfiles and along with a setup.sh file like the one shown in ExampleÂ 2.\n\nExample 2 Â \n\n\nsetup.sh\n\necho | /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\"\n(echo; echo 'eval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"') &gt;&gt; /home/codespace/.profile\neval \"$(/home/linuxbrew/.linuxbrew/bin/brew shellenv)\"\nbrew bundle"
  },
  {
    "objectID": "Tools/quarto/quarto.html#sec-over",
    "href": "Tools/quarto/quarto.html#sec-over",
    "title": "Quarto",
    "section": "Publishing overview",
    "text": "Publishing overview\nOnce you are done setting up your computer or Codespace, you create a Quarto website template and make your site publicly available on the internet using one of the many available publishing services. To explore and assess different publishing workflows and free website hosting options, I set up my personal website on four different web hosts: GitHub Pages.\nThere are so many different ways to publish a Quarto site that I decided to come up with a naming system for Quarto publishing methods. The naming system derives a code for each publishing method from the numbered lists in the Quarto publishing documentation. For example, I refer to the two methods to publish to Quarto Pub as Q1 and Q2:\nTableÂ 1 uses this naming system in its Code column to identify the publishing methods I discuss in this blog post. Each publishing method targets a particular web Host, Renders content locally or on a remote server, and deploys sites using either the quarto publish or the git push shell Command.\n\n\nTableÂ 1: Quarto website publishing methods\n\n\nCode\nHost\nRender\nCommand\n\n\n\n\nG1\nGitHub\nLocal\npush\n\n\nG2\nGitHub\nLocal\npublish\n\n\nG3\nGitHub\nRemote\npush\n\n\nN1\nNetlify\nLocal\npublish\n\n\nN2\nNetlify\nLocal\npush\n\n\nN3\nNetlify\nRemote\npush\n\n\nQ1\nQuarto\nLocal\npublish\n\n\nQ2\nQuarto\nRemote\npush\n\n\n\n\n\nGit workflow\nThe first publishing method I tried for my personal site was G1, which requires the use of Git to push all website content to a Git provider like GitHub or GitLab. After I set up GitHub Pages and GitLab Pages, I could update my website on both of these web hosts just by going through the standard Git workflow shown in FigureÂ 1.\n\n\n\n\n\nflowchart LR\n   A[working&lt;br/&gt;directory]-.git&lt;br/&gt;add.-&gt;B{{staging&lt;br/&gt;area}}-.git&lt;br/&gt;commit.-&gt;C([local&lt;br/&gt;repo])-.git&lt;br/&gt;push.-&gt;D(remote&lt;br/&gt;repo)\n\n\nFigureÂ 1: Git workflow\n\n\n\n\nTo make it easier to make incremental changes to my website and frequently release new content, I combined all of the git shell commands in FigureÂ 1 into a shell alias. You can add shell aliases to a shell configuration file like .bashrc or .zshrc on your computer to shorten commands or combine any sequence of commands into one step.\nThe aacmp alias in the .zshrc file in my setup repo allows me to enter a free-form commit message directly on the command line without quotes, e.g.Â qp edit first blog. If you decide to try my aacmp alias, please exercise extreme caution as any un-escaped or un-quoted metacharacters may yield surprising effects instead of being included verbatim in the commit message. For example, qp * will list the contents of the current directory in the commit message in place of the asterisk!\nAn alternative to a shell alias that combines git commands is a keyboard shortcut in Git-enabled GUI. ExampleÂ 3 shows two files, tasks.json and update.sh, that we can use to set up VSCode, VSCodium, and GitHub Codespaces to go through the Git workflow whenever we press Ctrl+Shift+B on Linux/Windows or âŒ˜+Shift+B on Mac (mnemonic: B is for Build).\n\nExample 3 Â \n\nupdate.shtasks.json\n\n\n\n\n\n\n\n\n\n\nThis mechanism is called Tasks and is used to automate software build tasks, which can include any steps required to build and publish a website. Importantly, the Tasks mechanism requires that the tasks.json file be added to the .vscode directory and that we enable execution of the update.sh script by running chmod +x update.sh in our project root.\nShell aliases and keyboard shortcuts can greatly facilitate the Git workflow which is essential not only for G1, but also G3, N3, and Q2. Unlike these other publishing methods, G1 leads to messy commits that contain changes to both source and output files.\n\n\nquarto publish\nTo have cleaner commits, I switched from G1 to G2 by adding quarto publish to my publishing workflow. With G2, I can track changes to my source files on my main branch and publish my output files to GitHub Pages and GitLab Pages from my gh-pages branch.\nQ1, N1, and G2 all use quarto publish to render website content locally and then deploy it in one fell swoop. If you do not plan to use the Git version control system or the advanced features offered by GitHub, GitLab, or Netlify, then I recommend deploying your site to Quarto Pub by running quarto publish quarto-pub (Q1 in TableÂ 1).\nLike Q1, N1 is a deployment method that does not require Git and makes it possible to deploy our site with a single shell command: quarto publish netlify. N1 provides access to the advanced web hosting features offered by Netlify and can even render content as long as code execution is frozen.\nI deployed my site to GitHub Pages currently. When I run quarto publish gh-pages, Quarto renders my site into my output directory, copies the output directory contents to the gh-pages branch of my local repo, and then commits and pushes the changes to my remote repos on GitHub, I am planning to trigger Netlify and GitLab as well to build my site from the gh-pages branch through this process.\nTo summarize the Quarto publishing methods I discussed so far, Q1 and N1 are easy to configure and use, N2 automatically builds sites from Git repos, G1 is not recommended because it pollutes commits with output file changes, and G2 is more difficult to set up and use but provides clean commits and a nice separation of source and output files.\n\n\nGitHub Actions\nAll of the publishing methods I have discussed so far require us to generate output files locally by rendering our source files. In contrast, Q2, G3, and N3 make it possible to skip local rendering in favor of relying on GitHub Actions to handle all of the necessary steps.\nG3 is noteworthy, because it offers the same convenience of G1 but without messy commits that mix changes to source and output files. An added bonus of G3 is that rendering with GitHub Actions provides a reproducible computational environment that is not dependent on what you have installed on your computer.\nInstead of using GitHub Actions, I could have used GitLab CI/CD to build my site. I decided not to go down this route because the Quarto dev team has many GitHub Actions workflows available but currently no official support for GitLab CI/CD.\nBefore trying to use GitHub Actions or any other continuous integration systems in your publishing workflow, I suggest getting used to working with quarto publish (Q1, G2, or N1). You can always set up other publishing methods later without sacrificing anything, because all of the publishing methods except G1 can be combined together.\nIn SectionÂ 1.3, I will walk through the setup of both G2 and G3 to provide the option of rendering locally by running quarto publish or rendering remotely with GitHub Actions by pushing to the main branch. Along the way, I will share many practical tips and general advice that you can apply to any project."
  },
  {
    "objectID": "Tools/quarto/quarto.html#sec-set",
    "href": "Tools/quarto/quarto.html#sec-set",
    "title": "Quarto",
    "section": "Publishing setup",
    "text": "Publishing setup\n\nRepo setup\nBefore you can use GitHub Actions to publish your site, you will need a GitHub, an SSH key, and a repo like Blog that has a default branch called main and another branch which must be called gh-pages. If you want to publish on GitLab Pages, you will also need a GitLab account.\nYou can create the repo and the gh-pages branch using the web interface of https://github.com or https://gitlab.com in your browser, but the best way to start a new project is using the CLI for GitHub or GitLab in your terminal. First, run gh auth login or glab auth login and follow the prompts to authenticate via your web browser or with an authentication token.\nThe GitHub CLI allows you to add an SSH key to your account during or after authentication. The GitLab CLI does not handle SSH keys during authentication, but has a similar command for adding an SSH key to your GitLab account.\nAfter authentication and SSH key setup, you can run the code in either of the code chunks in ExampleÂ 4 to set up your local and remote repos and create a Quarto website project in the local repo. You can create shell alias that combine all of the repo creation steps like I did in my .zshrc.\n\nExample 4 Â \n\nGitHubGitLab\n\n\ncd # start in home directory\nmkdir -p USERNAME\ncd USERNAME\ngh repo create USERNAME.github.io --add-readme --clone --public\ncd SITENAME\nquarto create project website USERNAME.github.io\n\n\ncd # start in home directory\nmkdir -p USERNAME\ncd USERNAME\nglab repo create USERNAME.gitlab.io --readme --defaultBranch main --public\ncd SITENAME\ngit pull origin main\ngit branch --set-upstream-to=origin/main main\nquarto create project website USERNAME.gitlab.io\n\n\n\n\nTo make it easier to maintain my site on both GitHub and GitLab Pages, I set up my local repo cloned to have two origin remote URLs using the code as shown in ExampleÂ 5. Now, running quarto publish or git push in my local repo, updates my content on both GitHub and GitLab.\n\nExample 5 Â \ngit remote add hub git@github.com:NishantBharali/Blog\ngit remote set-url --add origin $(git remote get-url lab)\n\nIf you want to have your website hosted on GitHub Pages, you will need to set gh-pages as your source branch in your repo settings. For GitLab Pages, you will need to add a .gitlab-ci.yml file to your repo and update your _quarto.yml file as shown in ExampleÂ 6 to include .gitlab-ci.yml as a resource in your output directory.\n\nExample 6 Â \n\n.gitlab-ci.yml_quarto.yml\n\n\n\n\n\n\n\n\n\n\nBy default, GitLab Pages includes a random hash in site URLs. To shorten the URL of my GitLab Pages site, one has to uncheck Use unique domain under Deploy &gt; Pages in the GitLab sidebar.\nAt this point, we have completed G2 setup and you should be able to run quarto publish gh-pages from your main branch to render your site and deploy it to GitHub and/or GitLab Pages. Deploying with quarto publish at least once is a prerequisite for setting up any of the publishing methods that rely on GitHub Actions, because quarto publish creates a _publish.yml file in the root of your project that is required for publishing via GitHub Actions.\n\n\nGitHub Actions\nIn addition to the steps described above, G3 setup requires that we create a .github/workflows directory and add a YAML file to that directory. ExampleÂ 7 contains the gh-pages.yml file I use for my own site and the shell code that can used to obtain this file.\n\nExample 7 Â \n\ncurlgh-pages.yml\n\n\nmkdir -p .github/workflows\ncd .github/workflows\ncurl -O https://raw.githubusercontent.com/NishantBharali/Blog/main/.github/workflows/gh-pages.yml\n\n\n\n\n\n\n\nThe gh-pages.yml file in ExampleÂ 7 installs Quarto, the R and Python programming languages, and the packages in the renv.lock and requirements.txt files. If you do not need R and/or Python, you can remove any unnecessary portions of the file.\nTo modify the Python files that are installed by GitHub Actions, you can edit the requirements.txt file in your repo. To update your renv.lock file so that it includes all of the R packages your site requires, run renv::snapshot() in an R session or Rscript -e renv::snapshot() in a shell.\nAfter pushing the gh-pages.yml file, you can visit the Actions tab in your remote repo on GitHub to check the progress of the deployment of your site. If your site did not build successfully, you can go through the logs to try to diagnose the problem.\nI added a â€œPush to GitLabâ€ step to my gh-pages.yml to make GitHub Actions push my remote gh-pages to GitLab so that my site is kept in sync on both GitHub and GitLab Pages. This required manually creating a token on GitLab and adding it to GitHub, which I accomplished using the GitHub CLI as shown in ExampleÂ 8.\n\nExample 8 Â \ngh secret set GITLAB_AUTH_TOKEN\n\nAfter the setup described above, I now have two options for publishing my Quarto site: quarto publish and 2) git push. In addition to GitHub and GitLab Pages, both of these options automatically update my site on Netlify via N2.\nTo also automatically update my site on Quarto Pub, I created a separate GitHub Actions workflow by adding another YAML file to the .github/workflows directory in my repo. ExampleÂ 9 shows my quarto-pub.yml file and the shell code that can be used to obtain it.\n\nExample 9 Â \n\ncurlquarto-pub.yml\n\n\nmkdir -p .github/workflows\ncd .github/workflows\ncurl -O https://raw.githubusercontent.com/NishantBharali/Blog/main/.github/workflows/gh-pages.yml\n\n\n\n\n\n\n\nMy quarto-pub.yml file is based on Q2, but it runs upon completion of the pages-build-deployment workflow instead of a push to main. I changed the workflow trigger so that it runs after my GitHub Pages site is built, regardless of whether I triggered the build by running quarto publish gh-pages or pushing to main.\nFigureÂ 2 summarizes all of the steps that occur during my Quarto publishing workflow. This workflow allows me to publish my site on four web hosts every time I run quarto publish gh-pages (G2) or git push (G3)!\n\n\n\n\n\nflowchart LR\n    A[local&lt;br/&gt;main]-.G2.-&gt;B[local&lt;br/&gt;gh-pages]-.G2.-&gt;C[GitHub&lt;br/&gt;gh-pages]\n    B[local&lt;br/&gt;gh-pages]-.G2.-&gt;D[GitLab&lt;br/&gt;gh-pages]\n    A[local&lt;br/&gt;main]-.G3.-&gt;F[GitHub&lt;br/&gt;main]-.G3.-&gt;C[GitHub&lt;br/&gt;gh-pages]-.G3.-&gt;D[GitLab&lt;br/&gt;gh-pages]\n    A[local&lt;br/&gt;main]-.G3.-&gt;G[GitLab&lt;br/&gt;main]\n    C[GitHub&lt;br/&gt;gh-pages]-.N2.-&gt;E[Netlify]\n    C[GitHub&lt;br/&gt;gh-pages]-.Q2.-&gt;H[Quarto&lt;br/&gt;Pub]\n\n\nFigureÂ 2: Quarto publishing workflow\n\n\n\n\nSo far I have only noticed one difference between the four web hosts I use for my site: GitHub Pages is the only web host that properly differentiates between internal and external links. All of the other web hosts include the external link icon on all links regardless of whether they target my site or an external site.\nI tried unsuccessfully to solve this issue by setting the link-external-filter property to a regular expression. If you notice a problem with the link-external-icon feature on other web hosts, I suggest switching to GitHub Pages."
  },
  {
    "objectID": "Tools/quarto/quarto.html#sec-nav",
    "href": "Tools/quarto/quarto.html#sec-nav",
    "title": "Quarto",
    "section": "Site navigation",
    "text": "Site navigation\nOnce your site is built and published, you can make it easier to navigate with a customized navigation bar (navbar) and sidebar, as described in the Quarto documentation. Check out the _quarto.yml files for my site and the Quarto documentation site to see how the navigation components are set up.\nIn my navbar, I added toggle buttons for reader mode and dark mode. The names of the other icons on right side of the navbar are included in the sidebar."
  },
  {
    "objectID": "Tools/quarto/quarto.html#sec-html",
    "href": "Tools/quarto/quarto.html#sec-html",
    "title": "Quarto",
    "section": "HTML blocks",
    "text": "HTML blocks\nMy navbar also provides the current date and time in Dec ordinal (deco) format, which counts the years since 1 BC and the days since March 1. Dec is a calendar and time system that I created and use throughout my blog.\nMine Ã‡etinkaya-Rundel wrote about HTML blocks as part of her A Quarto tip a day project.\nHTML blocks are useful for running JavaScript code in the context of a single page on your site without affecting the others. I also use HTML blocks to add a &lt;style&gt; element to my .qmd files as a last ditch effort to fine tune the style of my site.\nBefore I resort to the &lt;style&gt; element approach, I try writing CSS in my styles.css, light.css, and dark.css files. So far, this approach has been sufficient to make any styling changes I want, but if it every fails, I can use JavaScript to override the default styling provided by Quarto by modifying style attributes, which have the highest specificity in CSS."
  },
  {
    "objectID": "Tools/quarto/quarto.html#sec-filt",
    "href": "Tools/quarto/quarto.html#sec-filt",
    "title": "Quarto",
    "section": "Pandoc filter",
    "text": "Pandoc filter\nIn addition to customizing dates on the listing page of my blog, I wanted to customize the date format in every blog post. To complete this task, I used a Lua script as a Pandoc filter.\nPandoc is a program that converts documents into practically any format. The â€œpanâ€ in Pandoc comes from the Ancient Greek word for all. Pandoc strives to convert all document formats, just like Pangea contained all the land and a panacea solves all problems.\nQuarto uses Pandoc to convert markdown files into target format(s) like html or pdf. If the source files contain executable code, Quarto executes the code via one of two computational engines: Jupyter or Knitr. FigureÂ 3 shows the Quarto workflow.\n\n\n\n\n\nflowchart LR\n   A[qmd&lt;br/&gt;ipynb]-.Knitr&lt;br/&gt;Jupyter.-&gt;B((md))-.Pandoc with&lt;br/&gt;Lua filters.-&gt;C(html&lt;br/&gt;pdf&lt;br/&gt;docx&lt;br/&gt;etc.)\n\n\nFigureÂ 3: Quarto rendering workflow\n\n\n\n\nMine Ã‡etinkaya-Rundelâ€™s Quarto tip series includes a similar Quarto workflow mermaid diagram and her â€œHello, Quarto!â€ rstudio::conf(2022) keynote with Julia Stewart Lowndes contains truly beautiful Quarto workflow images by Allison Horst: 1, 2, 3, and 4.\nQuarto controls Pandoc, Jupyter, and Knitr in two ways: 1) with arguments passed to the Quarto CLI commands and 2) with YAML key-value pairs in .qmd, .ipynb, or .yml files.\nHTML blocks can run JavaScript which excels at making content dynamic and interactive, pre- and post-render scripts can be in any programming language, while Pandoc filters are written in Lua and modify output during rendering."
  }
]